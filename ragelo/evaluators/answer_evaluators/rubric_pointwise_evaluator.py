from __future__ import annotations

from typing import Type

from pydantic import BaseModel, Field, create_model

from ragelo.evaluators.answer_evaluators import BaseAnswerEvaluator
from ragelo.evaluators.answer_evaluators.base_answer_evaluator import AnswerEvaluatorFactory
from ragelo.types.answer_formats import Criterion, CriterionEvaluationPointwise, RubricPointwiseAnswerFormat
from ragelo.types.configurations import RubricPointwiseEvaluatorConfig
from ragelo.types.evaluables import AgentAnswer, Document
from ragelo.types.formats import LLMInputPrompt, LLMResponseType
from ragelo.types.query import Query
from ragelo.types.types import AnswerEvaluatorTypes
from ragelo.utils import call_async_fn, string_to_template


class RubricSchema(BaseModel):
    criteria: list[Criterion] = Field(description="The criteria to be used to evaluate the quality of the responses.")


@AnswerEvaluatorFactory.register(AnswerEvaluatorTypes.RUBRIC_POINTWISE)
class RubricPointwiseEvaluator(BaseAnswerEvaluator):
    config: RubricPointwiseEvaluatorConfig

    criteria_prompt = string_to_template("""
        You are a domain expert in {{ expert_in }}.{% if company %} You work for {{ company }}.{% endif %}
        Your task is to, given a user question and a set of relevant retrieved documents, create a rubric for evaluating the quality of reports generated by other agents.
        You should think deeply and carefully about what questions should a complete and high-quality report that answer the question should answer. 
        Each criterion should be a short yes/no question that can be used to evaluate the quality of the responses. 
        You should write {{ n_criteria }} criteria.
        If a criterion is supported by a document, you should include the document ID in the supporting_documents list for that criterion.
        """)

    criteria_user_prompt = string_to_template("""
        [User Question]
        {{ query.query }}

        [Retrieved Documents]
        {% for doc in documents %}
        [[{{doc.did}}]] {{doc.text}}
        --------------------------------
        {% endfor %}
        """)

    system_prompt = string_to_template("""
        You are a domain expert in {{ expert_in }}.{% if company %} You work for {{ company }}.{% endif %} 
        You are tasked with evaluating the quality of a report written by a deep research agent in response of a user's question.
        The report was written based on a set of documents retrieved by the agent, and should thoroughly answer the user's question based exclusively on the relevant documents retrieved by the agent.

        To properly evaluate the quality of the report, you will be provided with a list of criteria to evaluate its quality.
        Each criterion includes a short question and a list of documents that support the inclusion of the criterion in the report.
        For each criterion, you should think carefully about wether the report answers the criterion, and include a brief reasoning for your decision.

        You should think carefully about the criteria and the answers, and assign the final judgement accordingly.

        ## Criteria
        {% for criteria in criteria.criteria %}
        Criterion: {{criteria.criterion_name}}
        Supporting Documents: {{criteria.supporting_documents}}
        Short Question: {{criteria.short_question}}
        --------------------------------
        {% endfor %}
        """)

    user_prompt = string_to_template("""
        [User Question]
            {{query.query}}

        [Agent's Report]
            {{ answer.text }}
        """)

    criteria_cache: dict[str, Criterion] = {}
    answer_schema_cache: dict[str, Type[BaseModel]] = {}

    async def _build_criteria(self, query: Query, documents: list[Document]) -> RubricSchema:
        documents = self._filter_documents(query)
        context = {
            "expert_in": self.config.expert_in,
            "company": self.config.company,
            "documents": documents,
            "query": query,
            "n_criteria": self.config.n_criteria,
        }
        criteria_prompt = self.criteria_prompt.render(context)
        user_prompt = self.criteria_user_prompt.render(context)
        llm_input = LLMInputPrompt(system_prompt=criteria_prompt, user_message=user_prompt)
        llm_response = await self.llm_provider.call_async(llm_input, response_schema=RubricSchema)
        criteria_models = {}
        for criteria in llm_response.parsed_answer.criteria:
            criteria_models[criteria.criterion_name] = create_model(
                criteria.criterion_name,
                reasoning=(
                    str,
                    Field(
                        description="A brief explanation about your judgement, and why you believe the report fulfills or not the criterion"
                    ),
                ),
                fulfillment=(
                    bool,
                    Field(description="Whether the report fulfills the criterion or not"),
                ),
            )

        evaluation_schema = create_model("EvaluationSchema", **criteria_models)
        self.answer_schema_cache[query.qid] = evaluation_schema
        return llm_response.parsed_answer

    def _build_message(self, query: Query, answer: AgentAnswer) -> LLMInputPrompt:
        documents = self._filter_documents(query)
        if query.qid not in self.criteria_cache:
            self.criteria_cache[query.qid] = call_async_fn(
                self._build_criteria, query, list(query.retrieved_docs.values())
            )
        criteria = self.criteria_cache[query.qid]
        self.config.llm_response_schema = self.answer_schema_cache[query.qid]
        system_prompt = self.system_prompt.render(
            expert_in=self.config.expert_in, criteria=criteria, company=self.config.company
        )
        user_prompt = self.user_prompt.render(query=query, answer=answer)
        return LLMInputPrompt(system_prompt=system_prompt, user_message=user_prompt)

    def _process_answer(self, llm_response: LLMResponseType, query: Query) -> LLMResponseType:
        response_dict = llm_response.parsed_answer.model_dump()
        criteria: list[CriterionEvaluationPointwise] = []
        for response in response_dict.values():
            criterion = CriterionEvaluationPointwise(
                criterion=self.criteria_cache[query.qid],
                reasoning=response["reasoning"],
                fulfillment=response["fulfillment"],
            )
            criteria.append(criterion)
        return LLMResponseType(
            raw_answer=llm_response.raw_answer,
            parsed_answer=RubricPointwiseAnswerFormat(
                criteria=criteria,
                average_score=sum(response["fulfillment"] for response in response_dict.values()) / len(response_dict),
            ),
        )
