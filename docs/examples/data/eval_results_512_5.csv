,Index,question,answer,contexts,ground_truth,answer_relevancy_evaluator,aspect_critic_evaluator,context_utilization_evaluator,correctness_evaluator,faithfullness_evaluator,precision_evaluator,recall_evaluator,relevancy_evaluator,similarity_evaluator
0,0,What is vaccum optimizer ?,"The Vacuum Optimizer in Qdrant is used to address the accumulation of deleted records within a segment repository. Instead of immediately deleting entries after a query, Qdrant marks records as deleted and ignores them for future queries. Over time, these deleted records can accumulate, occupy memory, and slow down the system. The Vacuum Optimizer is triggered when a segment has accumulated too many deleted records, as defined in the configuration file. You can set parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to perform segment optimization. This optimizer helps maintain system efficiency by managing deleted records effectively. For more details, you can refer to the documentation on Optimizer under the section ""Vacuum Optimizer"" with the source page [here](../optimizer).","['document:--- title: Optimizer weight: 70 aliases: - ../optimizer --- # Optimizer It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely. Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)). In this case, the segment to be optimized remains readable for the time of the rebuild. ![Segment optimization](/docs/optimization.svg) The availability is achieved by wrapping the segment into a proxy that transparently handles data changes. Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates. ## Vacuum Optimizer The simplest example of a case where you need to rebuild a segment repository is to remove points. Like many other databases, Qdrant does not delete entries immediately after a query. Instead, it marks records as deleted and ignores them for future queries. This strategy allows us to minimize disk access - one of the slowest operations. However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system. To avoid these adverse effects, Vacuum Optimizer is used. It is used if the segment has accumulated too many deleted records. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # The minimal fraction of deleted vectors in a segment, required to perform segment optimization deleted_threshold: 0.2 # The minimal number of vectors in a segment, required to perform segment optimization vacuum_min_vector_number: 1000 ``` ## Merge Optimizer The service may require the creation of temporary segments. Such segments, for example, are created as copy-on-write segments during optimization itself. It is also essential to have at least one small segment that Qdrant will use to store frequently updated data. On the other hand, too many small segments lead to suboptimal search performance. There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # If the number of segments exceeds this value, the optimizer will merge the smallest segments. max_segment_number: 5 ``` ## Indexing Optimizer Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records. So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan. The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only ,source:documentation/concepts/optimizer.mddocument:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.mddocument:--- title: Optimize Resources weight: 11 aliases: - ../tutorials/optimize --- # Optimize Qdrant Different use cases have different requirements for balancing between memory, speed, and precision. Qdrant is designed to be flexible and customizable so you can tune it to your needs. ![Trafeoff](/docs/tradeoff.png) Let\'s look deeper into each of those possible optimization scenarios. ## Prefer low memory footprint with high speed search The main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads. Vector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true } } ); ``` `mmmap_threshold` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM. Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision. ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""rescore"": false } }, ""vector"": [0.2, 0.1, 0.9, 0.7], ""limit"": 10 } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.search( collection_name=""{collection_name}"", query_vector=[0.2, 0.1, 0.9, 0.7], search_params=models.SearchParams( quantization=models.QuantizationSearchParams(rescore=False) ), ) ``` ```typescript import { QdrantClient ,source:documentation/guides/optimize.mddocument:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.mddocument:while maintaining low latency across hundreds of thousands of collections with retrieval only taking milliseconds, as well as maintaining high accuracy. Additionally, Polu highlights the efficiency gains Dust was able to unlock with Qdrant: ""We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as we don’t have to run lots of nodes in parallel. While being memory-bound, we were able to push the same instances further with the help of quantization. While you get pressure on MMAP in this case you maintain very good performance even if the RAM is fully used. With this we were able to reduce our cost by 2x."" ## Outlook Dust will continue to build out their platform, aiming to be the platform of choice for companies to execute on their internal GenAI strategy, unlocking company knowledge and driving team productivity. Over the coming months, Dust will add more connections, such as Intercom, Jira, or Salesforce. Additionally, Dust will expand on its structured data capabilities. To learn more about how Dust uses Qdrant to help employees in their day to day tasks, check out our [Vector Space Talk](https://www.youtube.com/watch?v=toIgkJuysQ4) featuring Stanislas Polu, Co-Founder of Dust.,source:blog/case-study-dust.md']","The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ",0.8935675537554882,0,0.9999999999,0.7820763355395592,1.0,0.9999999999,1.0,0.008130081300813009,True
1,1,Tell me about ‘always_ram’ parameter?,"The `always_ram` parameter in Qdrant determines whether quantized vectors are kept cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, setting `always_ram` to `true` allows you to store quantized vectors in RAM, which can speed up the search process. This parameter is particularly useful in setups where you want to optimize search speed by keeping quantized vectors readily available in memory. You can find more information about this parameter in the documentation under the 'Setting up Product Quantization' section. (Source: documentation/guides/quantization.md)","['document:vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Binary = new BinaryQuantization { AlwaysRam = true } } ); ``` `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. In this case, you can set `always_ram` to `true` to store quantized vectors in RAM. ### Setting up Product Quantization To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""product"": { ""compression"": ""x16"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ProductQuantization( product=models.ProductQuantizationConfig( compression=models.CompressionRatio.X16, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { product: { compression: ""x16"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CompressionRatio, CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Product(ProductQuantization { compression: CompressionRatio::X16.into(), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CompressionRatio; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.ProductQuantization; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setProduct( ProductQuantization.newBuilder() .setCompression(CompressionRatio.x16) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Product = new ProductQuantization { Compression = CompressionRatio.X16, AlwaysRam = true } } ); ``` There are two parameters that you can specify in the `quantization_config` section: `compression` - compression ratio. Compression ratio represents the size of the quantized vector in bytes divided by the size of the original vector in bytes. In this case, the quantized vector will be 16 times smaller than the original vector. `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`. ### Searching with Quantization Once you have configured quantization for a collection, you don\'t need to do anything extra to search with quantization. Qdrant will automatically use quantized vectors if they are available. However, there are a few options that you can use to control the search process: ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""ignore"": false, ""rescore"": true, ""oversampling"": 2.0 } ,source:documentation/guides/quantization.mddocument:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.mddocument:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.mddocument:--- title: Minimal RAM you need to serve a million vectors short_description: How to properly measure RAM usage and optimize Qdrant for memory consumption. description: How to properly measure RAM usage and optimize Qdrant for memory consumption. social_preview_image: /articles_data/memory-consumption/preview/social_preview.jpg preview_dir: /articles_data/memory-consumption/preview small_preview_image: /articles_data/memory-consumption/icon.svg weight: 7 author: Andrei Vasnetsov author_link: https://blog.vasnetsov.com/ date: 2022-12-07T10:18:00.000Z # aliases: [ /articles/memory-consumption/ ] --- <!-- 1. How people usually measure memory and why it might be misleading 2. How to properly measure memory 3. Try different configurations of Qdrant and see how they affect the memory consumption and search speed 4. Conclusion --> <!-- Introduction: 1. We are used to measure memory consumption by looking into `htop`. But it could be misleading. 2. There are multiple reasons why it is wrong: 1. Process may allocate memory, but not use it. 2. Process may not free deallocated memory. 3. Process might be forked and memory is shared between processes. 3. Process may use disk cache. 3. As a result, if you see `10Gb` memory consumption in `htop`, it doesn\'t mean that your process actually needs `10Gb` of RAM to work. --> When it comes to measuring the memory consumption of our processes, we often rely on tools such as `htop` to give us an indication of how much RAM is being used. However, this method can be misleading and doesn\'t always accurately reflect the true memory usage of a process. There are many different ways in which `htop` may not be a reliable indicator of memory usage. For instance, a process may allocate memory in advance but not use it, or it may not free deallocated memory, leading to overstated memory consumption. A process may be forked, which means that it will have a separate memory space, but it will share the same code and data with the parent process. This means that the memory consumption of the child process will be counted twice. Additionally, a process may utilize disk cache, which is also accounted as resident memory in the `htop` measurements. As a result, even if `htop` shows that a process is using 10GB of memory, it doesn\'t necessarily mean that the process actually requires 10GB of RAM to operate efficiently. In this article, we will explore how to properly measure RAM usage and optimize Qdrant for optimal memory consumption. ## How to measure actual memory requirements <!-- 1. We need to know how much RAM we need to have for the program to work, so why not just do a straightforward experiment. 2. Let\'s limit the allowed memory of the process and see at which point the process will working. 3. We can do a grid search, but it is better to apply binary search to find the minimum amount of RAM more quickly. 4. We will use docker to limit the memory usage of the process. 5. Before running docker we will use ``` # Ensure that there is no data in page cache before each benchmark run sudo bash -c \'sync; echo 1 > /proc/sys/vm/drop_caches\' ``` to clear the page between runs and make sure ,source:articles/memory-consumption.mddocument:5 rps 300mb - 0.9 rps / 1.1 sec per query 150mb - 0.4 rps / 2.5 sec per query 75mb - out of memory 110mb - out of memory 125mb - out of memory 135mb - 0.33 rps / 3 sec per query --> ### All in Memory In the first experiment, we tested how well our system performs when all vectors are stored in memory. We tried using different amounts of memory, ranging from 1512mb to 1024mb, and measured the number of requests per second (rps) that our system was able to handle. | Memory | Requests/s | |--------|---------------| | 1512mb | 774.38 | | 1256mb | 760.63 | | 1200mb | 794.72 | | 1152mb | out of memory | | 1024mb | out of memory | We found that 1152Mb memory limit resulted in our system running out of memory, but using 1512mb, 1256mb, and 1200mb of memory resulted in our system being able to handle around 780 RPS. This suggests that about 1.2Gb of memory is needed to serve around 1 million vectors, and there is no speed degradation when limiting memory usage above 1.2Gb. ### Vectors stored using MMAP Let\'s go a bit further! In the second experiment, we tested how well our system performs when **vectors are stored using the memory-mapped file** (mmap). Create collection with: ```http PUT /collections/benchmark { ... ""optimizers_config"": { ""mmap_threshold_kb"": 20000 } } ``` This configuration tells Qdrant to use mmap for vectors if the segment size is greater than 20000Kb (which is approximately 40K 128d-vectors). Now the out-of-memory happens when we allow using **600mb** RAM only <details> <summary>Experiments details</summary> | Memory | Requests/s | |--------|---------------| | 1200mb | 759.94 | | 1100mb | 687.00 | | 1000mb | 10 | --- use a bit faster disk --- | Memory | Requests/s | |--------|---------------| | 1000mb | 25 rps | | 750mb | 5 rps | | 625mb | 2.5 rps | | 600mb | out of memory | </details> <br/> At this point we have to switch from network-mounted storage to a faster disk, as the network-based storage is too slow to handle the amount of sequential reads that our system needs to serve the queries. But let\'s first see how much RAM we need to serve 1 million vectors and then we will discuss the speed optimization as well. ### Vectors and HNSW graph stored using MMAP In the third experiment, we tested how well our system performs when vectors and HNSW graph are stored using the memory-mapped files. Create collection with: ```http PUT /collections/benchmark { ... ""hnsw_config"": { ""on_disk"": true }, ""optimizers_config"": { ""mmap_threshold_kb"": 20000 } } ``` With this configuration we are able to serve 1 million vectors with **only 135mb of RAM**! <details> <summary>Experiments details</summary> | Memory | Requests/s | |--------|---------------| | 600mb | 5 rps | | 300mb | 0.9 rps / 1.1 sec per query | | 150mb | 0.4 rps / 2.5 sec per query | | 135mb | 0.33 rps / 3 sec per query | | 125mb | out of memory ,source:articles/memory-consumption.md']","In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.",0.8958794509559264,0,0.9999999999,0.8900462038973831,0.6,0.9999999999,1.0,0.034782608695652174,True
2,2,What is difference between scalar and product quantization?,"Scalar quantization and product quantization are two different methods used in quantization for high-dimensional vectors in Qdrant.

Scalar quantization, as described in the documentation, is a compression technique that converts floating point values into integers, specifically converting `float32` into `int8`. This conversion reduces the memory footprint by a factor of 4 and speeds up the search process. However, it comes with a trade-off of a slight loss in accuracy due to the conversion process.

On the other hand, product quantization offers a higher compression rate compared to scalar quantization. It is favored in scenarios where memory footprint is a top priority and search speed is not critical. Product quantization may reduce memory usage and search time, but it also introduces trade-offs in accuracy and in-RAM search speed.

In summary, scalar quantization is more balanced in terms of accuracy, speed, and compression, while product quantization is preferred when memory reduction is crucial and search speed is not a critical factor.

For more detailed information, you can refer to the documentation on [Scalar Quantization](/documentation/guides/quantization/#setting-up-scalar-quantization) and [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) in the Qdrant documentation.","['document:s</td> <td>474 s</td> </tr> </tbody> </table> It turns out that in some cases, Product Quantization may not only reduce the memory usage, but also the search time. ## Good practices Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed. Product Quantization tends to be favored in certain specific scenarios: - Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself - Situations where the dimensionality of the original vectors is sufficiently high - Cases where indexing speed is not a critical factor In circumstances that do not align with the above, Scalar Quantization should be the preferred choice. Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) will help you to set and configure the new quantization for your data and achieve even up to 64x memory reduction.,source:articles/product-quantization.mddocument:section for more information on how to optimize the quantization parameters for your use case. ## How to choose the right quantization method Here is a brief table of the pros and cons of each quantization method: | Quantization method | Accuracy | Speed | Compression | |---------------------|----------|--------------|-------------| | Scalar | 0.99 | up to x2 | 4 | | Product | 0.7 | 0.5 | up to 64 | | Binary | 0.95* | up to x40 | 32 | `*` - for compatible models * **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only. * **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable. * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical. ## Setting up Quantization in Qdrant You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration. Quantization will be automatically applied to all vectors during the indexation process. Quantized vectors are stored alongside the original vectors in the collection, so you will still have access to the original vectors if you need them. *Available as of v1.1.1* The `quantization_config` can also be set on a per vector basis by specifying it in a named vector. ### Setting up Scalar Quantization To enable scalar quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""quantile"": 0.99, ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, quantile=0.99, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { scalar: { type: ""int8"", quantile: 0.99, always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), quantile: Some(0.99), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.99f) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { ,source:documentation/guides/quantization.mddocument:--- title: ""Qdrant under the hood: Scalar Quantization"" short_description: ""Scalar Quantization is a newly introduced mechanism of reducing the memory footprint and increasing performance"" description: ""Scalar Quantization is a newly introduced mechanism of reducing the memory footprint and increasing performance"" social_preview_image: /articles_data/scalar-quantization/social_preview.png small_preview_image: /articles_data/scalar-quantization/scalar-quantization-icon.svg preview_dir: /articles_data/scalar-quantization/preview weight: 5 author: Kacper Łukawski author_link: https://medium.com/@lukawskikacper date: 2023-03-27T10:45:00+01:00 draft: false keywords: - vector search - scalar quantization - memory optimization --- High-dimensional vector embeddings can be memory-intensive, especially when working with large datasets consisting of millions of vectors. Memory footprint really starts being a concern when we scale things up. A simple choice of the data type used to store a single number impacts even billions of numbers and can drive the memory requirements crazy. The higher the precision of your type, the more accurately you can represent the numbers. The more accurate your vectors, the more precise is the distance calculation. But the advantages stop paying off when you need to order more and more memory. Qdrant chose `float32` as a default type used to store the numbers of your embeddings. So a single number needs 4 bytes of the memory and a 512-dimensional vector occupies 2 kB. That\'s only the memory used to store the vector. There is also an overhead of the HNSW graph, so as a rule of thumb we estimate the memory size with the following formula: ```text memory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes ``` While Qdrant offers various options to store some parts of the data on disk, starting from version 1.1.0, you can also optimize your memory by compressing the embeddings. We\'ve implemented the mechanism of **Scalar Quantization**! It turns out to have not only a positive impact on memory but also on the performance. ## Scalar Quantization Scalar quantization is a data compression technique that converts floating point values into integers. In case of Qdrant `float32` gets converted into `int8`, so a single number needs 75% less memory. It\'s not a simple rounding though! It\'s a process that makes that transformation partially reversible, so we can also revert integers back to floats with a small loss of precision. ### Theoretical background Assume we have a collection of `float32` vectors and denote a single value as `f32`. In reality neural embeddings do not cover a whole range represented by the floating point numbers, but rather a small subrange. Since we know all the other vectors, we can establish some statistics of all the numbers. For example, the distribution of the values will be typically normal: ![A distribution of the vector values](/articles_data/scalar-quantization/float32-distribution.png) Our example shows that 99% of the values come from a `[-2.0, 5.0]` range. And the conversion to `int8` will surely lose some precision, so we rather prefer keeping the representation accuracy within the range of 99% of the most probable values and ignoring the precision of the outliers. There might be a different choice of the range width, actually, any value from a range `[0, 1]`, where `0` means empty range, and `1` would keep all the values. That\'s a hyperparameter of ,source:articles/scalar-quantization.mddocument:--- title: Quantization weight: 120 aliases: - ../quantization --- # Quantization Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors. By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors. Different quantization methods have different mechanics and tradeoffs. We will cover them in this section. Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed. There are tradeoffs associated with quantization. On the one hand, quantization allows for significant reductions in storage requirements and faster search times. This can be particularly beneficial in large-scale applications where minimizing the use of resources is a top priority. On the other hand, quantization introduces an approximation error, which can lead to a slight decrease in search quality. The level of this tradeoff depends on the quantization method and its parameters, as well as the characteristics of the data. ## Scalar Quantization *Available as of v1.1.0* Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component. For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8. In other words, Qdrant performs `float32 -> uint8` conversion for each vector component. Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4. In addition to reducing the memory footprint, scalar quantization also speeds up the search process. Qdrant uses a special SIMD CPU instruction to perform fast vector comparison. This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster. The main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality. However, this error is usually negligible, and tends to be less significant for high-dimensional vectors. In our experiments, we found that the error introduced by scalar quantization is usually less than 1%. However, this value depends on the data and the quantization parameters. Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case. ## Binary Quantization *Available as of v1.5.0* Binary quantization is an extreme case of scalar quantization. This feature lets you represent each vector component as a single bit, effectively reducing the memory footprint by a **factor of 32**. This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions. Binary quantization can achieve up to a **40x** speedup compared to the original vectors. However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. At the moment, binary ,source:documentation/guides/quantization.mddocument:We enabled scalar quantization + HNSW with m=16 and ef_construct=512. We do the quantization in RAM, HNSW in RAM but keep the original vectors on disk (which was a network drive rented from Hetzner for the benchmark). If you want to reproduce the benchmarks, you can get snapshots containing the datasets: * [mmap only](https://storage.googleapis.com/common-datasets-snapshots/laion-768-6m-mmap.snapshot) * [with scalar quantization](https://storage.googleapis.com/common-datasets-snapshots/laion-768-6m-sq-m16-mmap.shapshot) Running the benchmark, we get the following IOPS, CPU loads and wall clock times: | | oversampling | parallel | ~max IOPS | CPU% (of 4 cores) | time (s) (avg of 3) | |----------|--------------|----------|-----------|-------------------|---------------------| | io_uring | 1 | 4 | 4000 | 200 | 12 | | mmap | 1 | 4 | 2000 | 93 | 43 | | io_uring | 1 | 8 | 4000 | 200 | 12 | | mmap | 1 | 8 | 2000 | 90 | 43 | | io_uring | 4 | 8 | 7000 | 100 | 30 | | mmap | 4 | 8 | 2300 | 50 | 145 | Note that in this case, the IO operations have relatively high latency due to using a network disk. Thus, the kernel takes more time to fulfil the mmap requests, and application threads need to wait, which is reflected in the CPU percentage. On the other hand, with the io\\_uring backend, the application threads can better use available cores for the rescore operation without any IO-induced delays. Oversampling is a new feature to improve accuracy at the cost of some performance. It allows setting a factor, which is multiplied with the `limit` while doing the search. The results are then re-scored using the original vector and only then the top results up to the limit are selected. ## Discussion Looking back, disk IO used to be very serialized; re-positioning read-write heads on moving platter was a slow and messy business. So the system overhead didn\'t matter as much, but nowadays with SSDs that can often even parallelize operations while offering near-perfect random access, the overhead starts to become quite visible. While memory-mapped IO gives us a fair deal in terms of ease of use and performance, we can improve on the latter in exchange for some modest complexity increase. io\\_uring is still quite young, having only been introduced in 2019 with kernel 5.1, so some administrators will be wary of introducing it. Of course, as with performance, the right answer is usually ""it depends"", so please review your personal risk profile and act accordingly. ## Best Practices If your on-disk collection\'s query performance is of sufficiently high priority to you, enable the io\\_uring-based async\\_scorer to greatly reduce operating system overhead from disk IO. On the other hand, if your collections are in memory only, activating it will be ineffective. Also note that many queries are not IO bound, so the overhead may or may not become measurable in your workload. Finally, on-device disks typically carry lower latency than network drives, which may also affect mmap overhead. Therefore before you roll out io\\_uring, perform the above or a similar benchmark with both mmap ,source:articles/io_uring.md']","While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.",0.8644671701264005,0,0.9999999999,0.6411640517393696,1.0,0.9999999999,1.0,0.008695652173913044,True
3,3,What is ‘best_score’ strategy?,"The 'best_score' strategy is a new recommendation strategy introduced in version 1.6.0. It focuses on finding similar vectors that are closer to a positive example while avoiding those closer to a negative example. This strategy calculates the best positive and best negative scores for each candidate point and selects the final score based on these scores. If the best positive score is higher than the best negative score, the candidate point is preferred. However, if the best negative score is higher, the candidate point is penalized by taking the negative squared value of the best negative score. This strategy aims to pull the traversal procedure away from negative examples, ultimately improving the quality of recommendations. You can set the strategy to 'best_score' in the recommendation request to utilize this approach (source: documentation/concepts/explore.md).","['document:step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation. The way to produce the searching vector is by first averaging all the positive and negative examples separately, and then combining them into a single vector using the following formula: ```rust avg_positive + avg_positive - avg_negative ``` In the case of not having any negative examples, the search vector will simply be equal to `avg_positive`. This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request. ### Best score strategy *Available as of v1.6.0* A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one. The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula: ```rust let score = if best_positive_score > best_negative_score { best_positive_score; } else { -(best_negative_score * best_negative_score); }; ``` <aside role=""alert""> The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples. </aside> Since we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic. <aside role=""status""> Accuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code> </aside> To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request. #### Using only negative examples A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one. Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning. ### Multiple vectors *Available as of v0.10.0* If the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request: ```http POST /collections/{collection_name}/points/recommend { ""positive"": [100, 231], ""negative"": [718], ""using"": ""image"", ""limit"": 10 } ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, }); ``` ```rust use qdrant_client::qdrant::RecommendPoints; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, ..Default::default() }) .await?; ``` ```java import java.util.List; ,source:documentation/concepts/explore.mddocument:for choosing the best match, which is also true in the case of vector recommendations. We can use different approaches to determine the path of traversing the HNSW graph by changing how we calculate the score of a candidate point during traversal. The default behaviour is based on pure distance, but Qdrant 1.6 exposes two strategies for the recommendation API. ### Average vector The default strategy, called `average_vector` is the previous one, based on the average of positive and negative examples. It simplifies the recommendations process and converts it into a single vector search. It supports both point IDs and vectors as parameters. For example, you can get recommendations based on past interactions with existing points combined with query vector embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the following formula: $$ \\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right) $$ The `average_vector` converts the problem of recommendations into a single vector search. ### The new hotness - Best score The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative samples and uses a slightly more sophisticated algorithm under the hood. The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a bunch of positive and negative queries**. As a result, for each sample in the query, we have a set of distances, one for each sample. In the next step, we simply take the best scores for positives and negatives, creating two separate values. Best scores are just the closest distances of a query to positives and negatives. The idea is: **if a point is closer to any negative than to any positive example, we do not want it**. We penalize being close to the negatives, so instead of using the similarity value directly, we check if it’s closer to positives or negatives. The following formula is used to calculate the score of a traversed potential point: ```rust if best_positive_score > best_negative_score { score = best_positive_score } else { score = -(best_negative_score * best_negative_score) } ``` If the point is closer to the negatives, we penalize it by taking the negative squared value of the best negative score. For a closer negative, the score of the candidate point will always be lower or equal to zero, making the chances of choosing that point significantly lower. However, if the best negative score is higher than the best positive score, we still prefer those that are further away from the negatives. That procedure effectively **pulls the traversal procedure away from the negative examples**. If you want to know more about the internals of HNSW, you can check out the article about the [Filtrable HNSW](https://qdrant.tech/articles/filtrable-hnsw/) that covers the topic thoroughly. ## Food Discovery demo Our [Food Discovery demo](https://qdrant.tech/articles/food-discovery-demo/) is an application built on ,source:articles/new-recommendation-api.mddocument:strategy. Mikko Lehtimäki: So when you get the query from the user, for example, you want to take different steps in order to answer it. For example, you may want to decompose the query to small questions that you answer individually, and each individual question may take a different path. So you may want to do a query based on metadata, for example pages five and six from a document. Or you may want to look up based on keywords full each page or chunk with a specific word. And there\'s really like a massive amount of choices how this can go. Another example is generating hypothetical documents based on the query and embedding those rather than the query itself. That will in some cases lead to higher quality retrieval results. But now all this leads into the right side of the query path. Mikko Lehtimäki: So here we have a re ranker. So if we implement all of this, we end up really retrieving a lot of data. We typically will retrieve more than it makes sense to give to the language model in a single call. So we can add a re ranker step here and it will firstly filter out low quality retrieved content and secondly, it will put the higher quality content on the top of the retrieved documents. And now when you pass this reranked content to the language model, it should be able to pay better attention to the details that actually matter given the query. And this should lead to you better managing the amount of data that you have to handle with your final response generator, LLM. And it should also make the response generator a bit faster because you will be feeding slightly less data in one go. The simplest way to build a re ranker is probably just asking a large language model to re rank or summarize the content that you\'ve retrieved before you feed it to the language model. Mikko Lehtimäki: That\'s one way to do it. So yeah, that\'s a lot of complexity and honestly, we\'re not doing all of this right now with Yokot AI, either. We\'ve tried all of it in different scopes, but really it\'s a lot of logic to maintain. And to me this just like screams the bitter lesson, because we\'re building so many steps, so much logic, so many rules into the system, when really all of this is done just because the language model can\'t be trusted, or it can\'t be with the current architectures trained reliably, or cannot be trained in real time with the current approaches that we have. So there\'s one thing in this picture, in my opinion, that is more promising than the others for leveraging data and compute, which should dominate the quality of the solution in the long term. And if we focus only on that, or not only, but if we focus heavily on that part of the process, we should be able to eliminate some complexity elsewhere. So if you\'re watching the recording, you ,source:blog/the-bitter-lesson-of-retrieval-in-generative-language-model-workflows-mikko-lehtimäki-vector-space-talks.mddocument:Contrastive Loss not only mathematically but also in its sample selection strategies, and I implemented the batch-all strategy for online triplet mining in this post efficiently by using several tricks. There are other more complicated strategies such as batch-hard and batch-semihard mining, but their implementations, and discussions of the tricks I used for efficiency in this post, are worth separate posts of their own. The future posts will cover such topics and additional discussions on some tricks to avoid vector collapsing and control intra-class and inter-class variance.,source:articles/triplet-loss.mddocument:] ) # Out: [ # [ScoredPoint(id=2, version=0, score=1.9, # payload=None, vector=None)], # [ScoredPoint(id=3, version=0, score=0.09, # payload=None, vector=[0.0, 0.0, 0.0, 0.1]), # ScoredPoint(id=1, version=0, score=0.10049876, # payload=None, vector=[0.0, 0.1, 0.0, 0.0])] # ] ``` Each instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them. And that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood. ## Benchmark The batch search is fairly easy to be integrated into your application, but if you prefer to see some numbers before deciding to switch, then it’s worth comparing four different options: 1. Querying the database sequentially. 2. Using many threads/processes with individual requests. 3. Utilizing the batch search of Qdrant in a single request. 4. Combining parallel processing and batch search. In order to do that, we’ll create a richer collection of points, with vectors from the\xa0*glove-25-angular*\xa0dataset, quite a common choice for ANN comparison. If you’re interested in seeing some more details of how we benchmarked Qdrant, let’s take a\xa0[look at the Gist](https://gist.github.com/kacperlukawski/2d12faa49e06a5080f4c35ebcb89a2a3). ## The results We launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts: 1. Sequential search: 225.9 seconds 2. Batch search: 208.0 seconds 3. Multiprocessing search (8 processes): 194.2 seconds 4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time. Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests. ## Summary Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!,source:blog/batch-vector-search-with-qdrant.md']","The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.",0.9737617808900074,0,0.9999999999,0.8533026313578769,1.0,0.9999999999,1.0,0.01,True
4,4,How does oversampling helps?,"Oversampling helps to control the precision of search in real-time by internally retrieving more vectors than needed from quantized storage. This allows for a more precise score during rescoring with original vectors, leading to the selection of only those vectors that are good for the user. By oversampling, accuracy can be controlled without rebuilding the index, simply by adjusting a parameter in the search query itself. This technique is explained in detail by Andrey Vasnetsov in the documentation on binary quantization and vector space talk (source: blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md).","['document:if we\'re using smaller models that aren\'t the GBTs, will that help? Andrey Vasnetsov: Right. So not all models are as big as OpenAI, but what we see, the trend in this area, the trend of development of different models, indicates that they will become bigger and bigger over time. Just because we want to store more information inside vectors, we want to have larger context, we want to have more detailed information, more detailed separation and so on. This trend is obvious if like five years ago the usual size of the vector was 100 dimensions now the usual size is 700 dimensions, so it\'s basically. Demetrios: Preparing for the future while also optimizing for today. Andrey Vasnetsov: Right? Demetrios: Yeah. Okay, so you mentioned on here oversampling. Can you go into that a little bit more and explain to me what that is? Andrey Vasnetsov: Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user. And that\'s how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself. Demetrios: I see, okay, so basically this is the quantization. And now let\'s dive into the binary quantization and how it works. Andrey Vasnetsov: Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it\'s either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever. It\'s Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it\'s very fast. Andrey Vasnetsov: It uses very few CPU cycles to actually produce a result. That\'s why binary quantization is over 30 times faster than regular product. And it actually solves the problem of complicated index building, because this computation of dot products is the main source of computational requirements for HNSW. Demetrios: So if I\'m understanding this correctly, it\'s basically taking all of these numbers that are on the left, which can be, yes, decimal numbers. Andrey Vasnetsov: On the left you can see original vector and it converts it in binary representation. And of course it does lose a lot of precision in the process. But because first we have very large vector and second, we have oversampling feature, we can compensate for ,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.mddocument:just the dominant one. Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects. ![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png) ### Leveraging Binary Quantization: Best Practices We recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings: 1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested. 2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages. 3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications. 4. Rescoring: Enable rescoring to improve the accuracy of search results. 5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible. Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant) Learn more about how to boost your vector search speed and accuracy while reducing costs: [Binary Quantization.](https://qdrant.tech/documentation/guides/quantization/?selector=aHRtbCA%2BIGJvZHkgPiBkaXY6bnRoLW9mLXR5cGUoMSkgPiBzZWN0aW9uID4gZGl2ID4gZGl2ID4gZGl2Om50aC1vZi10eXBlKDIpID4gYXJ0aWNsZSA%2BIGgyOm50aC1vZi10eXBlKDIp),source:articles/binary-quantization-openai.mddocument:an oversampling of 3 and a limit of 100, we\'ve a 95% recall against the exact nearest neighbors with rescore enabled. | oversampling | | 1 | 1 | 2 | 2 | 3 | 3 | |--------------|---------|----------|----------|----------|----------|----------|----------| | limit | | | | | | | | | | rescore | False | True | False | True | False | True | | 10 | | 0.523333 | 0.831111 | 0.523333 | 0.915556 | 0.523333 | 0.950000 | | 20 | | 0.510000 | 0.836667 | 0.510000 | 0.912222 | 0.510000 | 0.937778 | | 50 | | 0.489111 | 0.841556 | 0.489111 | 0.913333 | 0.488444 | 0.947111 | | 100 | | 0.485778 | 0.846556 | 0.485556 | 0.929000 | 0.486000 | **0.956333** | That\'s it! You can now use Gemini Embedding Models with Qdrant!,source:documentation/embeddings/gemini.mddocument:leaving those two parameters out of the search query. ## Benchmark results We retrieved some early results on the relationship between limit and oversampling using the the DBPedia OpenAI 1M vector dataset. We ran all these experiments on a Qdrant instance where 100K vectors were indexed and used 100 random queries. We varied the 3 parameters that will affect query time and accuracy: limit, rescore and oversampling. We offer these as an initial exploration of this new feature. You are highly encouraged to reproduce these experiments with your data sets. > Aside: Since this is a new innovation in vector databases, we are keen to hear feedback and results. [Join our Discord server](https://discord.gg/Qy6HCJK9Dc) for further discussion! **Oversampling:** In the figure below, we illustrate the relationship between recall and number of candidates: ![Correct vs candidates](/articles_data/binary-quantization/bq-5.png) We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0 As oversampling increases, we see a general improvement in results – but that does not hold in every case. **Rescore:** As expected, rescoring increases the time it takes to return a query. We also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. ![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png) **Limit:** We experiment with limits from Top 1 to Top 50 and we are able to get to 100% recall at limit 50, with rescore=True, in an index with 100K vectors. ## Recommendations Quantization gives you the option to make tradeoffs against other parameters: Dimension count/embedding size Throughput and Latency requirements Recall requirements If you\'re working with OpenAI or Cohere embeddings, we recommend the following oversampling settings: |Method|Dimensionality|Test Dataset|Recall|Oversampling| |-|-|-|-|-| |OpenAI text-embedding-ada-002|1536|[DbPedia](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) 1M|0.98|4x| |Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x| If you determine that binary quantization is appropriate for your datasets and queries then we suggest the following: - Binary Quantization with always_ram=True - Vectors stored on disk - Oversampling=2.0 (or more) - Rescore=True ## What\'s next? Binary quantization is exceptional if you need to work with large volumes of data under high recall expectations. You can try this feature either by spinning up a [Qdrant container image](https://hub.docker.com/r/qdrant/qdrant) locally or, having us create one for you through a [free account](https://cloud.qdrant.io/login) in our cloud hosted service. The article gives examples of data sets and configuration you can use to get going. Our documentation covers [adding the data](/documentation/tutorials/bulk-upload/) to your Qdrant instance as well as [creating your indices](/documentation/tutorials/optimize/). If you have any feedback, drop us a note on Twitter or LinkedIn to tell us about your results. [Join our lively Discord Server](https://discord.gg/Qy6HCJK9Dc) if you want to discuss BQ with like-minded people!,source:articles/binary-quantization.mddocument:quantization method, since it lets you perform a vector comparison with a few CPU instructions. Binary quantization can achieve up to a **40x** speedup compared to the original vectors. However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. At the moment, binary quantization shows good accuracy results with the following models: - OpenAI `text-embedding-ada-002` - 1536d tested with [dbpedia dataset](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) achieving 0.98 recall@100 with 4x oversampling - Cohere AI `embed-english-v2.0` - 4096d tested on [wikipedia embeddings](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) - 0.98 recall@50 with 2x oversampling Models with a lower dimensionality or a different distribution of vector components may require additional experiments to find the optimal quantization parameters. We recommend using binary quantization only with rescoring enabled, as it can significantly improve the search quality with just a minor performance impact. Additionally, oversampling can be used to tune the tradeoff between search speed and search quality in the query time. ### Binary Quantization as Hamming Distance The additional benefit of this method is that you can efficiently emulate Hamming distance with dot product. Specifically, if original vectors contain `{-1, 1}` as possible values, then the dot product of two vectors is equal to the Hamming distance by simply replacing `-1` with `0` and `1` with `1`. <!-- hidden section --> <details> <summary><b>Sample truth table</b></summary> | Vector 1 | Vector 2 | Dot product | |----------|----------|-------------| | 1 | 1 | 1 | | 1 | -1 | -1 | | -1 | 1 | -1 | | -1 | -1 | 1 | | Vector 1 | Vector 2 | Hamming distance | |----------|----------|------------------| | 1 | 1 | 0 | | 1 | 0 | 1 | | 0 | 1 | 1 | | 0 | 0 | 0 | </details> As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent. Binary quantization makes it efficient to compare vectors using this representation. ## Product Quantization *Available as of v1.2.0* Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into chunks and quantizing each segment individually. Each chunk is approximated by a centroid index that represents the original vector component. The positions of the centroids are determined through the utilization of a clustering algorithm such as k-means. For now, Qdrant uses only 256 centroids, so each centroid index can be represented by a single byte. Product quantization can compress by a more prominent factor than a scalar one. But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization. Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors. Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case. ## How to choose the right quantization method Here is a brief table of the pros and cons of each quantization method: | Quantization method | Accuracy | Speed | Compression | |---------------------|----------|--------------|-------------| | Scalar ,source:documentation/guides/quantization.md']","Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.",0.8970753701856654,0,0.9999999999,0.8032537374860962,1.0,0.9999999999,0.6666666666666666,0.01020408163265306,True
5,5,What is the purpose of ‘CreatePayloadIndexAsync’?,"The purpose of the 'CreatePayloadIndexAsync' function is to create a keyword payload index for a specific field in a collection. This function allows you to define the field name and field schema (in this case, keyword) for indexing purposes. By using this function, you can efficiently manage and index payload data within your collection. You can refer to the relevant source documentation for more details on how to use this function effectively. 
[source: documentation/concepts/collections.md]","['document:Some(Operation::SetPayload(SetPayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), payload: HashMap::from([ (""test_payload_2"".to_string(), 2.into()), (""test_payload_3"".to_string(), 3.into()), ]), })), }, PointsUpdateOperation { operation: Some(Operation::DeletePayload(DeletePayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), keys: vec![""test_payload_2"".to_string()], })), }, PointsUpdateOperation { operation: Some(Operation::ClearPayload(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, PointsUpdateOperation { operation: Some(Operation::Delete(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, ], None, ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.PointVectors; import io.qdrant.client.grpc.Points.PointsIdsList; import io.qdrant.client.grpc.Points.PointsSelector; import io.qdrant.client.grpc.Points.PointsUpdateOperation; import io.qdrant.client.grpc.Points.PointsUpdateOperation.ClearPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePoints; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeleteVectors; import io.qdrant.client.grpc.Points.PointsUpdateOperation.PointStructList; import io.qdrant.client.grpc.Points.PointsUpdateOperation.SetPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.UpdateVectors; import io.qdrant.client.grpc.Points.VectorsSelector; client .batchUpdateAsync( ""{collection_name}"", List.of( PointsUpdateOperation.newBuilder() .setUpsert( PointStructList.newBuilder() .addPoints( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setUpdateVectors( UpdateVectors.newBuilder() .addPoints( PointVectors.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeleteVectors( DeleteVectors.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .setVectors(VectorsSelector.newBuilder().addNames("""").build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setOverwritePayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload(Map.of(""test_payload"", value(1))) .build()) .build(), PointsUpdateOperation.newBuilder() .setSetPayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload( Map.of(""test_payload_2"", value(2), ""test_payload_3"", value(3))) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePayload( DeletePayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .addKeys(""test_payload_2"") .build()) .build(), PointsUpdateOperation.newBuilder() .setClearPayload( ClearPayload.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePoints( DeletePoints.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build())) .get(); ``` To batch many points with a single operation type, please use batching functionality in that operation directly.,source:documentation/concepts/points.mddocument:```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""create_alias"": { ""collection_name"": ""test_collection1"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.CreateAliasOperation( create_alias=models.CreateAlias( collection_name=""example_collection"", alias_name=""production_collection"" ) ) ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { create_alias: { collection_name: ""example_collection"", alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.create_alias(""example_collection"", ""production_collection"").await?; ``` ```java client.createAliasAsync(""production_collection"", ""example_collection"").get(); ``` ```csharp await client.CreateAliasAsync(aliasName: ""production_collection"", collectionName: ""example_collection""); ``` ### Remove alias ```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""delete_alias"": { ""collection_name"": ""test_collection1"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```http POST /collections/aliases { ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } } ] } ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.DeleteAliasOperation( delete_alias=models.DeleteAlias(alias_name=""production_collection"") ), ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { delete_alias: { alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.delete_alias(""production_collection"").await?; ``` ```java client.deleteAliasAsync(""production_collection"").get(); ``` ```csharp await client.DeleteAliasAsync(""production_collection""); ``` ### Switch collection Multiple alias actions are performed atomically. For example, you can switch underlying collection with the following command: ```http POST /collections/aliases { ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } }, { ""create_alias"": { ""collection_name"": ""test_collection2"", ""alias_name"": ""production_collection"" } } ] } ``` ```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } }, { ""create_alias"": { ""collection_name"": ""test_collection2"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.DeleteAliasOperation( delete_alias=models.DeleteAlias(alias_name=""production_collection"") ), models.CreateAliasOperation( create_alias=models.CreateAlias( collection_name=""example_collection"", alias_name=""production_collection"" ) ), ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { delete_alias: { alias_name: ""production_collection"", }, }, { create_alias: { collection_name: ""example_collection"", alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.delete_alias(""production_collection"").await?; client.create_alias(""example_collection"", ""production_collection"").await?; ``` ```java client.deleteAliasAsync(""production_collection"").get(); client.createAliasAsync(""production_collection"", ""example_collection"").get(); ``` ```csharp await client.DeleteAliasAsync(""production_collection""); await client.CreateAliasAsync(aliasName: ""production_collection"", collectionName: ""example_collection""); ``` ### List collection aliases ```http GET /collections/test_collection2/aliases ``` ```bash curl -X GET http://localhost:6333/collections/test_collection2/aliases ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_collection_aliases(collection_name=""{collection_name}"") ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getCollectionAliases(""{collection_name}""); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_collection_aliases(""{collection_name}"").await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listCollectionAliasesAsync(""{collection_name}"").get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.ListCollectionAliasesAsync(""{collection_name}""); ``` ### List all aliases ```http GET /aliases ``` ```bash curl -X GET http://localhost:6333/aliases ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_aliases() ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getAliases(); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_aliases().await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listAliasesAsync().get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.ListAliasesAsync(); ``` ### List all collections ```http GET /collections ``` ```bash curl -X GET http://localhost:6333/collections ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_collections() ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getCollections(); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_collections().await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listCollectionsAsync().get(); ``` ```csharp using Qdrant.Client; var client ,source:documentation/concepts/collections.mddocument:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.mddocument:.await?; ``` ```java import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient( QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.createCollectionAsync(""{collection_name}"", VectorParams.newBuilder().setDistance(Distance.Cosine).setSize(100).build()).get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine } ); ``` In addition to the required options, you can also specify custom values for the following collection options: * `hnsw_config` - see [indexing](../indexing/#vector-index) for details. * `wal_config` - Write-Ahead-Log related configuration. See more details about [WAL](../storage/#versioning) * `optimizers_config` - see [optimizer](../optimizer) for details. * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details. * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload. * `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details. Default parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). See [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#operation/create_collection) and a [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) for more information about collection and vector parameters. *Available as of v1.2.0* Vectors all live in RAM for very quick access. The `on_disk` parameter can be set in the vector configuration. If true, all vectors will live on disk. This will enable the use of [memmaps](../../concepts/storage/#configuring-memmap-storage), which is suitable for ingesting a large amount of data. ### Create collection from another collection *Available as of v1.0.0* It is possible to initialize a collection from another existing collection. This might be useful for experimenting quickly with different configurations for the same data set. Make sure the vectors have the same `size` and `distance` function when setting up the vectors configuration in the new collection. If you used the previous sample code, `""size"": 300` and `""distance"": ""Cosine""`. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 100, ""distance"": ""Cosine"" }, ""init_from"": { ""collection"": ""{from_collection_name}"" } } ``` ```bash curl -X PUT http://localhost:6333/collections/test_collection2 \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""vectors"": { ""size"": 300, ""distance"": ""Cosine"" }, ""init_from"": { ""collection"": ""test_collection1"" } }\' ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=100, distance=models.Distance.COSINE), init_from=models.InitFrom(collection=""{from_collection_name}""), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 100, distance: ""Cosine"" }, init_from: { collection: ""{from_collection_name}"" }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 100, distance: Distance::Cosine.into(), ..Default::default() })), }), init_from_collection: Some(""{from_collection_name}"".to_string()), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(100) .setDistance(Distance.Cosine) .build())) .setInitFromCollection(""{from_collection_name}"") .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine }, initFromCollection: ""{from_collection_name}"" ); ``` ### Collection with multiple vectors *Available as of v0.10.0* It is possible to have multiple vectors per record. This ,source:documentation/concepts/collections.mddocument:the AWS side, you need to setup a Lambda and IAM role to use with your function. ![create lambda web page](/articles_data/serverless/create_lambda.png) Choose your function name, select ""Provide your own bootstrap on Amazon Linux 2"". As architecture, use `arm64`. You will also activate a function URL. Here it is up to you if you want to protect it via IAM or leave it open, but be aware that open end points can be accessed by anyone, potentially costing money if there is too much traffic. By default, this will also create a basic role. To look up the role, you can go into the Function overview: ![function overview](/articles_data/serverless/lambda_overview.png) Click on the ""Info"" link near the ""▸ Function overview"" heading, and select the ""Permissions"" tab on the left. You will find the ""Role name"" directly under *Execution role*. Note it down for later. ![function overview](/articles_data/serverless/lambda_role.png) To test that your ""Hello, Lambda"" service works, you can compile and upload the function: ```bash $ export LAMBDA_FUNCTION_NAME=hello $ export LAMBDA_ROLE=<role name from lambda web ui> $ export LAMBDA_REGION=us-east-1 $ cargo lambda build --release --arm --bin helloworld --output-format zip Downloaded libc v0.2.137 # [..] output omitted for brevity Finished release [optimized] target(s) in 1m 27s $ # Delete the old empty definition $ aws lambda delete-function-url-config --region $LAMBDA_REGION --function-name $LAMBDA_FUNCTION_NAME $ aws lambda delete-function --region $LAMBDA_REGION --function-name $LAMBDA_FUNCTION_NAME $ # Upload the function $ aws lambda create-function --function-name $LAMBDA_FUNCTION_NAME \\ --handler bootstrap \\ --architectures arm64 \\ --zip-file fileb://./target/lambda/helloworld/bootstrap.zip \\ --runtime provided.al2 \\ --region $LAMBDA_REGION \\ --role $LAMBDA_ROLE \\ --tracing-config Mode=Active $ # Add the function URL $ aws lambda add-permission \\ --function-name $LAMBDA_FUNCTION_NAME \\ --action lambda:InvokeFunctionUrl \\ --principal ""*"" \\ --function-url-auth-type ""NONE"" \\ --region $LAMBDA_REGION \\ --statement-id url $ # Here for simplicity unauthenticated URL access. Beware! $ aws lambda create-function-url-config \\ --function-name $LAMBDA_FUNCTION_NAME \\ --region $LAMBDA_REGION \\ --cors ""AllowOrigins=*,AllowMethods=*,AllowHeaders=*"" \\ --auth-type NONE ``` Now you can go to your *Function Overview* and click on the Function URL. You should see something like this: ```text Hello, Lambda! ``` Bearer ! You have set up a Lambda function in Rust. On to the next ingredient: ## Embedding Most providers supply a simple https GET or POST interface you can use with an API key, which you have to supply in an authentication header. If you are using this for non-commercial purposes, the rate limited trial key from Cohere is just a few clicks away. Go to [their welcome page](https://dashboard.cohere.ai/welcome/register), register and you\'ll be able to get to the dashboard, which has an ""API keys"" menu entry which will bring you to the following page: [cohere dashboard](/articles_data/serverless/cohere-dashboard.png) From there you can click on the ⎘ symbol next to your API key to copy it to the clipboard. *Don\'t put your API key in the code!* Instead read it from an env variable you can set in the lambda environment. This avoids accidentally putting your key into a public repo. Now all you need to get embeddings is a bit of code. First you need to extend your dependencies with `reqwest` and also add `anyhow` for easier error handling: ```toml anyhow ,source:articles/serverless.md']",CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,0.9830132838921438,0,0.9999999999,0.7493610183489399,1.0,0.9999999999,0.3333333333333333,0.0055248618784530384,False
6,6,What is the purpose of ef_construct in HNSW ?,"The `ef_construct` parameter in HNSW (Hierarchical Navigable Small World Graph) serves the purpose of specifying the number of neighbors to consider during the index building process. A higher value for `ef_construct` leads to more accurate search results but requires more time to build the index. This parameter can be configured in the HNSW section of the configuration file, as shown below:

```yaml
hnsw_index:
  m: 16
  ef_construct: 100
  full_scan_threshold: 10000
```

You can adjust the `ef_construct` value based on your requirements for search accuracy and index building time. For more details, you can refer to the [Indexing documentation](documentation/concepts/indexing.md) page.","['document:), ) ``` ```typescript client.updateCollection(""{collection_name}"", { vectors: { my_vector: { hnsw_config: { m: 32, ef_construct: 123, }, quantization_config: { product: { compression: ""x32"", always_ram: true, }, }, on_disk: true, }, }, hnsw_config: { ef_construct: 123, }, quantization_config: { scalar: { type: ""int8"", quantile: 0.8, always_ram: true, }, }, }); ``` ```rust use qdrant_client::client::QdrantClient; use qdrant_client::qdrant::{ quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff, QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff, VectorsConfigDiff, }; client .update_collection( ""{collection_name}"", None, None, None, Some(&HnswConfigDiff { ef_construct: Some(123), ..Default::default() }), Some(&VectorsConfigDiff { config: Some(Config::ParamsMap( qdrant_client::qdrant::VectorParamsDiffMap { map: HashMap::from([( (""my_vector"".into()), VectorParamsDiff { hnsw_config: Some(HnswConfigDiff { m: Some(32), ef_construct: Some(123), ..Default::default() }), ..Default::default() }, )]), }, )), }), Some(&QuantizationConfigDiff { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8 as i32, quantile: Some(0.8), always_ram: Some(true), ..Default::default() })), }), ) .await?; ``` ```java import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.UpdateCollection; import io.qdrant.client.grpc.Collections.VectorParamsDiff; import io.qdrant.client.grpc.Collections.VectorParamsDiffMap; import io.qdrant.client.grpc.Collections.VectorsConfigDiff; client .updateCollectionAsync( UpdateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setHnswConfig(HnswConfigDiff.newBuilder().setEfConstruct(123).build()) .setVectorsConfig( VectorsConfigDiff.newBuilder() .setParamsMap( VectorParamsDiffMap.newBuilder() .putMap( ""my_vector"", VectorParamsDiff.newBuilder() .setHnswConfig( HnswConfigDiff.newBuilder() .setM(3) .setEfConstruct(123) .build()) .build()))) .setQuantizationConfig( QuantizationConfigDiff.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.8f) .setAlwaysRam(true) .build())) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpdateCollectionAsync( collectionName: ""{collection_name}"", hnswConfig: new HnswConfigDiff { EfConstruct = 123 }, vectorsConfig: new VectorParamsDiffMap { Map = { { ""my_vector"", new VectorParamsDiff { HnswConfig = new HnswConfigDiff { M = 3, EfConstruct = 123 } } } } }, quantizationConfig: new QuantizationConfigDiff { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, Quantile = 0.8f, AlwaysRam = true } } ); ``` ## Collection info Qdrant allows determining the configuration parameters of an existing collection to better understand how the points are distributed and indexed. ```http GET /collections/test_collection1 ``` ```bash curl -X GET http://localhost:6333/collections/test_collection1 ``` ```python client.get_collection(collection_name=""{collection_name}"") ``` ```typescript client.getCollection(""{collection_name}""); ``` ```rust client.collection_info(""{collection_name}"").await?; ``` ```java client.getCollectionInfoAsync(""{collection_name}"").get(); ``` <details> <summary>Expected result</summary> ```json { ""result"": { ""status"": ""green"", ""optimizer_status"": ""ok"", ""vectors_count"": 1068786, ""indexed_vectors_count"": 1024232, ""points_count"": 1068786, ""segments_count"": 31, ""config"": { ""params"": { ""vectors"": { ""size"": 384, ""distance"": ""Cosine"" }, ""shard_number"": 1, ""replication_factor"": 1, ""write_consistency_factor"": 1, ""on_disk_payload"": false }, ""hnsw_config"": { ""m"": 16, ""ef_construct"": 100, ""full_scan_threshold"": 10000, ""max_indexing_threads"": 0 }, ""optimizer_config"": { ""deleted_threshold"": 0.2, ""vacuum_min_vector_number"": 1000, ""default_segment_number"": 0, ""max_segment_size"": null, ""memmap_threshold"": null, ""indexing_threshold"": 20000, ""flush_interval_sec"": 5, ""max_optimization_threads"": 1 }, ""wal_config"": { ""wal_capacity_mb"": 32, ""wal_segments_ahead"": 0 } }, ""payload_schema"": {} }, ""status"": ""ok"", ""time"": 0.00010143 } ``` </details> <br/> ```csharp await client.GetCollectionInfoAsync(""{collection_name}""); ``` If you insert the vectors into the collection, the `status` field may become `yellow` whilst it is optimizing. It will become `green` once all the points are successfully processed. The following color statuses are possible: - 🟢 `green`: collection is ready - 🟡 `yellow`: collection is optimizing - 🔴 `red`: an error occurred which the engine could not recover from ### Approximate point and vector counts You may be interested in the count attributes: - `points_count` - total number of objects (vectors and their payloads) stored in the collection - `vectors_count` - total number of vectors in a collection, useful if you have multiple vectors per point - `indexed_vectors_count` - total number of vectors stored in the HNSW or sparse index. Qdrant does not store all the vectors in the index, but only if an ,source:documentation/concepts/collections.mddocument:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.mddocument:io.qdrant.client.grpc.Points.PointStruct; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .upsertAsync( ""{collection_name}"", List.of( PointStruct.newBuilder() .setId(id(UUID.fromString(""5c56c793-69f3-4fbf-87e6-c4bf54c28c26""))) .setVectors(vectors(0.05f, 0.61f, 0.76f, 0.74f)) .putAllPayload(Map.of(""color"", value(""Red""))) .build())) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = Guid.Parse(""5c56c793-69f3-4fbf-87e6-c4bf54c28c26""), Vectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f }, Payload = { [""city""] = ""red"" } } } ); ``` and ```http PUT /collections/{collection_name}/points { ""points"": [ { ""id"": 1, ""payload"": {""color"": ""red""}, ""vector"": [0.9, 0.1, 0.1] } ] } ``` ```python client.upsert( collection_name=""{collection_name}"", points=[ models.PointStruct( id=1, payload={ ""color"": ""red"", }, vector=[0.9, 0.1, 0.1], ), ], ) ``` ```typescript client.upsert(""{collection_name}"", { points: [ { id: 1, payload: { color: ""red"", }, vector: [0.9, 0.1, 0.1], }, ], }); ``` ```rust use qdrant_client::qdrant::PointStruct; use serde_json::json; client .upsert_points_blocking( 1, None, vec![PointStruct::new( ""5c56c793-69f3-4fbf-87e6-c4bf54c28c26"".to_string(), vec![0.05, 0.61, 0.76, 0.74], json!( {""color"": ""Red""} ) .try_into() .unwrap(), )], None, ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Points.PointStruct; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .upsertAsync( ""{collection_name}"", List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.05f, 0.61f, 0.76f, 0.74f)) .putAllPayload(Map.of(""color"", value(""Red""))) .build())) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f }, Payload = { [""city""] = ""red"" } } } ); ``` are both possible. ## Upload points To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call. Batching allows you to minimize the overhead of creating a network connection. The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. Create points with batch: ```http PUT /collections/{collection_name}/points { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, }); ``` or record-oriented equivalent: ```http PUT /collections/{collection_name}/points { ""points"": [ { ""id"": 1, ""payload"": {""color"": ""red""}, ""vector"": [0.9, 0.1, 0.1] }, { ""id"": 2, ""payload"": {""color"": ""green""}, ""vector"": [0.1, 0.9, 0.1] }, { ""id"": 3, ""payload"": {""color"": ""blue""}, ""vector"": [0.1, 0.1, 0.9] } ] } ``` ```python client.upsert( collection_name=""{collection_name}"", points=[ models.PointStruct( id=1, payload={ ""color"": ""red"", }, vector=[0.9, 0.1, 0.1], ), models.PointStruct( id=2, payload={ ""color"": ""green"", }, vector=[0.1, 0.9, 0.1], ), models.PointStruct( id=3, payload={ ""color"": ""blue"", }, vector=[0.1, 0.1, 0.9], ), ], ) ``` ```typescript client.upsert(""{collection_name}"", { points: [ { id: 1, payload: { color: ""red"" }, vector: [0.9, 0.1, 0.1], }, { id: ,source:documentation/concepts/points.mddocument:`0`. # If not set, the default value will be used. indexing_threshold_kb: 20000 # Interval between forced flushes. flush_interval_sec: 5 # Max number of threads, which can be used for optimization per collection. # Note: Each optimization thread will also use `max_indexing_threads` for index building. # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads` # If `max_optimization_threads = 0`, optimization will be disabled. max_optimization_threads: 1 # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually hnsw_index: # Number of edges per node in the index graph. Larger the value - more accurate the search, more space required. m: 16 # Number of neighbours to consider during the index building. Larger the value - more accurate the search, more time required to build index. ef_construct: 100 # Minimal size (in KiloBytes) of vectors for additional payload-based indexing. # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won\'t be used - # in this case full-scan search should be preferred by query planner and additional indexing is not required. # Note: 1Kb = 1 vector of size 256 full_scan_threshold_kb: 10000 # Number of parallel threads used for background index building. If 0 - auto selection. max_indexing_threads: 0 # Store HNSW index on disk. If set to false, index will be stored in RAM. Default: false on_disk: false # Custom M param for hnsw graph built for payload index. If not set, default M will be used. payload_m: null service: # Maximum size of POST data in a single request in megabytes max_request_size_mb: 32 # Number of parallel workers used for serving the api. If 0 - equal to the number of available cores. # If missing - Same as storage.max_search_threads max_workers: 0 # Host to bind the service on host: 0.0.0.0 # HTTP(S) port to bind the service on http_port: 6333 # gRPC port to bind the service on. # If `null` - gRPC is disabled. Default: null # Comment to disable gRPC: grpc_port: 6334 # Enable CORS headers in REST API. # If enabled, browsers would be allowed to query REST endpoints regardless of query origin. # More info: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS # Default: true enable_cors: true # Enable HTTPS for the REST and gRPC API enable_tls: false # Check user HTTPS client certificate against CA file specified in tls config verify_https_client_certificate: false # Set an api-key. # If set, all requests must include a header with the api-key. # example header: `api-key: <API-KEY>` # # If you enable this you should also enable TLS. # (Either above or via an external service like nginx.) # Sending an api-key over an unencrypted channel is insecure. # # Uncomment to enable. # api_key: your_secret_api_key_here # Set an api-key for read-only operations. # If set, all requests must include a header with the api-key. # example header: `api-key: <API-KEY>` # # If you enable this you should also enable TLS. # (Either above or via an external service like nginx.) # Sending an api-key over an unencrypted channel is insecure. # # Uncomment to enable. ,source:documentation/guides/configuration.mddocument:with full-text index. ## Vector Index A vector index is a data structure built on vectors through a specific mathematical model. Through the vector index, we can efficiently query several vectors similar to the target vector. Qdrant currently only uses HNSW as a dense vector index. [HNSW](https://arxiv.org/abs/1603.09320) (Hierarchical Navigable Small World Graph) is a graph-based indexing algorithm. It builds a multi-layer navigation structure for an image according to certain rules. In this structure, the upper layers are more sparse and the distances between nodes are farther. The lower layers are denser and the distances between nodes are closer. The search starts from the uppermost layer, finds the node closest to the target in this layer, and then enters the next layer to begin another search. After multiple iterations, it can quickly approach the target position. In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range. The corresponding parameters could be configured in the configuration file: ```yaml storage: # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually hnsw_index: # Number of edges per node in the index graph. # Larger the value - more accurate the search, more space required. m: 16 # Number of neighbours to consider during the index building. # Larger the value - more accurate the search, more time required to build index. ef_construct: 100 # Minimal size (in KiloBytes) of vectors for additional payload-based indexing. # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won\'t be used - # in this case full-scan search should be preferred by query planner and additional indexing is not required. # Note: 1Kb = 1 vector of size 256 full_scan_threshold: 10000 ``` And so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`. HNSW is chosen for several reasons. First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search. Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks). *Available as of v1.1.1* The HNSW parameters can also be configured on a collection and named vector level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search performance. ## Sparse Vector Index *Available as of v1.7.0* ### Key Features of Sparse Vector Index - **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes. - **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed. ### Search Mechanism - **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search. - **Scoring Method:** Vectors are scored using the dot product. ### Optimizations - **Reducing Vectors to Score:** Implementations are in place to minimize the number of vectors scored, especially for dimensions with numerous vectors. ### Filtering and Configuration - **Filtering Support:** Similar to dense vectors, supports filtering ,source:documentation/concepts/indexing.md']","In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100",0.9544418112980074,0,0.9999999999,0.5800240827742112,0.3333333333333333,0.9999999999,0.6666666666666666,0.006578947368421052,True
7,7,How do you use ‘ordering’ parameter?,"To use the 'ordering' parameter, you can specify the ordering type for write operations in Qdrant. There are two options available:

1. `medium` ordering: This option serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change.
2. `strong` ordering: This option serializes all write operations through the permanent leader, providing strong consistency. However, write operations may be unavailable if the leader is down.

You can set the 'ordering' parameter when performing write operations in different programming languages like Python, TypeScript, Rust, Java, and C#. Here is an example in Python:

```python
client.upsert(
    collection_name=""{collection_name}"",
    points=models.Batch(
        ids=[1, 2, 3],
        payloads=[{""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}],
        vectors=[[0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9]],
    ),
    ordering=""strong"",
)
```

This code snippet demonstrates how to use the 'ordering' parameter with the 'strong' option when upserting points in a collection. You can refer to the documentation for more details on using the 'ordering' parameter in different programming languages and scenarios.","['document:can be freely reordered. - `medium` ordering serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change. - `strong` ordering serializes all write operations through the permanent leader, which provides strong consistency, but write operations may be unavailable if the leader is down. ```http PUT /collections/{collection_name}/points?ordering=strong { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ordering=""strong"", ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, ordering: ""strong"", }); ``` ```rust use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType}; use serde_json::json; client .upsert_points_blocking( ""{collection_name}"", None, vec![ PointStruct::new( 1, vec![0.9, 0.1, 0.1], json!({ ""color"": ""red"" }) .try_into() .unwrap(), ), PointStruct::new( 2, vec![0.1, 0.9, 0.1], json!({ ""color"": ""green"" }) .try_into() .unwrap(), ), PointStruct::new( 3, vec![0.1, 0.1, 0.9], json!({ ""color"": ""blue"" }) .try_into() .unwrap(), ), ], Some(WriteOrdering { r#type: WriteOrderingType::Strong.into(), }), ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.UpsertPoints; import io.qdrant.client.grpc.Points.WriteOrdering; import io.qdrant.client.grpc.Points.WriteOrderingType; client .upsertAsync( UpsertPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPoints( List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.9f, 0.1f, 0.1f)) .putAllPayload(Map.of(""color"", value(""red""))) .build(), PointStruct.newBuilder() .setId(id(2)) .setVectors(vectors(0.1f, 0.9f, 0.1f)) .putAllPayload(Map.of(""color"", value(""green""))) .build(), PointStruct.newBuilder() .setId(id(3)) .setVectors(vectors(0.1f, 0.1f, 0.94f)) .putAllPayload(Map.of(""color"", value(""blue""))) .build())) .setOrdering(WriteOrdering.newBuilder().setType(WriteOrderingType.Strong).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.9f, 0.1f, 0.1f }, Payload = { [""city""] = ""red"" } }, new() { Id = 2, Vectors = new[] { 0.1f, 0.9f, 0.1f }, Payload = { [""city""] = ""green"" } }, new() { Id = 3, Vectors = new[] { 0.1f, 0.1f, 0.9f }, Payload = { [""city""] = ""blue"" } } }, ordering: WriteOrderingType.Strong ); ``` ## Listener mode <aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside> In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations. There are several scenarios where this can be useful: - Listener option can be used to store data in a separate node, which can be used for backup purposes or to store data for a long time. - Listener node can be used to syncronize data into another region, while still performing search operations in the local region. To enable listener mode, set `node_type` to `Listener` in the config file: ```yaml storage: node_type: ""Listener"" ``` Listener node will not participate in search operations, but will still accept write operations and will store the data in the local storage. All shards, stored on the listener node, will be converted to the `Listener` state. Additionally, all write requests sent to ,source:documentation/guides/distributed_deployment.mddocument:""must"": [ { ""key"": ""food"", ""match"": { ""value"": ""meat"" } }, { ""key"": ""likes"", ""match"": { ""value"": true } } ] } } }] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.NestedCondition( nested=models.Nested( key=""diet"", filter=models.Filter( must=[ models.FieldCondition( key=""food"", match=models.MatchValue(value=""meat"") ), models.FieldCondition( key=""likes"", match=models.MatchValue(value=True) ), ] ), ) ) ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { nested: { key: ""diet"", filter: { must: [ { key: ""food"", match: { value: ""meat"" }, }, { key: ""likes"", match: { value: true }, }, ], }, }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, NestedCondition, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must([NestedCondition { key: ""diet"".to_string(), filter: Some(Filter::must([ Condition::matches(""food"", ""meat"".to_string()), Condition::matches(""likes"", true), ])), } .into()])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.match; import static io.qdrant.client.ConditionFactory.matchKeyword; import static io.qdrant.client.ConditionFactory.nested; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addMust( nested( ""diet"", Filter.newBuilder() .addAllMust( List.of( matchKeyword(""food"", ""meat""), match(""likes"", true))) .build())) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); await client.ScrollAsync( collectionName: ""{collection_name}"", filter: Nested(""diet"", MatchKeyword(""food"", ""meat"") & Match(""likes"", true)) ); ``` The matching logic is modified to be applied at the level of an array element within the payload. Nested filters work in the same way as if the nested filter was applied to a single element of the array at a time. Parent document is considered to match the condition if at least one element of the array matches the nested filter. **Limitations** The `has_id` condition is not supported within the nested object filter. If you need it, place it in an adjacent `must` clause. ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must"": [ ""nested"": { { ""key"": ""diet"", ""filter"":{ ""must"": [ { ""key"": ""food"", ""match"": { ""value"": ""meat"" } }, { ""key"": ""likes"", ""match"": { ""value"": true } } ] } } }, { ""has_id"": [1] } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.NestedCondition( nested=models.Nested( key=""diet"", filter=models.Filter( must=[ models.FieldCondition( key=""food"", match=models.MatchValue(value=""meat"") ), models.FieldCondition( key=""likes"", match=models.MatchValue(value=True) ), ] ), ) ), models.HasIdCondition(has_id=[1]), ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { nested: { key: ""diet"", filter: { must: [ { key: ""food"", match: { value: ""meat"" }, }, { key: ""likes"", match: { value: true }, }, ], }, }, }, { has_id: [1], }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, NestedCondition, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must([ NestedCondition { key: ""diet"".to_string(), filter: Some(Filter::must([ Condition::matches(""food"", ""meat"".to_string()), Condition::matches(""likes"", true), ])), } .into(), Condition::has_id([1]), ])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.hasId; import static io.qdrant.client.ConditionFactory.match; import static io.qdrant.client.ConditionFactory.matchKeyword; import static io.qdrant.client.ConditionFactory.nested; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addMust( nested( ""diet"", Filter.newBuilder() .addAllMust( List.of( matchKeyword(""food"", ""meat""), match(""likes"", true))) .build())) .addMust(hasId(id(1))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); await client.ScrollAsync( collectionName: ""{collection_name}"", filter: Nested(""diet"", MatchKeyword(""food"", ""meat"") & Match(""likes"", true)) & HasId(1) ); ``` ### Full Text Match *Available as of v0.10.0* A special case of the `match` condition is the `text` match ,source:documentation/concepts/filtering.mddocument:} ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( should=[ models.FieldCondition( key=""city"", match=models.MatchValue(value=""London""), ), models.FieldCondition( key=""color"", match=models.MatchValue(value=""red""), ), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { should: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::should([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; import java.util.List; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllShould( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // | operator combines two conditions in an OR disjunction(should) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"") ); ``` Filtered points would be: ```json [ { ""id"": 1, ""city"": ""London"", ""color"": ""green"" }, { ""id"": 2, ""city"": ""London"", ""color"": ""red"" }, { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" }, { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" } ] ``` When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied. In this sense, `should` is equivalent to the operator `OR`. ### Must Not Example: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must_not"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } }, { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must_not=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must_not: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must_not([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllMustNot( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // The ! operator negates the condition(must not) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")) ); ``` Filtered points would be: ```json [ { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" }, { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" } ] ``` When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied. In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`. ### Clauses combination It is also possible to use several clauses simultaneously: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ], ""must_not"": [ { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), ], must_not=[ models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { key: ""city"", match: { value: ""London"" }, }, ], must_not: [ { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter { ,source:documentation/concepts/filtering.mddocument:} ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, }); ``` ```rust use qdrant_client::qdrant::RecommendPoints; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.RecommendPoints; client .recommendAsync( RecommendPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPositive(List.of(id(100), id(231))) .addAllNegative(List.of(id(718))) .setUsing(""image"") .setLimit(10) .build()) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.RecommendAsync( collectionName: ""{collection_name}"", positive: new ulong[] { 100, 231 }, negative: new ulong[] { 718 }, usingVector: ""image"", limit: 10 ); ``` Parameter `using` specifies which stored vectors to use for the recommendation. ### Lookup vectors from another collection *Available as of v0.11.6* If you have collections with vectors of the same dimensionality, and you want to look for recommendations in one collection based on the vectors of another collection, you can use the `lookup_from` parameter. It might be useful, e.g. in the item-to-user recommendations scenario. Where user and item embeddings, although having the same vector parameters (distance type and dimensionality), are usually stored in different collections. ```http POST /collections/{collection_name}/points/recommend { ""positive"": [100, 231], ""negative"": [718], ""using"": ""image"", ""limit"": 10, ""lookup_from"": { ""collection"":""{external_collection_name}"", ""vector"":""{external_vector_name}"" } } ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, lookup_from=models.LookupLocation( collection=""{external_collection_name}"", vector=""{external_vector_name}"" ), ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, lookup_from: { ""collection"" : ""{external_collection_name}"", ""vector"" : ""{external_vector_name}"" }, }); ``` ```rust use qdrant_client::qdrant::{LookupLocation, RecommendPoints}; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, lookup_from: Some(LookupLocation { collection_name: ""{external_collection_name}"".to_string(), vector_name: Some(""{external_vector_name}"".to_string()), ..Default::default() }), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.LookupLocation; import io.qdrant.client.grpc.Points.RecommendPoints; client .recommendAsync( RecommendPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPositive(List.of(id(100), id(231))) .addAllNegative(List.of(id(718))) .setUsing(""image"") .setLimit(10) .setLookupFrom( LookupLocation.newBuilder() .setCollectionName(""{external_collection_name}"") .setVectorName(""{external_vector_name}"") .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.RecommendAsync( collectionName: ""{collection_name}"", positive: new ulong[] { 100, 231 }, negative: new ulong[] { 718 }, usingVector: ""image"", limit: 10, lookupFrom: new LookupLocation { CollectionName = ""{external_collection_name}"", VectorName = ""{external_vector_name}"", } ); ``` Vectors are retrieved from the external collection by ids provided in the `positive` and `negative` lists. These vectors then used to perform the recommendation in the current collection, comparing against the ""using"" or default vector. ## Batch recommendation API *Available as of v0.10.0* Similar to the batch search API in terms of usage and advantages, it enables the batching of recommendation requests. ```http POST /collections/{collection_name}/points/recommend/batch { ""searches"": [ { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ] }, ""negative"": [718], ""positive"": [100, 231], ""limit"": 10 }, { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ] }, ""negative"": [300], ""positive"": [200, 67], ""limit"": 10 } ] } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) filter = models.Filter( must=[ models.FieldCondition( key=""city"", match=models.MatchValue( value=""London"", ), ) ] ) recommend_queries = [ models.RecommendRequest( positive=[100, 231], negative=[718], filter=filter, limit=3 ), models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3), ] client.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries) ``` ```typescript import { QdrantClient } ,source:documentation/concepts/explore.mddocument:= new QdrantClient(""localhost"", 6334); await client.UpdateCollectionAsync( collectionName: ""{collection_name}"", optimizersConfig: new OptimizersConfigDiff { IndexingThreshold = 10000 } ); ``` The following parameters can be updated: * `optimizers_config` - see [optimizer](../optimizer/) for details. * `hnsw_config` - see [indexing](../indexing/#vector-index) for details. * `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details. * `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings. * `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection). Calls to this endpoint may be blocking as it waits for existing optimizers to finish. We recommended against using this in a production database as it may introduce huge overhead due to the rebuilding of the index. #### Update vector parameters *Available as of v1.4.0* <aside role=""status"">To update vector parameters using the collection update API, you must always specify a vector name. If your collection does not have named vectors, use an empty (<code>""""</code>) name.</aside> Qdrant 1.4 adds support for updating more collection parameters at runtime. HNSW index, quantization and disk configurations can now be changed without recreating a collection. Segments (with index and quantized data) will automatically be rebuilt in the background to match updated parameters. To put vector data on disk for a collection that **does not have** named vectors, use `""""` as name: ```http PATCH /collections/{collection_name} { ""vectors"": { """": { ""on_disk"": true } } } ``` ```bash curl -X PATCH http://localhost:6333/collections/test_collection1 \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""vectors"": { """": { ""on_disk"": true } } }\' ``` To put vector data on disk for a collection that **does have** named vectors: Note: To create a vector name, follow the procedure from our [Points](/documentation/concepts/points/#create-vector-name). ```http PATCH /collections/{collection_name} { ""vectors"": { ""my_vector"": { ""on_disk"": true } } } ``` ```bash curl -X PATCH http://localhost:6333/collections/test_collection1 \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""vectors"": { ""my_vector"": { ""on_disk"": true } } }\' ``` In the following example the HNSW index and quantization parameters are updated, both for the whole collection, and for `my_vector` specifically: ```http PATCH /collections/{collection_name} { ""vectors"": { ""my_vector"": { ""hnsw_config"": { ""m"": 32, ""ef_construct"": 123 }, ""quantization_config"": { ""product"": { ""compression"": ""x32"", ""always_ram"": true } }, ""on_disk"": true } }, ""hnsw_config"": { ""ef_construct"": 123 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""quantile"": 0.8, ""always_ram"": false } } } ``` ```bash curl -X PATCH http://localhost:6333/collections/test_collection1 \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""vectors"": { ""my_vector"": { ""hnsw_config"": { ""m"": 32, ""ef_construct"": 123 }, ""quantization_config"": { ""product"": { ""compression"": ""x32"", ""always_ram"": true } }, ""on_disk"": true } }, ""hnsw_config"": { ""ef_construct"": 123 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""quantile"": 0.8, ""always_ram"": false } } }\' ``` ```python client.update_collection( collection_name=""{collection_name}"", vectors_config={ ""my_vector"": models.VectorParamsDiff( hnsw_config=models.HnswConfigDiff( m=32, ef_construct=123, ), quantization_config=models.ProductQuantization( product=models.ProductQuantizationConfig( compression=models.CompressionRatio.X32, always_ram=True, ), ), on_disk=True, ), }, hnsw_config=models.HnswConfigDiff( ef_construct=123, ), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, quantile=0.8, always_ram=False, ), ), ) ``` ```typescript client.updateCollection(""{collection_name}"", { vectors: { my_vector: { hnsw_config: { m: 32, ef_construct: 123, }, quantization_config: { product: { compression: ""x32"", always_ram: true, }, }, on_disk: true, }, }, hnsw_config: { ef_construct: 123, }, quantization_config: { scalar: { type: ""int8"", quantile: 0.8, always_ram: true, }, }, }); ``` ,source:documentation/concepts/collections.md']","Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.",0.8303606879747796,0,0.9999999999,0.7829336555555783,1.0,0.9999999999,0.5,0.005952380952380952,False
8,8,What is significance of ‘on_disk_payload’ setting?,"The 'on_disk_payload' setting in Qdrant determines whether the payload data associated with vectors is stored on disk or in memory. When this setting is enabled, it allows for the payload data to be stored on disk, reducing the memory footprint of the service. This can be particularly useful when dealing with large collections and limited memory resources.

You can configure this setting by setting the 'on_disk_payload' option to 'true' in the collection create API. This feature is available as of version 1.2.0. For more detailed information on configuring this setting, you can refer to the documentation on [Storage](../storage) under the 'Storage' section.","['document:as cloud, happening when users were pushing massive amounts of data into the service using `wait=false`. This data influx resulted in an overreaching of disk or RAM limits before the Write-Ahead Logging (WAL) was fully applied. This situation was causing Qdrant to attempt a restart and reapplication of WAL, failing recurrently due to the same memory constraints and pushing the service into a frustrating crash loop with many Out-of-Memory errors. Qdrant 1.2 enters recovery mode, if enabled, when it detects a failure on startup. That makes the service halt the loading of collection data and commence operations in a partial state. This state allows for removing collections but doesn\'t support search or update functions. **Recovery mode [has to be enabled by user](/documentation/administration/#recovery-mode).** ### Appendable mmap For a long time, segments using mmap storage were `non-appendable` and could only be constructed by the optimizer. Dynamically adding vectors to the mmap file is fairly complicated and thus not implemented in Qdrant, but we did our best to implement it in the recent release. If you want to read more about segments, check out our docs on [vector storage](/documentation/storage/#vector-storage). ## Security There are two major changes in terms of [security](/documentation/security/): 1. **API-key support** - basic authentication with a static API key to prevent unwanted access. Previously API keys were only supported in [Qdrant Cloud](https://cloud.qdrant.io/). 2. **TLS support** - to use encrypted connections and prevent sniffing/MitM attacks. ## Release notes As usual, [our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.2.0) describe all the changes introduced in the latest version.,source:articles/qdrant-1.2.x.mddocument:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.mddocument:--- title: Storage weight: 80 aliases: - ../storage --- # Storage All data within one collection is divided into segments. Each segment has its independent vector and payload storage as well as indexes. Data stored in segments usually do not overlap. However, storing the same point in different segments will not cause problems since the search contains a deduplication mechanism. The segments consist of vector and payload storages, vector and payload [indexes](../indexing), and id mapper, which stores the relationship between internal and external ids. A segment can be `appendable` or `non-appendable` depending on the type of storage and index used. You can freely add, delete and query data in the `appendable` segment. With `non-appendable` segment can only read and delete data. The configuration of the segments in the collection can be different and independent of one another, but at least one `appendable\' segment must be present in a collection. ## Vector storage Depending on the requirements of the application, Qdrant can use one of the data storage options. The choice has to be made between the search speed and the size of the RAM used. **In-memory storage** - Stores all vectors in RAM, has the highest speed since disk access is required only for persistence. **Memmap storage** - Creates a virtual address space associated with the file on disk. [Wiki](https://en.wikipedia.org/wiki/Memory-mapped_file). Mmapped files are not directly loaded into RAM. Instead, they use page cache to access the contents of the file. This scheme allows flexible use of available memory. With sufficient RAM, it is almost as fast as in-memory storage. ### Configuring Memmap storage There are two ways to configure the usage of memmap(also known as on-disk) storage: - Set up `on_disk` option for the vectors in the collection create API: *Available as of v1.2.0* ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"", ""on_disk"": true } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams( size=768, distance=models.Distance.COSINE, on_disk=True ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", on_disk: true, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), on_disk: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( ""{collection_name}"", VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .setOnDisk(true) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( ""{collection_name}"", new VectorParams { Size = 768, Distance = Distance.Cosine, OnDisk = true } ); ``` This will create a collection with all vectors immediately stored in memmap storage. This is the recommended way, in case your Qdrant instance operates with fast disks and you are working with large collections. - Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage. There are two ways to do ,source:documentation/concepts/storage.mddocument:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.mddocument:many queries are not IO bound, so the overhead may or may not become measurable in your workload. Finally, on-device disks typically carry lower latency than network drives, which may also affect mmap overhead. Therefore before you roll out io\\_uring, perform the above or a similar benchmark with both mmap and io\\_uring and measure both wall time and IOps). Benchmarks are always highly use-case dependent, so your mileage may vary. Still, doing that benchmark once is a small price for the possible performance wins. Also please [tell us](https://discord.com/channels/907569970500743200/907569971079569410) about your benchmark results!,source:articles/io_uring.md']","The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.",0.8822701162401129,0,0.9999999999,0.7899231873618774,0.0,0.9999999999,1.0,0.009433962264150943,False
9,9,What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` in Qdrant represents the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, write operations become more tolerant to network partitions in the cluster, ensuring data consistency. However, it also requires a higher number of replicas to be active to perform write operations. You can configure the `write_consistency_factor` at the collection's creation time, as shown in the documentation on page [distributed_deployment.md](https://qdrant.tech/documentation/guides/distributed_deployment.md). This setting is crucial for maintaining consistency guarantees, especially during hardware instabilities or mass concurrent updates of the same documents.","['document:request dispatches all operations according to the current topology in order to keep the data synchronized across the cluster. - reads are using a partial fan-out strategy to optimize latency and availability - writes are executed in parallel on all active sharded replicas ![Embeddings](/docs/concurrent-operations-replicas.png) However, in some cases, it is necessary to ensure additional guarantees during possible hardware instabilities, mass concurrent updates of same documents, etc. Qdrant provides a few options to control consistency guarantees: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations. - Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low. - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical. ### Write consistency factor The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default. It can be configured at the collection\'s creation time. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 300, ""distance"": ""Cosine"" }, ""shard_number"": 6, ""replication_factor"": 2, ""write_consistency_factor"": 2, } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE), shard_number=6, replication_factor=2, write_consistency_factor=2, ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 300, distance: ""Cosine"", }, shard_number: 6, replication_factor: 2, write_consistency_factor: 2, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".into(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 300, distance: Distance::Cosine.into(), ..Default::default() })), }), shard_number: Some(6), replication_factor: Some(2), write_consistency_factor: Some(2), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(300) .setDistance(Distance.Cosine) .build()) .build()) .setShardNumber(6) .setReplicationFactor(2) .setWriteConsistencyFactor(2) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine }, shardNumber: ,source:documentation/guides/distributed_deployment.mddocument:this consideration can lead not only to functional errors but also erode the trust of users due to inconsistency and confusion, which then leads to them no longer using my work. **3. Speak Up and Effectively Communicate** Finally, In the course of development, encountering differing opinions is commonplace. It\'s essential to remain open to others\' ideas, while also possessing the resolve to communicate one\'s own perspective clearly. This fosters productive discussions and ultimately elevates the quality of the development process. ### Wrap up Being selected for Google Summer of Code 2023 and collaborating with Arnaud and the other Qdrant engineers, along with all the other community members, has been a true privilege. I\'m deeply grateful to those who invested their time and effort in reviewing my code, engaging in discussions about alternatives and design choices, and offering assistance when needed. Through these interactions, I\'ve experienced firsthand the essence of open source and the culture that encourages collaboration. This experience not only allowed me to write Rust code for a real-world product for the first time, but it also opened the door to the amazing world of open source. Without a doubt, I\'m eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I\'ve also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I\'m excited to witness new users and contributors emerge from within my own network! If you want to try out my work, read the [documentation](https://qdrant.tech/documentation/concepts/filtering/#geo-polygon) and then, either sign up for a free [cloud account](https://cloud.qdrant.io) or download the [Docker image](https://hub.docker.com/r/qdrant/qdrant). I look forward to seeing how people are using my work in their own applications!,source:articles/geo-polygon-filter-gsoc.mddocument:many queries are not IO bound, so the overhead may or may not become measurable in your workload. Finally, on-device disks typically carry lower latency than network drives, which may also affect mmap overhead. Therefore before you roll out io\\_uring, perform the above or a similar benchmark with both mmap and io\\_uring and measure both wall time and IOps). Benchmarks are always highly use-case dependent, so your mileage may vary. Still, doing that benchmark once is a small price for the possible performance wins. Also please [tell us](https://discord.com/channels/907569970500743200/907569971079569410) about your benchmark results!,source:articles/io_uring.mddocument:to try. Demetrios: Okay, the next question that I had is you talked about how benchmarking with the horizontal solution, surprisingly, has been more effective in certain use cases. I\'m guessing that\'s why you got a little bit of love for Qdrant and what we\'re doing here. Stanislas Polu: Yeah I think the benchmarking was really about quality of models, answers in the. Context of ritual augmented generation. So it\'s not as much as performance, but obviously performance matters, and that\'s why we love using Qdrants. But I think the main idea of. Stanislas Polu: What I mentioned is that it\'s interesting because today the retrieval is noisy, because the embedders are not perfect, which is an interesting point. Sorry, I\'m double clicking, but I\'ll come back. The embedded are really not perfect. Are really not perfect. So that\'s interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss. Of precision because of the compression, et cetera, et cetera. And that\'s funny, like in all kind of retrieval and mental generation world, it really doesn\'t matter. We take all the performance we can because the loss of precision coming from compression of those vectors at the vector DB level are completely negligible compared to. The holon fuckness of the embedders in. Stanislas Polu: Terms of capability to correctly embed text, because they\'re extremely powerful, but they\'re far from being perfect. And so that\'s an interesting thing where you can really go as far as you want in terms of performance, because your error is dominated completely by the. Quality of your embeddings. Going back up. I think what\'s interesting is that the. Retrieval is noisy, mostly because of the embedders, and the models are not perfect. And so the reality is that more. Data in a rack context is not. Necessarily better data because the retrievals become noisy. The model kind of gets confused and it starts hallucinating stuff, et cetera. And so the right trade off is that you want to access to as. Much data as possible, but you want To give the ability to our users. To select very narrowly the data required for a given task. Stanislas Polu: And so that\'s kind of what our product does, is the ability to create assistants that are specialized to a given task. And most of the specification of an assistant is obviously a prompt, but also. Saying, oh, I\'m working on helping sales find interesting next leads. And you really want to narrow the data exactly where that information lies. And that\'s where there, we\'re really relying. Hard on Qdrants as well. So the kind of indexing capabilities on. Top of the vector search, where whenever. Stanislas Polu: We insert the documents, we kind of try to insert an array of parents that reproduces the hierarchy of whatever that document is coming from, which lets us create a very nice user experience where when you create an assistant, you can say, oh, I\'m going down two levels within ,source:blog/qdrant-x-dust-how-vector-search-helps-make-work-work-better-stan-polu-vector-space-talk-010.mddocument:can be freely reordered. - `medium` ordering serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change. - `strong` ordering serializes all write operations through the permanent leader, which provides strong consistency, but write operations may be unavailable if the leader is down. ```http PUT /collections/{collection_name}/points?ordering=strong { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ordering=""strong"", ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, ordering: ""strong"", }); ``` ```rust use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType}; use serde_json::json; client .upsert_points_blocking( ""{collection_name}"", None, vec![ PointStruct::new( 1, vec![0.9, 0.1, 0.1], json!({ ""color"": ""red"" }) .try_into() .unwrap(), ), PointStruct::new( 2, vec![0.1, 0.9, 0.1], json!({ ""color"": ""green"" }) .try_into() .unwrap(), ), PointStruct::new( 3, vec![0.1, 0.1, 0.9], json!({ ""color"": ""blue"" }) .try_into() .unwrap(), ), ], Some(WriteOrdering { r#type: WriteOrderingType::Strong.into(), }), ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.UpsertPoints; import io.qdrant.client.grpc.Points.WriteOrdering; import io.qdrant.client.grpc.Points.WriteOrderingType; client .upsertAsync( UpsertPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPoints( List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.9f, 0.1f, 0.1f)) .putAllPayload(Map.of(""color"", value(""red""))) .build(), PointStruct.newBuilder() .setId(id(2)) .setVectors(vectors(0.1f, 0.9f, 0.1f)) .putAllPayload(Map.of(""color"", value(""green""))) .build(), PointStruct.newBuilder() .setId(id(3)) .setVectors(vectors(0.1f, 0.1f, 0.94f)) .putAllPayload(Map.of(""color"", value(""blue""))) .build())) .setOrdering(WriteOrdering.newBuilder().setType(WriteOrderingType.Strong).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.9f, 0.1f, 0.1f }, Payload = { [""city""] = ""red"" } }, new() { Id = 2, Vectors = new[] { 0.1f, 0.9f, 0.1f }, Payload = { [""city""] = ""green"" } }, new() { Id = 3, Vectors = new[] { 0.1f, 0.1f, 0.9f }, Payload = { [""city""] = ""blue"" } } }, ordering: WriteOrderingType.Strong ); ``` ## Listener mode <aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside> In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations. There are several scenarios where this can be useful: - Listener option can be used to store data in a separate node, which can be used for backup purposes or to store data for a long time. - Listener node can be used to syncronize data into another region, while still performing search operations in the local region. To enable listener mode, set `node_type` to `Listener` in the config file: ```yaml storage: node_type: ""Listener"" ``` Listener node will not participate in search operations, but will still accept write operations and will store the data in the local storage. All shards, stored on the listener node, will be converted to the `Listener` state. Additionally, all write requests sent to ,source:documentation/guides/distributed_deployment.md']","The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.",0.9043153549496795,0,0.9999999999,0.8299383814291521,1.0,0.9999999999,1.0,0.02112676056338028,True
