question,answer,contexts,ground_truth
What is vaccum optimizer ?,"The Vacuum Optimizer in Qdrant is used to address the accumulation of deleted records within a segment repository. Instead of immediately deleting entries after a query, Qdrant marks records as deleted and ignores them for future queries to minimize disk access. Over time, these deleted records can accumulate, occupy memory, and slow down the system. The Vacuum Optimizer is triggered when a segment has accumulated too many deleted records, as defined in the configuration file. You can set parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to perform segment optimization. This optimizer helps maintain system efficiency by managing deleted records effectively. For more details, you can refer to the documentation on Optimizer under the section ""Vacuum Optimizer"" from the source provided.","['document:--- title: Optimizer weight: 70 aliases: - ../optimizer --- # Optimizer It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely. Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)). In this case, the segment to be optimized remains readable for the time of the rebuild. ![Segment optimization](/docs/optimization.svg) The availability is achieved by wrapping the segment into a proxy that transparently handles data changes. Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates. ## Vacuum Optimizer The simplest example of a case where you need to rebuild a segment repository is to remove points. Like many other databases, Qdrant does not delete entries immediately after a query. Instead, it marks records as deleted and ignores them for future queries. This strategy allows us to minimize disk access - one of the slowest operations. However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system. To avoid these adverse effects, Vacuum Optimizer is used. It is used if the segment has accumulated too many deleted records. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # The minimal fraction of deleted vectors in a segment, required to perform segment optimization deleted_threshold: 0.2 # The minimal number of vectors in a segment, required to perform segment optimization vacuum_min_vector_number: 1000 ``` ## Merge Optimizer The service may require the creation of temporary segments. Such segments, for example, are created as copy-on-write segments during optimization itself. It is also essential to have at least one small segment that Qdrant will use to store frequently updated data. On the other hand, too many small segments lead to suboptimal search performance. There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # If the number of segments exceeds this value, the optimizer will merge the smallest segments. max_segment_number: 5 ``` ## Indexing Optimizer Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records. So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan. The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only ,source:documentation/concepts/optimizer.md'
 'document:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.md'
 'document:--- title: Optimize Resources weight: 11 aliases: - ../tutorials/optimize --- # Optimize Qdrant Different use cases have different requirements for balancing between memory, speed, and precision. Qdrant is designed to be flexible and customizable so you can tune it to your needs. ![Trafeoff](/docs/tradeoff.png) Let\'s look deeper into each of those possible optimization scenarios. ## Prefer low memory footprint with high speed search The main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads. Vector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true } } ); ``` `mmmap_threshold` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM. Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision. ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""rescore"": false } }, ""vector"": [0.2, 0.1, 0.9, 0.7], ""limit"": 10 } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.search( collection_name=""{collection_name}"", query_vector=[0.2, 0.1, 0.9, 0.7], search_params=models.SearchParams( quantization=models.QuantizationSearchParams(rescore=False) ), ) ``` ```typescript import { QdrantClient ,source:documentation/guides/optimize.md']","The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. "
Tell me about ‘always_ram’ parameter?,"The `always_ram` parameter in Qdrant determines whether quantized vectors are kept cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, setting `always_ram` to `true` stores quantized vectors in RAM, which can speed up the search process. You can find more information about this parameter in the documentation under the 'Setting up Product Quantization' section. Check the source document at `documentation/guides/quantization.md` for detailed information and examples on how to configure this parameter.","['document:vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Binary = new BinaryQuantization { AlwaysRam = true } } ); ``` `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. In this case, you can set `always_ram` to `true` to store quantized vectors in RAM. ### Setting up Product Quantization To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""product"": { ""compression"": ""x16"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ProductQuantization( product=models.ProductQuantizationConfig( compression=models.CompressionRatio.X16, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { product: { compression: ""x16"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CompressionRatio, CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Product(ProductQuantization { compression: CompressionRatio::X16.into(), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CompressionRatio; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.ProductQuantization; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setProduct( ProductQuantization.newBuilder() .setCompression(CompressionRatio.x16) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Product = new ProductQuantization { Compression = CompressionRatio.X16, AlwaysRam = true } } ); ``` There are two parameters that you can specify in the `quantization_config` section: `compression` - compression ratio. Compression ratio represents the size of the quantized vector in bytes divided by the size of the original vector in bytes. In this case, the quantized vector will be 16 times smaller than the original vector. `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`. ### Searching with Quantization Once you have configured quantization for a collection, you don\'t need to do anything extra to search with quantization. Qdrant will automatically use quantized vectors if they are available. However, there are a few options that you can use to control the search process: ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""ignore"": false, ""rescore"": true, ""oversampling"": 2.0 } ,source:documentation/guides/quantization.md'
 'document:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.md'
 ""document:--- title: Minimal RAM you need to serve a million vectors short_description: How to properly measure RAM usage and optimize Qdrant for memory consumption. description: How to properly measure RAM usage and optimize Qdrant for memory consumption. social_preview_image: /articles_data/memory-consumption/preview/social_preview.jpg preview_dir: /articles_data/memory-consumption/preview small_preview_image: /articles_data/memory-consumption/icon.svg weight: 7 author: Andrei Vasnetsov author_link: https://blog.vasnetsov.com/ date: 2022-12-07T10:18:00.000Z # aliases: [ /articles/memory-consumption/ ] --- <!-- 1. How people usually measure memory and why it might be misleading 2. How to properly measure memory 3. Try different configurations of Qdrant and see how they affect the memory consumption and search speed 4. Conclusion --> <!-- Introduction: 1. We are used to measure memory consumption by looking into `htop`. But it could be misleading. 2. There are multiple reasons why it is wrong: 1. Process may allocate memory, but not use it. 2. Process may not free deallocated memory. 3. Process might be forked and memory is shared between processes. 3. Process may use disk cache. 3. As a result, if you see `10Gb` memory consumption in `htop`, it doesn't mean that your process actually needs `10Gb` of RAM to work. --> When it comes to measuring the memory consumption of our processes, we often rely on tools such as `htop` to give us an indication of how much RAM is being used. However, this method can be misleading and doesn't always accurately reflect the true memory usage of a process. There are many different ways in which `htop` may not be a reliable indicator of memory usage. For instance, a process may allocate memory in advance but not use it, or it may not free deallocated memory, leading to overstated memory consumption. A process may be forked, which means that it will have a separate memory space, but it will share the same code and data with the parent process. This means that the memory consumption of the child process will be counted twice. Additionally, a process may utilize disk cache, which is also accounted as resident memory in the `htop` measurements. As a result, even if `htop` shows that a process is using 10GB of memory, it doesn't necessarily mean that the process actually requires 10GB of RAM to operate efficiently. In this article, we will explore how to properly measure RAM usage and optimize Qdrant for optimal memory consumption. ## How to measure actual memory requirements <!-- 1. We need to know how much RAM we need to have for the program to work, so why not just do a straightforward experiment. 2. Let's limit the allowed memory of the process and see at which point the process will working. 3. We can do a grid search, but it is better to apply binary search to find the minimum amount of RAM more quickly. 4. We will use docker to limit the memory usage of the process. 5. Before running docker we will use ``` # Ensure that there is no data in page cache before each benchmark run sudo bash -c 'sync; echo 1 > /proc/sys/vm/drop_caches' ``` to clear the page between runs and make sure ,source:articles/memory-consumption.md""]","In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times."
What is difference between scalar and product quantization?,"Scalar quantization and product quantization are two different methods used in quantization for high-dimensional vectors in Qdrant.

Scalar quantization, as described in the documentation, involves compressing vectors by reducing the number of bits used to represent each vector component. This method allows for a reduction in memory usage by a factor of 4 and speeds up the search process. However, it comes with a trade-off of a slight loss in accuracy due to the conversion process from float32 to uint8.

On the other hand, product quantization offers a higher compression rate compared to scalar quantization. It may reduce memory usage and search time in certain scenarios, but it comes with significant trade-offs in accuracy and in-RAM search speed. Product quantization is recommended for specific situations like low-RAM environments, high-dimensional original vectors, and where indexing speed is not critical.

In summary, scalar quantization is a more universal method that balances accuracy, speed, and compression, while product quantization provides a higher compression rate but with trade-offs in accuracy and search speed. For more detailed information, you can refer to the Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization).","['document:section for more information on how to optimize the quantization parameters for your use case. ## How to choose the right quantization method Here is a brief table of the pros and cons of each quantization method: | Quantization method | Accuracy | Speed | Compression | |---------------------|----------|--------------|-------------| | Scalar | 0.99 | up to x2 | 4 | | Product | 0.7 | 0.5 | up to 64 | | Binary | 0.95* | up to x40 | 32 | `*` - for compatible models * **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only. * **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable. * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical. ## Setting up Quantization in Qdrant You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration. Quantization will be automatically applied to all vectors during the indexation process. Quantized vectors are stored alongside the original vectors in the collection, so you will still have access to the original vectors if you need them. *Available as of v1.1.1* The `quantization_config` can also be set on a per vector basis by specifying it in a named vector. ### Setting up Scalar Quantization To enable scalar quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""quantile"": 0.99, ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, quantile=0.99, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { scalar: { type: ""int8"", quantile: 0.99, always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), quantile: Some(0.99), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.99f) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { ,source:documentation/guides/quantization.md'
 'document:s</td> <td>474 s</td> </tr> </tbody> </table> It turns out that in some cases, Product Quantization may not only reduce the memory usage, but also the search time. ## Good practices Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed. Product Quantization tends to be favored in certain specific scenarios: - Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself - Situations where the dimensionality of the original vectors is sufficiently high - Cases where indexing speed is not a critical factor In circumstances that do not align with the above, Scalar Quantization should be the preferred choice. Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) will help you to set and configure the new quantization for your data and achieve even up to 64x memory reduction.,source:articles/product-quantization.md'
 'document:--- title: Quantization weight: 120 aliases: - ../quantization --- # Quantization Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors. By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors. Different quantization methods have different mechanics and tradeoffs. We will cover them in this section. Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed. There are tradeoffs associated with quantization. On the one hand, quantization allows for significant reductions in storage requirements and faster search times. This can be particularly beneficial in large-scale applications where minimizing the use of resources is a top priority. On the other hand, quantization introduces an approximation error, which can lead to a slight decrease in search quality. The level of this tradeoff depends on the quantization method and its parameters, as well as the characteristics of the data. ## Scalar Quantization *Available as of v1.1.0* Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component. For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8. In other words, Qdrant performs `float32 -> uint8` conversion for each vector component. Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4. In addition to reducing the memory footprint, scalar quantization also speeds up the search process. Qdrant uses a special SIMD CPU instruction to perform fast vector comparison. This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster. The main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality. However, this error is usually negligible, and tends to be less significant for high-dimensional vectors. In our experiments, we found that the error introduced by scalar quantization is usually less than 1%. However, this value depends on the data and the quantization parameters. Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case. ## Binary Quantization *Available as of v1.5.0* Binary quantization is an extreme case of scalar quantization. This feature lets you represent each vector component as a single bit, effectively reducing the memory footprint by a **factor of 32**. This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions. Binary quantization can achieve up to a **40x** speedup compared to the original vectors. However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. At the moment, binary ,source:documentation/guides/quantization.md']","While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed."
What is ‘best_score’ strategy?,"The 'best_score' strategy is a new recommendation strategy introduced in version 1.6.0. It focuses on finding similar vectors that are closer to a positive example while avoiding those closer to a negative one. This strategy calculates the best positive and best negative scores for each candidate point and selects the final score based on these values. The formula used to calculate the score is: 

```rust
if best_positive_score > best_negative_score {
    score = best_positive_score;
} else {
    score = -(best_negative_score * best_negative_score);
}
```

This strategy aims to penalize points closer to negative examples and prioritize those closer to positive examples during the search process. It is recommended to set `""strategy"": ""best_score""` in the recommendation request to utilize this algorithm effectively. For more details, you can refer to the documentation under the 'Best score strategy' section.","['document:step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation. The way to produce the searching vector is by first averaging all the positive and negative examples separately, and then combining them into a single vector using the following formula: ```rust avg_positive + avg_positive - avg_negative ``` In the case of not having any negative examples, the search vector will simply be equal to `avg_positive`. This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request. ### Best score strategy *Available as of v1.6.0* A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one. The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula: ```rust let score = if best_positive_score > best_negative_score { best_positive_score; } else { -(best_negative_score * best_negative_score); }; ``` <aside role=""alert""> The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples. </aside> Since we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic. <aside role=""status""> Accuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code> </aside> To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request. #### Using only negative examples A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one. Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning. ### Multiple vectors *Available as of v0.10.0* If the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request: ```http POST /collections/{collection_name}/points/recommend { ""positive"": [100, 231], ""negative"": [718], ""using"": ""image"", ""limit"": 10 } ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, }); ``` ```rust use qdrant_client::qdrant::RecommendPoints; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, ..Default::default() }) .await?; ``` ```java import java.util.List; ,source:documentation/concepts/explore.md'
 'document:for choosing the best match, which is also true in the case of vector recommendations. We can use different approaches to determine the path of traversing the HNSW graph by changing how we calculate the score of a candidate point during traversal. The default behaviour is based on pure distance, but Qdrant 1.6 exposes two strategies for the recommendation API. ### Average vector The default strategy, called `average_vector` is the previous one, based on the average of positive and negative examples. It simplifies the recommendations process and converts it into a single vector search. It supports both point IDs and vectors as parameters. For example, you can get recommendations based on past interactions with existing points combined with query vector embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the following formula: $$ \\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right) $$ The `average_vector` converts the problem of recommendations into a single vector search. ### The new hotness - Best score The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative samples and uses a slightly more sophisticated algorithm under the hood. The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a bunch of positive and negative queries**. As a result, for each sample in the query, we have a set of distances, one for each sample. In the next step, we simply take the best scores for positives and negatives, creating two separate values. Best scores are just the closest distances of a query to positives and negatives. The idea is: **if a point is closer to any negative than to any positive example, we do not want it**. We penalize being close to the negatives, so instead of using the similarity value directly, we check if it’s closer to positives or negatives. The following formula is used to calculate the score of a traversed potential point: ```rust if best_positive_score > best_negative_score { score = best_positive_score } else { score = -(best_negative_score * best_negative_score) } ``` If the point is closer to the negatives, we penalize it by taking the negative squared value of the best negative score. For a closer negative, the score of the candidate point will always be lower or equal to zero, making the chances of choosing that point significantly lower. However, if the best negative score is higher than the best positive score, we still prefer those that are further away from the negatives. That procedure effectively **pulls the traversal procedure away from the negative examples**. If you want to know more about the internals of HNSW, you can check out the article about the [Filtrable HNSW](https://qdrant.tech/articles/filtrable-hnsw/) that covers the topic thoroughly. ## Food Discovery demo Our [Food Discovery demo](https://qdrant.tech/articles/food-discovery-demo/) is an application built on ,source:articles/new-recommendation-api.md'
 ""document:strategy. Mikko Lehtimäki: So when you get the query from the user, for example, you want to take different steps in order to answer it. For example, you may want to decompose the query to small questions that you answer individually, and each individual question may take a different path. So you may want to do a query based on metadata, for example pages five and six from a document. Or you may want to look up based on keywords full each page or chunk with a specific word. And there's really like a massive amount of choices how this can go. Another example is generating hypothetical documents based on the query and embedding those rather than the query itself. That will in some cases lead to higher quality retrieval results. But now all this leads into the right side of the query path. Mikko Lehtimäki: So here we have a re ranker. So if we implement all of this, we end up really retrieving a lot of data. We typically will retrieve more than it makes sense to give to the language model in a single call. So we can add a re ranker step here and it will firstly filter out low quality retrieved content and secondly, it will put the higher quality content on the top of the retrieved documents. And now when you pass this reranked content to the language model, it should be able to pay better attention to the details that actually matter given the query. And this should lead to you better managing the amount of data that you have to handle with your final response generator, LLM. And it should also make the response generator a bit faster because you will be feeding slightly less data in one go. The simplest way to build a re ranker is probably just asking a large language model to re rank or summarize the content that you've retrieved before you feed it to the language model. Mikko Lehtimäki: That's one way to do it. So yeah, that's a lot of complexity and honestly, we're not doing all of this right now with Yokot AI, either. We've tried all of it in different scopes, but really it's a lot of logic to maintain. And to me this just like screams the bitter lesson, because we're building so many steps, so much logic, so many rules into the system, when really all of this is done just because the language model can't be trusted, or it can't be with the current architectures trained reliably, or cannot be trained in real time with the current approaches that we have. So there's one thing in this picture, in my opinion, that is more promising than the others for leveraging data and compute, which should dominate the quality of the solution in the long term. And if we focus only on that, or not only, but if we focus heavily on that part of the process, we should be able to eliminate some complexity elsewhere. So if you're watching the recording, you ,source:blog/the-bitter-lesson-of-retrieval-in-generative-language-model-workflows-mikko-lehtimäki-vector-space-talks.md""]","The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided."
How does oversampling helps?,"Oversampling helps to control the precision of search results in real-time by internally retrieving more vectors than needed and assigning more precise scores during rescoring. This technique allows for improved accuracy without rebuilding the index, simply by adjusting a parameter in the search query itself. You can learn more about oversampling and its impact on model fairness and performance in the documentation provided on page [here](/blog/openai/Oversampling_Impact.png).","['document:just the dominant one. Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects. ![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png) ### Leveraging Binary Quantization: Best Practices We recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings: 1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested. 2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages. 3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications. 4. Rescoring: Enable rescoring to improve the accuracy of search results. 5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible. Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant) Learn more about how to boost your vector search speed and accuracy while reducing costs: [Binary Quantization.](https://qdrant.tech/documentation/guides/quantization/?selector=aHRtbCA%2BIGJvZHkgPiBkaXY6bnRoLW9mLXR5cGUoMSkgPiBzZWN0aW9uID4gZGl2ID4gZGl2ID4gZGl2Om50aC1vZi10eXBlKDIpID4gYXJ0aWNsZSA%2BIGgyOm50aC1vZi10eXBlKDIp),source:articles/binary-quantization-openai.md'
 ""document:if we're using smaller models that aren't the GBTs, will that help? Andrey Vasnetsov: Right. So not all models are as big as OpenAI, but what we see, the trend in this area, the trend of development of different models, indicates that they will become bigger and bigger over time. Just because we want to store more information inside vectors, we want to have larger context, we want to have more detailed information, more detailed separation and so on. This trend is obvious if like five years ago the usual size of the vector was 100 dimensions now the usual size is 700 dimensions, so it's basically. Demetrios: Preparing for the future while also optimizing for today. Andrey Vasnetsov: Right? Demetrios: Yeah. Okay, so you mentioned on here oversampling. Can you go into that a little bit more and explain to me what that is? Andrey Vasnetsov: Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself. Demetrios: I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works. Andrey Vasnetsov: Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast. Andrey Vasnetsov: It uses very few CPU cycles to actually produce a result. That's why binary quantization is over 30 times faster than regular product. And it actually solves the problem of complicated index building, because this computation of dot products is the main source of computational requirements for HNSW. Demetrios: So if I'm understanding this correctly, it's basically taking all of these numbers that are on the left, which can be, yes, decimal numbers. Andrey Vasnetsov: On the left you can see original vector and it converts it in binary representation. And of course it does lose a lot of precision in the process. But because first we have very large vector and second, we have oversampling feature, we can compensate for ,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md""
 'document:leaving those two parameters out of the search query. ## Benchmark results We retrieved some early results on the relationship between limit and oversampling using the the DBPedia OpenAI 1M vector dataset. We ran all these experiments on a Qdrant instance where 100K vectors were indexed and used 100 random queries. We varied the 3 parameters that will affect query time and accuracy: limit, rescore and oversampling. We offer these as an initial exploration of this new feature. You are highly encouraged to reproduce these experiments with your data sets. > Aside: Since this is a new innovation in vector databases, we are keen to hear feedback and results. [Join our Discord server](https://discord.gg/Qy6HCJK9Dc) for further discussion! **Oversampling:** In the figure below, we illustrate the relationship between recall and number of candidates: ![Correct vs candidates](/articles_data/binary-quantization/bq-5.png) We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0 As oversampling increases, we see a general improvement in results – but that does not hold in every case. **Rescore:** As expected, rescoring increases the time it takes to return a query. We also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. ![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png) **Limit:** We experiment with limits from Top 1 to Top 50 and we are able to get to 100% recall at limit 50, with rescore=True, in an index with 100K vectors. ## Recommendations Quantization gives you the option to make tradeoffs against other parameters: Dimension count/embedding size Throughput and Latency requirements Recall requirements If you\'re working with OpenAI or Cohere embeddings, we recommend the following oversampling settings: |Method|Dimensionality|Test Dataset|Recall|Oversampling| |-|-|-|-|-| |OpenAI text-embedding-ada-002|1536|[DbPedia](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) 1M|0.98|4x| |Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x| If you determine that binary quantization is appropriate for your datasets and queries then we suggest the following: - Binary Quantization with always_ram=True - Vectors stored on disk - Oversampling=2.0 (or more) - Rescore=True ## What\'s next? Binary quantization is exceptional if you need to work with large volumes of data under high recall expectations. You can try this feature either by spinning up a [Qdrant container image](https://hub.docker.com/r/qdrant/qdrant) locally or, having us create one for you through a [free account](https://cloud.qdrant.io/login) in our cloud hosted service. The article gives examples of data sets and configuration you can use to get going. Our documentation covers [adding the data](/documentation/tutorials/bulk-upload/) to your Qdrant instance as well as [creating your indices](/documentation/tutorials/optimize/). If you have any feedback, drop us a note on Twitter or LinkedIn to tell us about your results. [Join our lively Discord Server](https://discord.gg/Qy6HCJK9Dc) if you want to discuss BQ with like-minded people!,source:articles/binary-quantization.md']","Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time."
What is the purpose of ‘CreatePayloadIndexAsync’?,The purpose of the 'CreatePayloadIndexAsync' function is to create a keyword payload index for a specific field in a collection. This function allows you to define the field name and field schema as 'keyword' for efficient indexing and retrieval of payload data associated with the field. You can refer to the documentation for more details on how to use this function and its parameters. (Source: documentation/guides/multiple-partitions.md),"['document:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.md'
 'document:.await?; ``` ```java import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient( QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.createCollectionAsync(""{collection_name}"", VectorParams.newBuilder().setDistance(Distance.Cosine).setSize(100).build()).get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine } ); ``` In addition to the required options, you can also specify custom values for the following collection options: * `hnsw_config` - see [indexing](../indexing/#vector-index) for details. * `wal_config` - Write-Ahead-Log related configuration. See more details about [WAL](../storage/#versioning) * `optimizers_config` - see [optimizer](../optimizer) for details. * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details. * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload. * `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details. Default parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). See [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#operation/create_collection) and a [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) for more information about collection and vector parameters. *Available as of v1.2.0* Vectors all live in RAM for very quick access. The `on_disk` parameter can be set in the vector configuration. If true, all vectors will live on disk. This will enable the use of [memmaps](../../concepts/storage/#configuring-memmap-storage), which is suitable for ingesting a large amount of data. ### Create collection from another collection *Available as of v1.0.0* It is possible to initialize a collection from another existing collection. This might be useful for experimenting quickly with different configurations for the same data set. Make sure the vectors have the same `size` and `distance` function when setting up the vectors configuration in the new collection. If you used the previous sample code, `""size"": 300` and `""distance"": ""Cosine""`. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 100, ""distance"": ""Cosine"" }, ""init_from"": { ""collection"": ""{from_collection_name}"" } } ``` ```bash curl -X PUT http://localhost:6333/collections/test_collection2 \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""vectors"": { ""size"": 300, ""distance"": ""Cosine"" }, ""init_from"": { ""collection"": ""test_collection1"" } }\' ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=100, distance=models.Distance.COSINE), init_from=models.InitFrom(collection=""{from_collection_name}""), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 100, distance: ""Cosine"" }, init_from: { collection: ""{from_collection_name}"" }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 100, distance: Distance::Cosine.into(), ..Default::default() })), }), init_from_collection: Some(""{from_collection_name}"".to_string()), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(100) .setDistance(Distance.Cosine) .build())) .setInitFromCollection(""{from_collection_name}"") .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine }, initFromCollection: ""{from_collection_name}"" ); ``` ### Collection with multiple vectors *Available as of v0.10.0* It is possible to have multiple vectors per record. This ,source:documentation/concepts/collections.md'
 'document:Some(Operation::SetPayload(SetPayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), payload: HashMap::from([ (""test_payload_2"".to_string(), 2.into()), (""test_payload_3"".to_string(), 3.into()), ]), })), }, PointsUpdateOperation { operation: Some(Operation::DeletePayload(DeletePayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), keys: vec![""test_payload_2"".to_string()], })), }, PointsUpdateOperation { operation: Some(Operation::ClearPayload(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, PointsUpdateOperation { operation: Some(Operation::Delete(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, ], None, ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.PointVectors; import io.qdrant.client.grpc.Points.PointsIdsList; import io.qdrant.client.grpc.Points.PointsSelector; import io.qdrant.client.grpc.Points.PointsUpdateOperation; import io.qdrant.client.grpc.Points.PointsUpdateOperation.ClearPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePoints; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeleteVectors; import io.qdrant.client.grpc.Points.PointsUpdateOperation.PointStructList; import io.qdrant.client.grpc.Points.PointsUpdateOperation.SetPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.UpdateVectors; import io.qdrant.client.grpc.Points.VectorsSelector; client .batchUpdateAsync( ""{collection_name}"", List.of( PointsUpdateOperation.newBuilder() .setUpsert( PointStructList.newBuilder() .addPoints( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setUpdateVectors( UpdateVectors.newBuilder() .addPoints( PointVectors.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeleteVectors( DeleteVectors.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .setVectors(VectorsSelector.newBuilder().addNames("""").build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setOverwritePayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload(Map.of(""test_payload"", value(1))) .build()) .build(), PointsUpdateOperation.newBuilder() .setSetPayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload( Map.of(""test_payload_2"", value(2), ""test_payload_3"", value(3))) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePayload( DeletePayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .addKeys(""test_payload_2"") .build()) .build(), PointsUpdateOperation.newBuilder() .setClearPayload( ClearPayload.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePoints( DeletePoints.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build())) .get(); ``` To batch many points with a single operation type, please use batching functionality in that operation directly.,source:documentation/concepts/points.md']",CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.
What is the purpose of ef_construct in HNSW ?,The `ef_construct` parameter in HNSW (Hierarchical Navigable Small World) is used to specify the number of neighbors to consider during the index building process. A larger value for `ef_construct` leads to more accurate search results but requires more time to build the index. You can find more information about the `ef_construct` parameter in the documentation under the configuration parameters for the HNSW Index section. (Source: documentation/guides/configuration.md),"['document:), ) ``` ```typescript client.updateCollection(""{collection_name}"", { vectors: { my_vector: { hnsw_config: { m: 32, ef_construct: 123, }, quantization_config: { product: { compression: ""x32"", always_ram: true, }, }, on_disk: true, }, }, hnsw_config: { ef_construct: 123, }, quantization_config: { scalar: { type: ""int8"", quantile: 0.8, always_ram: true, }, }, }); ``` ```rust use qdrant_client::client::QdrantClient; use qdrant_client::qdrant::{ quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff, QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff, VectorsConfigDiff, }; client .update_collection( ""{collection_name}"", None, None, None, Some(&HnswConfigDiff { ef_construct: Some(123), ..Default::default() }), Some(&VectorsConfigDiff { config: Some(Config::ParamsMap( qdrant_client::qdrant::VectorParamsDiffMap { map: HashMap::from([( (""my_vector"".into()), VectorParamsDiff { hnsw_config: Some(HnswConfigDiff { m: Some(32), ef_construct: Some(123), ..Default::default() }), ..Default::default() }, )]), }, )), }), Some(&QuantizationConfigDiff { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8 as i32, quantile: Some(0.8), always_ram: Some(true), ..Default::default() })), }), ) .await?; ``` ```java import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.UpdateCollection; import io.qdrant.client.grpc.Collections.VectorParamsDiff; import io.qdrant.client.grpc.Collections.VectorParamsDiffMap; import io.qdrant.client.grpc.Collections.VectorsConfigDiff; client .updateCollectionAsync( UpdateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setHnswConfig(HnswConfigDiff.newBuilder().setEfConstruct(123).build()) .setVectorsConfig( VectorsConfigDiff.newBuilder() .setParamsMap( VectorParamsDiffMap.newBuilder() .putMap( ""my_vector"", VectorParamsDiff.newBuilder() .setHnswConfig( HnswConfigDiff.newBuilder() .setM(3) .setEfConstruct(123) .build()) .build()))) .setQuantizationConfig( QuantizationConfigDiff.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.8f) .setAlwaysRam(true) .build())) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpdateCollectionAsync( collectionName: ""{collection_name}"", hnswConfig: new HnswConfigDiff { EfConstruct = 123 }, vectorsConfig: new VectorParamsDiffMap { Map = { { ""my_vector"", new VectorParamsDiff { HnswConfig = new HnswConfigDiff { M = 3, EfConstruct = 123 } } } } }, quantizationConfig: new QuantizationConfigDiff { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, Quantile = 0.8f, AlwaysRam = true } } ); ``` ## Collection info Qdrant allows determining the configuration parameters of an existing collection to better understand how the points are distributed and indexed. ```http GET /collections/test_collection1 ``` ```bash curl -X GET http://localhost:6333/collections/test_collection1 ``` ```python client.get_collection(collection_name=""{collection_name}"") ``` ```typescript client.getCollection(""{collection_name}""); ``` ```rust client.collection_info(""{collection_name}"").await?; ``` ```java client.getCollectionInfoAsync(""{collection_name}"").get(); ``` <details> <summary>Expected result</summary> ```json { ""result"": { ""status"": ""green"", ""optimizer_status"": ""ok"", ""vectors_count"": 1068786, ""indexed_vectors_count"": 1024232, ""points_count"": 1068786, ""segments_count"": 31, ""config"": { ""params"": { ""vectors"": { ""size"": 384, ""distance"": ""Cosine"" }, ""shard_number"": 1, ""replication_factor"": 1, ""write_consistency_factor"": 1, ""on_disk_payload"": false }, ""hnsw_config"": { ""m"": 16, ""ef_construct"": 100, ""full_scan_threshold"": 10000, ""max_indexing_threads"": 0 }, ""optimizer_config"": { ""deleted_threshold"": 0.2, ""vacuum_min_vector_number"": 1000, ""default_segment_number"": 0, ""max_segment_size"": null, ""memmap_threshold"": null, ""indexing_threshold"": 20000, ""flush_interval_sec"": 5, ""max_optimization_threads"": 1 }, ""wal_config"": { ""wal_capacity_mb"": 32, ""wal_segments_ahead"": 0 } }, ""payload_schema"": {} }, ""status"": ""ok"", ""time"": 0.00010143 } ``` </details> <br/> ```csharp await client.GetCollectionInfoAsync(""{collection_name}""); ``` If you insert the vectors into the collection, the `status` field may become `yellow` whilst it is optimizing. It will become `green` once all the points are successfully processed. The following color statuses are possible: - 🟢 `green`: collection is ready - 🟡 `yellow`: collection is optimizing - 🔴 `red`: an error occurred which the engine could not recover from ### Approximate point and vector counts You may be interested in the count attributes: - `points_count` - total number of objects (vectors and their payloads) stored in the collection - `vectors_count` - total number of vectors in a collection, useful if you have multiple vectors per point - `indexed_vectors_count` - total number of vectors stored in the HNSW or sparse index. Qdrant does not store all the vectors in the index, but only if an ,source:documentation/concepts/collections.md'
 ""document:`0`. # If not set, the default value will be used. indexing_threshold_kb: 20000 # Interval between forced flushes. flush_interval_sec: 5 # Max number of threads, which can be used for optimization per collection. # Note: Each optimization thread will also use `max_indexing_threads` for index building. # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads` # If `max_optimization_threads = 0`, optimization will be disabled. max_optimization_threads: 1 # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually hnsw_index: # Number of edges per node in the index graph. Larger the value - more accurate the search, more space required. m: 16 # Number of neighbours to consider during the index building. Larger the value - more accurate the search, more time required to build index. ef_construct: 100 # Minimal size (in KiloBytes) of vectors for additional payload-based indexing. # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won't be used - # in this case full-scan search should be preferred by query planner and additional indexing is not required. # Note: 1Kb = 1 vector of size 256 full_scan_threshold_kb: 10000 # Number of parallel threads used for background index building. If 0 - auto selection. max_indexing_threads: 0 # Store HNSW index on disk. If set to false, index will be stored in RAM. Default: false on_disk: false # Custom M param for hnsw graph built for payload index. If not set, default M will be used. payload_m: null service: # Maximum size of POST data in a single request in megabytes max_request_size_mb: 32 # Number of parallel workers used for serving the api. If 0 - equal to the number of available cores. # If missing - Same as storage.max_search_threads max_workers: 0 # Host to bind the service on host: 0.0.0.0 # HTTP(S) port to bind the service on http_port: 6333 # gRPC port to bind the service on. # If `null` - gRPC is disabled. Default: null # Comment to disable gRPC: grpc_port: 6334 # Enable CORS headers in REST API. # If enabled, browsers would be allowed to query REST endpoints regardless of query origin. # More info: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS # Default: true enable_cors: true # Enable HTTPS for the REST and gRPC API enable_tls: false # Check user HTTPS client certificate against CA file specified in tls config verify_https_client_certificate: false # Set an api-key. # If set, all requests must include a header with the api-key. # example header: `api-key: <API-KEY>` # # If you enable this you should also enable TLS. # (Either above or via an external service like nginx.) # Sending an api-key over an unencrypted channel is insecure. # # Uncomment to enable. # api_key: your_secret_api_key_here # Set an api-key for read-only operations. # If set, all requests must include a header with the api-key. # example header: `api-key: <API-KEY>` # # If you enable this you should also enable TLS. # (Either above or via an external service like nginx.) # Sending an api-key over an unencrypted channel is insecure. # # Uncomment to enable. ,source:documentation/guides/configuration.md""
 'document:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.md']","In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100"
How do you use ‘ordering’ parameter?,"To use the 'ordering' parameter, you can specify the type of ordering you want for your write operations. There are two options available: `medium` and `strong`. 

- `medium` ordering serializes all write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change.
- `strong` ordering serializes all write operations through the permanent leader, providing strong consistency, but write operations may be unavailable if the leader is down.

You can include the 'ordering' parameter in your API call or client code when performing write operations to specify the desired ordering type. For example, in the HTTP API call, you can include `ordering=strong` in the request URL. 

For more detailed implementation examples in various programming languages, you can refer to the documentation under the 'source' field of the fetched context from the documentation.","['document:can be freely reordered. - `medium` ordering serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change. - `strong` ordering serializes all write operations through the permanent leader, which provides strong consistency, but write operations may be unavailable if the leader is down. ```http PUT /collections/{collection_name}/points?ordering=strong { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ordering=""strong"", ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, ordering: ""strong"", }); ``` ```rust use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType}; use serde_json::json; client .upsert_points_blocking( ""{collection_name}"", None, vec![ PointStruct::new( 1, vec![0.9, 0.1, 0.1], json!({ ""color"": ""red"" }) .try_into() .unwrap(), ), PointStruct::new( 2, vec![0.1, 0.9, 0.1], json!({ ""color"": ""green"" }) .try_into() .unwrap(), ), PointStruct::new( 3, vec![0.1, 0.1, 0.9], json!({ ""color"": ""blue"" }) .try_into() .unwrap(), ), ], Some(WriteOrdering { r#type: WriteOrderingType::Strong.into(), }), ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.UpsertPoints; import io.qdrant.client.grpc.Points.WriteOrdering; import io.qdrant.client.grpc.Points.WriteOrderingType; client .upsertAsync( UpsertPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPoints( List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.9f, 0.1f, 0.1f)) .putAllPayload(Map.of(""color"", value(""red""))) .build(), PointStruct.newBuilder() .setId(id(2)) .setVectors(vectors(0.1f, 0.9f, 0.1f)) .putAllPayload(Map.of(""color"", value(""green""))) .build(), PointStruct.newBuilder() .setId(id(3)) .setVectors(vectors(0.1f, 0.1f, 0.94f)) .putAllPayload(Map.of(""color"", value(""blue""))) .build())) .setOrdering(WriteOrdering.newBuilder().setType(WriteOrderingType.Strong).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.9f, 0.1f, 0.1f }, Payload = { [""city""] = ""red"" } }, new() { Id = 2, Vectors = new[] { 0.1f, 0.9f, 0.1f }, Payload = { [""city""] = ""green"" } }, new() { Id = 3, Vectors = new[] { 0.1f, 0.1f, 0.9f }, Payload = { [""city""] = ""blue"" } } }, ordering: WriteOrderingType.Strong ); ``` ## Listener mode <aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside> In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations. There are several scenarios where this can be useful: - Listener option can be used to store data in a separate node, which can be used for backup purposes or to store data for a long time. - Listener node can be used to syncronize data into another region, while still performing search operations in the local region. To enable listener mode, set `node_type` to `Listener` in the config file: ```yaml storage: node_type: ""Listener"" ``` Listener node will not participate in search operations, but will still accept write operations and will store the data in the local storage. All shards, stored on the listener node, will be converted to the `Listener` state. Additionally, all write requests sent to ,source:documentation/guides/distributed_deployment.md'
 'document:} ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, }); ``` ```rust use qdrant_client::qdrant::RecommendPoints; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.RecommendPoints; client .recommendAsync( RecommendPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPositive(List.of(id(100), id(231))) .addAllNegative(List.of(id(718))) .setUsing(""image"") .setLimit(10) .build()) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.RecommendAsync( collectionName: ""{collection_name}"", positive: new ulong[] { 100, 231 }, negative: new ulong[] { 718 }, usingVector: ""image"", limit: 10 ); ``` Parameter `using` specifies which stored vectors to use for the recommendation. ### Lookup vectors from another collection *Available as of v0.11.6* If you have collections with vectors of the same dimensionality, and you want to look for recommendations in one collection based on the vectors of another collection, you can use the `lookup_from` parameter. It might be useful, e.g. in the item-to-user recommendations scenario. Where user and item embeddings, although having the same vector parameters (distance type and dimensionality), are usually stored in different collections. ```http POST /collections/{collection_name}/points/recommend { ""positive"": [100, 231], ""negative"": [718], ""using"": ""image"", ""limit"": 10, ""lookup_from"": { ""collection"":""{external_collection_name}"", ""vector"":""{external_vector_name}"" } } ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, lookup_from=models.LookupLocation( collection=""{external_collection_name}"", vector=""{external_vector_name}"" ), ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, lookup_from: { ""collection"" : ""{external_collection_name}"", ""vector"" : ""{external_vector_name}"" }, }); ``` ```rust use qdrant_client::qdrant::{LookupLocation, RecommendPoints}; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, lookup_from: Some(LookupLocation { collection_name: ""{external_collection_name}"".to_string(), vector_name: Some(""{external_vector_name}"".to_string()), ..Default::default() }), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.LookupLocation; import io.qdrant.client.grpc.Points.RecommendPoints; client .recommendAsync( RecommendPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPositive(List.of(id(100), id(231))) .addAllNegative(List.of(id(718))) .setUsing(""image"") .setLimit(10) .setLookupFrom( LookupLocation.newBuilder() .setCollectionName(""{external_collection_name}"") .setVectorName(""{external_vector_name}"") .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.RecommendAsync( collectionName: ""{collection_name}"", positive: new ulong[] { 100, 231 }, negative: new ulong[] { 718 }, usingVector: ""image"", limit: 10, lookupFrom: new LookupLocation { CollectionName = ""{external_collection_name}"", VectorName = ""{external_vector_name}"", } ); ``` Vectors are retrieved from the external collection by ids provided in the `positive` and `negative` lists. These vectors then used to perform the recommendation in the current collection, comparing against the ""using"" or default vector. ## Batch recommendation API *Available as of v0.10.0* Similar to the batch search API in terms of usage and advantages, it enables the batching of recommendation requests. ```http POST /collections/{collection_name}/points/recommend/batch { ""searches"": [ { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ] }, ""negative"": [718], ""positive"": [100, 231], ""limit"": 10 }, { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ] }, ""negative"": [300], ""positive"": [200, 67], ""limit"": 10 } ] } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) filter = models.Filter( must=[ models.FieldCondition( key=""city"", match=models.MatchValue( value=""London"", ), ) ] ) recommend_queries = [ models.RecommendRequest( positive=[100, 231], negative=[718], filter=filter, limit=3 ), models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3), ] client.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries) ``` ```typescript import { QdrantClient } ,source:documentation/concepts/explore.md'
 'document:} ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( should=[ models.FieldCondition( key=""city"", match=models.MatchValue(value=""London""), ), models.FieldCondition( key=""color"", match=models.MatchValue(value=""red""), ), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { should: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::should([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; import java.util.List; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllShould( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // | operator combines two conditions in an OR disjunction(should) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"") ); ``` Filtered points would be: ```json [ { ""id"": 1, ""city"": ""London"", ""color"": ""green"" }, { ""id"": 2, ""city"": ""London"", ""color"": ""red"" }, { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" }, { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" } ] ``` When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied. In this sense, `should` is equivalent to the operator `OR`. ### Must Not Example: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must_not"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } }, { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must_not=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must_not: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must_not([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllMustNot( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // The ! operator negates the condition(must not) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")) ); ``` Filtered points would be: ```json [ { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" }, { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" } ] ``` When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied. In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`. ### Clauses combination It is also possible to use several clauses simultaneously: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ], ""must_not"": [ { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), ], must_not=[ models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { key: ""city"", match: { value: ""London"" }, }, ], must_not: [ { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter { ,source:documentation/concepts/filtering.md']","Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request."
What is significance of ‘on_disk_payload’ setting?,"The 'on_disk_payload' setting in Qdrant determines whether the payload data associated with vectors is stored on disk or in memory. This setting impacts the memory footprint and search speed of the collection. When 'on_disk_payload' is set to true, the payload data is stored on disk, which can reduce memory usage but may slightly impact search speed. Conversely, setting 'on_disk_payload' to false stores the payload data in memory, potentially increasing search speed at the cost of higher memory consumption.

You can configure the 'on_disk_payload' setting when creating a collection by specifying it in the vectors configuration. For example, in the Python code snippet provided in the documentation, you can set 'on_disk_payload' to true or false based on your requirements.

For more detailed information on configuring storage options in Qdrant, you can refer to the documentation on storage [here](https://github.com/docs/documentation/concepts/storage.md).","['document:--- title: Storage weight: 80 aliases: - ../storage --- # Storage All data within one collection is divided into segments. Each segment has its independent vector and payload storage as well as indexes. Data stored in segments usually do not overlap. However, storing the same point in different segments will not cause problems since the search contains a deduplication mechanism. The segments consist of vector and payload storages, vector and payload [indexes](../indexing), and id mapper, which stores the relationship between internal and external ids. A segment can be `appendable` or `non-appendable` depending on the type of storage and index used. You can freely add, delete and query data in the `appendable` segment. With `non-appendable` segment can only read and delete data. The configuration of the segments in the collection can be different and independent of one another, but at least one `appendable\' segment must be present in a collection. ## Vector storage Depending on the requirements of the application, Qdrant can use one of the data storage options. The choice has to be made between the search speed and the size of the RAM used. **In-memory storage** - Stores all vectors in RAM, has the highest speed since disk access is required only for persistence. **Memmap storage** - Creates a virtual address space associated with the file on disk. [Wiki](https://en.wikipedia.org/wiki/Memory-mapped_file). Mmapped files are not directly loaded into RAM. Instead, they use page cache to access the contents of the file. This scheme allows flexible use of available memory. With sufficient RAM, it is almost as fast as in-memory storage. ### Configuring Memmap storage There are two ways to configure the usage of memmap(also known as on-disk) storage: - Set up `on_disk` option for the vectors in the collection create API: *Available as of v1.2.0* ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"", ""on_disk"": true } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams( size=768, distance=models.Distance.COSINE, on_disk=True ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", on_disk: true, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), on_disk: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( ""{collection_name}"", VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .setOnDisk(true) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( ""{collection_name}"", new VectorParams { Size = 768, Distance = Distance.Cosine, OnDisk = true } ); ``` This will create a collection with all vectors immediately stored in memmap storage. This is the recommended way, in case your Qdrant instance operates with fast disks and you are working with large collections. - Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage. There are two ways to do ,source:documentation/concepts/storage.md'
 'document:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.md'
 'document:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.md']","The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes."
What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` in Qdrant represents the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, you make write operations more tolerant to network partitions in the cluster. However, it will require a higher number of replicas to be active to perform write operations. This setting helps ensure data consistency and reliability during possible hardware instabilities or mass concurrent updates of the same documents. You can configure the `write_consistency_factor` at the collection's creation time as shown in the documentation snippet. For more details and examples, you can refer to the documentation on distributed deployment under the 'source' field provided.","['document:request dispatches all operations according to the current topology in order to keep the data synchronized across the cluster. - reads are using a partial fan-out strategy to optimize latency and availability - writes are executed in parallel on all active sharded replicas ![Embeddings](/docs/concurrent-operations-replicas.png) However, in some cases, it is necessary to ensure additional guarantees during possible hardware instabilities, mass concurrent updates of same documents, etc. Qdrant provides a few options to control consistency guarantees: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations. - Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low. - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical. ### Write consistency factor The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default. It can be configured at the collection\'s creation time. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 300, ""distance"": ""Cosine"" }, ""shard_number"": 6, ""replication_factor"": 2, ""write_consistency_factor"": 2, } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE), shard_number=6, replication_factor=2, write_consistency_factor=2, ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 300, distance: ""Cosine"", }, shard_number: 6, replication_factor: 2, write_consistency_factor: 2, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".into(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 300, distance: Distance::Cosine.into(), ..Default::default() })), }), shard_number: Some(6), replication_factor: Some(2), write_consistency_factor: Some(2), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(300) .setDistance(Distance.Cosine) .build()) .build()) .setShardNumber(6) .setReplicationFactor(2) .setWriteConsistencyFactor(2) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine }, shardNumber: ,source:documentation/guides/distributed_deployment.md'
 ""document:this consideration can lead not only to functional errors but also erode the trust of users due to inconsistency and confusion, which then leads to them no longer using my work. **3. Speak Up and Effectively Communicate** Finally, In the course of development, encountering differing opinions is commonplace. It's essential to remain open to others' ideas, while also possessing the resolve to communicate one's own perspective clearly. This fosters productive discussions and ultimately elevates the quality of the development process. ### Wrap up Being selected for Google Summer of Code 2023 and collaborating with Arnaud and the other Qdrant engineers, along with all the other community members, has been a true privilege. I'm deeply grateful to those who invested their time and effort in reviewing my code, engaging in discussions about alternatives and design choices, and offering assistance when needed. Through these interactions, I've experienced firsthand the essence of open source and the culture that encourages collaboration. This experience not only allowed me to write Rust code for a real-world product for the first time, but it also opened the door to the amazing world of open source. Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network! If you want to try out my work, read the [documentation](https://qdrant.tech/documentation/concepts/filtering/#geo-polygon) and then, either sign up for a free [cloud account](https://cloud.qdrant.io) or download the [Docker image](https://hub.docker.com/r/qdrant/qdrant). I look forward to seeing how people are using my work in their own applications!,source:articles/geo-polygon-filter-gsoc.md""
 'document:many queries are not IO bound, so the overhead may or may not become measurable in your workload. Finally, on-device disks typically carry lower latency than network drives, which may also affect mmap overhead. Therefore before you roll out io\\_uring, perform the above or a similar benchmark with both mmap and io\\_uring and measure both wall time and IOps). Benchmarks are always highly use-case dependent, so your mileage may vary. Still, doing that benchmark once is a small price for the possible performance wins. Also please [tell us](https://discord.com/channels/907569970500743200/907569971079569410) about your benchmark results!,source:articles/io_uring.md']","The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully."
