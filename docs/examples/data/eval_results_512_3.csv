,Index,question,answer,contexts,ground_truth,answer_relevancy_evaluator,aspect_critic_evaluator,context_utilization_evaluator,correctness_evaluator,faithfullness_evaluator,precision_evaluator,recall_evaluator,relevancy_evaluator,similarity_evaluator
0,0,What is vaccum optimizer ?,"The Vacuum Optimizer in Qdrant is used to address the accumulation of deleted records within a segment repository. Instead of immediately deleting entries after a query, Qdrant marks records as deleted and ignores them for future queries to minimize disk access. Over time, these deleted records can accumulate, occupy memory, and slow down the system. The Vacuum Optimizer is triggered when a segment has accumulated too many deleted records, as defined in the configuration file. You can set parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to perform segment optimization. This optimizer helps maintain system efficiency by managing deleted records effectively. You can find more details about the Vacuum Optimizer in the documentation under the ""Optimizer"" section, specifically in the ""Vacuum Optimizer"" subsection on page [source].","['document:--- title: Optimizer weight: 70 aliases: - ../optimizer --- # Optimizer It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely. Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)). In this case, the segment to be optimized remains readable for the time of the rebuild. ![Segment optimization](/docs/optimization.svg) The availability is achieved by wrapping the segment into a proxy that transparently handles data changes. Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates. ## Vacuum Optimizer The simplest example of a case where you need to rebuild a segment repository is to remove points. Like many other databases, Qdrant does not delete entries immediately after a query. Instead, it marks records as deleted and ignores them for future queries. This strategy allows us to minimize disk access - one of the slowest operations. However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system. To avoid these adverse effects, Vacuum Optimizer is used. It is used if the segment has accumulated too many deleted records. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # The minimal fraction of deleted vectors in a segment, required to perform segment optimization deleted_threshold: 0.2 # The minimal number of vectors in a segment, required to perform segment optimization vacuum_min_vector_number: 1000 ``` ## Merge Optimizer The service may require the creation of temporary segments. Such segments, for example, are created as copy-on-write segments during optimization itself. It is also essential to have at least one small segment that Qdrant will use to store frequently updated data. On the other hand, too many small segments lead to suboptimal search performance. There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # If the number of segments exceeds this value, the optimizer will merge the smallest segments. max_segment_number: 5 ``` ## Indexing Optimizer Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records. So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan. The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only ,source:documentation/concepts/optimizer.mddocument:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.mddocument:--- title: Optimize Resources weight: 11 aliases: - ../tutorials/optimize --- # Optimize Qdrant Different use cases have different requirements for balancing between memory, speed, and precision. Qdrant is designed to be flexible and customizable so you can tune it to your needs. ![Trafeoff](/docs/tradeoff.png) Let\'s look deeper into each of those possible optimization scenarios. ## Prefer low memory footprint with high speed search The main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads. Vector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true } } ); ``` `mmmap_threshold` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM. Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision. ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""rescore"": false } }, ""vector"": [0.2, 0.1, 0.9, 0.7], ""limit"": 10 } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.search( collection_name=""{collection_name}"", query_vector=[0.2, 0.1, 0.9, 0.7], search_params=models.SearchParams( quantization=models.QuantizationSearchParams(rescore=False) ), ) ``` ```typescript import { QdrantClient ,source:documentation/guides/optimize.md']","The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ",0.8946434517097585,0,0.9999999999,0.826935628111873,1.0,0.9999999999,1.0,0.011764705882352941,True
1,1,Tell me about ‘always_ram’ parameter?,"The `always_ram` parameter in the context of quantization configuration determines whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups where you might want to speed up the search process by keeping quantized vectors in RAM, you can set `always_ram` to `true`. This setting ensures that quantized vectors are stored in RAM for faster access during searches. You can find more information about this parameter in the documentation under the 'source' field of the fetched context from the 'quantization.md' guide.","['document:vectorsConfig: new VectorParams { Size = 1536, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Binary = new BinaryQuantization { AlwaysRam = true } } ); ``` `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. In this case, you can set `always_ram` to `true` to store quantized vectors in RAM. ### Setting up Product Quantization To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""product"": { ""compression"": ""x16"", ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ProductQuantization( product=models.ProductQuantizationConfig( compression=models.CompressionRatio.X16, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { product: { compression: ""x16"", always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CompressionRatio, CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Product(ProductQuantization { compression: CompressionRatio::X16.into(), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CompressionRatio; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.ProductQuantization; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setProduct( ProductQuantization.newBuilder() .setCompression(CompressionRatio.x16) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, quantizationConfig: new QuantizationConfig { Product = new ProductQuantization { Compression = CompressionRatio.X16, AlwaysRam = true } } ); ``` There are two parameters that you can specify in the `quantization_config` section: `compression` - compression ratio. Compression ratio represents the size of the quantized vector in bytes divided by the size of the original vector in bytes. In this case, the quantized vector will be 16 times smaller than the original vector. `always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`. ### Searching with Quantization Once you have configured quantization for a collection, you don\'t need to do anything extra to search with quantization. Qdrant will automatically use quantized vectors if they are available. However, there are a few options that you can use to control the search process: ```http POST /collections/{collection_name}/points/search { ""params"": { ""quantization"": { ""ignore"": false, ""rescore"": true, ""oversampling"": 2.0 } ,source:documentation/guides/quantization.mddocument:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.mddocument:amount of records is reached. The criteria for starting the optimizer are defined in the configuration file. Here is an example of parameter values: ```yaml storage: optimizers: # Maximum size (in kilobytes) of vectors to store in-memory per segment. # Segments larger than this threshold will be stored as read-only memmaped file. # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value. # To disable memmap storage, set this to `0`. # Note: 1Kb = 1 vector of size 256 memmap_threshold_kb: 200000 # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing # Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>. # To disable vector indexing, set to `0`. # Note: 1kB = 1 vector of size 256. indexing_threshold_kb: 20000 ``` In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections). Dynamic parameter updates may be useful, for example, for more efficient initial loading of points. You can disable indexing during the upload process with these settings and enable it immediately after it is finished. As a result, you will not waste extra computation resources on rebuilding the index.,source:documentation/concepts/optimizer.md']","In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.",0.8785362684146878,0,0.9999999999,0.8499707716238636,0.6666666666666666,0.9999999999,1.0,0.046511627906976744,True
2,2,What is difference between scalar and product quantization?,"Scalar Quantization and Product Quantization are two different methods used for quantization in vector search systems like Qdrant. 

Scalar Quantization involves converting floating-point values into integers, resulting in a significant reduction in memory usage. In Qdrant, `float32` values are converted to `int8`, leading to a 75% decrease in memory consumption. This method is reversible to some extent, allowing the conversion of integers back to floats with minimal loss of precision. Scalar Quantization is a good balance between accuracy, speed, and compression, making it a universal choice for quantization when binary quantization is not applicable (source: articles/scalar-quantization.md).

On the other hand, Product Quantization offers a higher compression rate compared to Scalar Quantization. However, this increased compression comes with trade-offs in accuracy and search speed. Product Quantization is favored in scenarios where memory footprint is a top priority and search speed is not critical. It is suitable for high-dimensional vectors and low-RAM environments where disk reads are the limiting factor. In cases where indexing speed is crucial or the scenarios do not align with the above conditions, Scalar Quantization is recommended (source: articles/product-quantization.md).

In summary, Scalar Quantization is more versatile and provides a good balance between accuracy and memory reduction, while Product Quantization offers higher compression but sacrifices accuracy and search speed. The choice between the two methods depends on the specific requirements of the use case, with Scalar Quantization being the default option in many scenarios.","['document:s</td> <td>474 s</td> </tr> </tbody> </table> It turns out that in some cases, Product Quantization may not only reduce the memory usage, but also the search time. ## Good practices Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed. Product Quantization tends to be favored in certain specific scenarios: - Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself - Situations where the dimensionality of the original vectors is sufficiently high - Cases where indexing speed is not a critical factor In circumstances that do not align with the above, Scalar Quantization should be the preferred choice. Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) will help you to set and configure the new quantization for your data and achieve even up to 64x memory reduction.,source:articles/product-quantization.mddocument:section for more information on how to optimize the quantization parameters for your use case. ## How to choose the right quantization method Here is a brief table of the pros and cons of each quantization method: | Quantization method | Accuracy | Speed | Compression | |---------------------|----------|--------------|-------------| | Scalar | 0.99 | up to x2 | 4 | | Product | 0.7 | 0.5 | up to 64 | | Binary | 0.95* | up to x40 | 32 | `*` - for compatible models * **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only. * **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable. * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical. ## Setting up Quantization in Qdrant You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration. Quantization will be automatically applied to all vectors during the indexation process. Quantized vectors are stored alongside the original vectors in the collection, so you will still have access to the original vectors if you need them. *Available as of v1.1.1* The `quantization_config` can also be set on a per vector basis by specifying it in a named vector. ### Setting up Scalar Quantization To enable scalar quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""quantile"": 0.99, ""always_ram"": true } } } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, quantile=0.99, always_ram=True, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, quantization_config: { scalar: { type: ""int8"", quantile: 0.99, always_ram: true, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), quantile: Some(0.99), always_ram: Some(true), })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.99f) .setAlwaysRam(true) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { ,source:documentation/guides/quantization.mddocument:--- title: ""Qdrant under the hood: Scalar Quantization"" short_description: ""Scalar Quantization is a newly introduced mechanism of reducing the memory footprint and increasing performance"" description: ""Scalar Quantization is a newly introduced mechanism of reducing the memory footprint and increasing performance"" social_preview_image: /articles_data/scalar-quantization/social_preview.png small_preview_image: /articles_data/scalar-quantization/scalar-quantization-icon.svg preview_dir: /articles_data/scalar-quantization/preview weight: 5 author: Kacper Łukawski author_link: https://medium.com/@lukawskikacper date: 2023-03-27T10:45:00+01:00 draft: false keywords: - vector search - scalar quantization - memory optimization --- High-dimensional vector embeddings can be memory-intensive, especially when working with large datasets consisting of millions of vectors. Memory footprint really starts being a concern when we scale things up. A simple choice of the data type used to store a single number impacts even billions of numbers and can drive the memory requirements crazy. The higher the precision of your type, the more accurately you can represent the numbers. The more accurate your vectors, the more precise is the distance calculation. But the advantages stop paying off when you need to order more and more memory. Qdrant chose `float32` as a default type used to store the numbers of your embeddings. So a single number needs 4 bytes of the memory and a 512-dimensional vector occupies 2 kB. That\'s only the memory used to store the vector. There is also an overhead of the HNSW graph, so as a rule of thumb we estimate the memory size with the following formula: ```text memory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes ``` While Qdrant offers various options to store some parts of the data on disk, starting from version 1.1.0, you can also optimize your memory by compressing the embeddings. We\'ve implemented the mechanism of **Scalar Quantization**! It turns out to have not only a positive impact on memory but also on the performance. ## Scalar Quantization Scalar quantization is a data compression technique that converts floating point values into integers. In case of Qdrant `float32` gets converted into `int8`, so a single number needs 75% less memory. It\'s not a simple rounding though! It\'s a process that makes that transformation partially reversible, so we can also revert integers back to floats with a small loss of precision. ### Theoretical background Assume we have a collection of `float32` vectors and denote a single value as `f32`. In reality neural embeddings do not cover a whole range represented by the floating point numbers, but rather a small subrange. Since we know all the other vectors, we can establish some statistics of all the numbers. For example, the distribution of the values will be typically normal: ![A distribution of the vector values](/articles_data/scalar-quantization/float32-distribution.png) Our example shows that 99% of the values come from a `[-2.0, 5.0]` range. And the conversion to `int8` will surely lose some precision, so we rather prefer keeping the representation accuracy within the range of 99% of the most probable values and ignoring the precision of the outliers. There might be a different choice of the range width, actually, any value from a range `[0, 1]`, where `0` means empty range, and `1` would keep all the values. That\'s a hyperparameter of ,source:articles/scalar-quantization.md']","While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.",0.8686197812333952,0,0.9999999999,0.7064801654246993,1.0,0.9999999999,1.0,0.05970149253731343,True
3,3,What is ‘best_score’ strategy?,"The 'best_score' strategy is a new recommendation strategy introduced in version 1.6.0. It focuses on finding similar vectors that are closer to a positive example while avoiding those closer to a negative one. This strategy calculates the best positive and best negative scores for each candidate point and chooses the final score based on these values. The formula used to calculate the score is: 

```rust
if best_positive_score > best_negative_score {
    score = best_positive_score;
} else {
    score = -(best_negative_score * best_negative_score);
}
```

This strategy aims to penalize points that are closer to negative examples and pulls the traversal procedure away from negative examples. It can be very powerful but may impact accuracy, especially with a large number of examples. To use the 'best_score' strategy, you need to set `""strategy"": ""best_score""` in the recommendation request. You can find more information about this strategy in the documentation under the 'Best score strategy' section. (Source: documentation/concepts/explore.md)","['document:step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation. The way to produce the searching vector is by first averaging all the positive and negative examples separately, and then combining them into a single vector using the following formula: ```rust avg_positive + avg_positive - avg_negative ``` In the case of not having any negative examples, the search vector will simply be equal to `avg_positive`. This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request. ### Best score strategy *Available as of v1.6.0* A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one. The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula: ```rust let score = if best_positive_score > best_negative_score { best_positive_score; } else { -(best_negative_score * best_negative_score); }; ``` <aside role=""alert""> The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples. </aside> Since we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic. <aside role=""status""> Accuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code> </aside> To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request. #### Using only negative examples A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one. Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning. ### Multiple vectors *Available as of v0.10.0* If the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request: ```http POST /collections/{collection_name}/points/recommend { ""positive"": [100, 231], ""negative"": [718], ""using"": ""image"", ""limit"": 10 } ``` ```python client.recommend( collection_name=""{collection_name}"", positive=[100, 231], negative=[718], using=""image"", limit=10, ) ``` ```typescript client.recommend(""{collection_name}"", { positive: [100, 231], negative: [718], using: ""image"", limit: 10, }); ``` ```rust use qdrant_client::qdrant::RecommendPoints; client .recommend(&RecommendPoints { collection_name: ""{collection_name}"".to_string(), positive: vec![100.into(), 231.into()], negative: vec![718.into()], using: Some(""image"".to_string()), limit: 10, ..Default::default() }) .await?; ``` ```java import java.util.List; ,source:documentation/concepts/explore.mddocument:for choosing the best match, which is also true in the case of vector recommendations. We can use different approaches to determine the path of traversing the HNSW graph by changing how we calculate the score of a candidate point during traversal. The default behaviour is based on pure distance, but Qdrant 1.6 exposes two strategies for the recommendation API. ### Average vector The default strategy, called `average_vector` is the previous one, based on the average of positive and negative examples. It simplifies the recommendations process and converts it into a single vector search. It supports both point IDs and vectors as parameters. For example, you can get recommendations based on past interactions with existing points combined with query vector embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the following formula: $$ \\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right) $$ The `average_vector` converts the problem of recommendations into a single vector search. ### The new hotness - Best score The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative samples and uses a slightly more sophisticated algorithm under the hood. The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a bunch of positive and negative queries**. As a result, for each sample in the query, we have a set of distances, one for each sample. In the next step, we simply take the best scores for positives and negatives, creating two separate values. Best scores are just the closest distances of a query to positives and negatives. The idea is: **if a point is closer to any negative than to any positive example, we do not want it**. We penalize being close to the negatives, so instead of using the similarity value directly, we check if it’s closer to positives or negatives. The following formula is used to calculate the score of a traversed potential point: ```rust if best_positive_score > best_negative_score { score = best_positive_score } else { score = -(best_negative_score * best_negative_score) } ``` If the point is closer to the negatives, we penalize it by taking the negative squared value of the best negative score. For a closer negative, the score of the candidate point will always be lower or equal to zero, making the chances of choosing that point significantly lower. However, if the best negative score is higher than the best positive score, we still prefer those that are further away from the negatives. That procedure effectively **pulls the traversal procedure away from the negative examples**. If you want to know more about the internals of HNSW, you can check out the article about the [Filtrable HNSW](https://qdrant.tech/articles/filtrable-hnsw/) that covers the topic thoroughly. ## Food Discovery demo Our [Food Discovery demo](https://qdrant.tech/articles/food-discovery-demo/) is an application built on ,source:articles/new-recommendation-api.mddocument:strategy. Mikko Lehtimäki: So when you get the query from the user, for example, you want to take different steps in order to answer it. For example, you may want to decompose the query to small questions that you answer individually, and each individual question may take a different path. So you may want to do a query based on metadata, for example pages five and six from a document. Or you may want to look up based on keywords full each page or chunk with a specific word. And there\'s really like a massive amount of choices how this can go. Another example is generating hypothetical documents based on the query and embedding those rather than the query itself. That will in some cases lead to higher quality retrieval results. But now all this leads into the right side of the query path. Mikko Lehtimäki: So here we have a re ranker. So if we implement all of this, we end up really retrieving a lot of data. We typically will retrieve more than it makes sense to give to the language model in a single call. So we can add a re ranker step here and it will firstly filter out low quality retrieved content and secondly, it will put the higher quality content on the top of the retrieved documents. And now when you pass this reranked content to the language model, it should be able to pay better attention to the details that actually matter given the query. And this should lead to you better managing the amount of data that you have to handle with your final response generator, LLM. And it should also make the response generator a bit faster because you will be feeding slightly less data in one go. The simplest way to build a re ranker is probably just asking a large language model to re rank or summarize the content that you\'ve retrieved before you feed it to the language model. Mikko Lehtimäki: That\'s one way to do it. So yeah, that\'s a lot of complexity and honestly, we\'re not doing all of this right now with Yokot AI, either. We\'ve tried all of it in different scopes, but really it\'s a lot of logic to maintain. And to me this just like screams the bitter lesson, because we\'re building so many steps, so much logic, so many rules into the system, when really all of this is done just because the language model can\'t be trusted, or it can\'t be with the current architectures trained reliably, or cannot be trained in real time with the current approaches that we have. So there\'s one thing in this picture, in my opinion, that is more promising than the others for leveraging data and compute, which should dominate the quality of the solution in the long term. And if we focus only on that, or not only, but if we focus heavily on that part of the process, we should be able to eliminate some complexity elsewhere. So if you\'re watching the recording, you ,source:blog/the-bitter-lesson-of-retrieval-in-generative-language-model-workflows-mikko-lehtimäki-vector-space-talks.md']","The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.",0.9786040655187987,0,0.9999999999,0.7736998507400532,1.0,0.9999999999,1.0,0.0136986301369863,True
4,4,How does oversampling helps?,"Oversampling helps to control the precision of search in real-time by internally retrieving more vectors than needed and assigning more precise scores during rescoring. This technique allows for better accuracy without rebuilding the index, as explained in the documentation on oversampling by Andrey Vasnetsov. You can find more details on oversampling and its benefits in the source document under the section discussing oversampling in model fairness and performance.","[""document:if we're using smaller models that aren't the GBTs, will that help? Andrey Vasnetsov: Right. So not all models are as big as OpenAI, but what we see, the trend in this area, the trend of development of different models, indicates that they will become bigger and bigger over time. Just because we want to store more information inside vectors, we want to have larger context, we want to have more detailed information, more detailed separation and so on. This trend is obvious if like five years ago the usual size of the vector was 100 dimensions now the usual size is 700 dimensions, so it's basically. Demetrios: Preparing for the future while also optimizing for today. Andrey Vasnetsov: Right? Demetrios: Yeah. Okay, so you mentioned on here oversampling. Can you go into that a little bit more and explain to me what that is? Andrey Vasnetsov: Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself. Demetrios: I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works. Andrey Vasnetsov: Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast. Andrey Vasnetsov: It uses very few CPU cycles to actually produce a result. That's why binary quantization is over 30 times faster than regular product. And it actually solves the problem of complicated index building, because this computation of dot products is the main source of computational requirements for HNSW. Demetrios: So if I'm understanding this correctly, it's basically taking all of these numbers that are on the left, which can be, yes, decimal numbers. Andrey Vasnetsov: On the left you can see original vector and it converts it in binary representation. And of course it does lose a lot of precision in the process. But because first we have very large vector and second, we have oversampling feature, we can compensate for ,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.mddocument:just the dominant one. Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects. ![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png) ### Leveraging Binary Quantization: Best Practices We recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings: 1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested. 2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages. 3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications. 4. Rescoring: Enable rescoring to improve the accuracy of search results. 5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible. Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant) Learn more about how to boost your vector search speed and accuracy while reducing costs: [Binary Quantization.](https://qdrant.tech/documentation/guides/quantization/?selector=aHRtbCA%2BIGJvZHkgPiBkaXY6bnRoLW9mLXR5cGUoMSkgPiBzZWN0aW9uID4gZGl2ID4gZGl2ID4gZGl2Om50aC1vZi10eXBlKDIpID4gYXJ0aWNsZSA%2BIGgyOm50aC1vZi10eXBlKDIp),source:articles/binary-quantization-openai.mddocument:an oversampling of 3 and a limit of 100, we've a 95% recall against the exact nearest neighbors with rescore enabled. | oversampling | | 1 | 1 | 2 | 2 | 3 | 3 | |--------------|---------|----------|----------|----------|----------|----------|----------| | limit | | | | | | | | | | rescore | False | True | False | True | False | True | | 10 | | 0.523333 | 0.831111 | 0.523333 | 0.915556 | 0.523333 | 0.950000 | | 20 | | 0.510000 | 0.836667 | 0.510000 | 0.912222 | 0.510000 | 0.937778 | | 50 | | 0.489111 | 0.841556 | 0.489111 | 0.913333 | 0.488444 | 0.947111 | | 100 | | 0.485778 | 0.846556 | 0.485556 | 0.929000 | 0.486000 | **0.956333** | That's it! You can now use Gemini Embedding Models with Qdrant!,source:documentation/embeddings/gemini.md""]","Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.",0.9037460272316094,0,0.9999999999,0.8054511147458343,0.5,0.9999999999,0.6666666666666666,0.07407407407407407,False
5,5,What is the purpose of ‘CreatePayloadIndexAsync’?,"The purpose of the `CreatePayloadIndexAsync` function is to create a keyword payload index for a specific field in a collection. This function allows you to define the field name, field schema (in this case, keyword), and any additional parameters related to the index creation. It is used to optimize search and retrieval operations based on the specified field in the collection. You can find more details about this function in the documentation under the 'source' field of fetched context from the documentation.","['document:Some(Operation::SetPayload(SetPayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), payload: HashMap::from([ (""test_payload_2"".to_string(), 2.into()), (""test_payload_3"".to_string(), 3.into()), ]), })), }, PointsUpdateOperation { operation: Some(Operation::DeletePayload(DeletePayload { points_selector: Some(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points( PointsIdsList { ids: vec![1.into()], }, )), }), keys: vec![""test_payload_2"".to_string()], })), }, PointsUpdateOperation { operation: Some(Operation::ClearPayload(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, PointsUpdateOperation { operation: Some(Operation::Delete(PointsSelector { points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList { ids: vec![1.into()], })), })), }, ], None, ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.PointVectors; import io.qdrant.client.grpc.Points.PointsIdsList; import io.qdrant.client.grpc.Points.PointsSelector; import io.qdrant.client.grpc.Points.PointsUpdateOperation; import io.qdrant.client.grpc.Points.PointsUpdateOperation.ClearPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeletePoints; import io.qdrant.client.grpc.Points.PointsUpdateOperation.DeleteVectors; import io.qdrant.client.grpc.Points.PointsUpdateOperation.PointStructList; import io.qdrant.client.grpc.Points.PointsUpdateOperation.SetPayload; import io.qdrant.client.grpc.Points.PointsUpdateOperation.UpdateVectors; import io.qdrant.client.grpc.Points.VectorsSelector; client .batchUpdateAsync( ""{collection_name}"", List.of( PointsUpdateOperation.newBuilder() .setUpsert( PointStructList.newBuilder() .addPoints( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setUpdateVectors( UpdateVectors.newBuilder() .addPoints( PointVectors.newBuilder() .setId(id(1)) .setVectors(vectors(1.0f, 2.0f, 3.0f, 4.0f)) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeleteVectors( DeleteVectors.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .setVectors(VectorsSelector.newBuilder().addNames("""").build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setOverwritePayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload(Map.of(""test_payload"", value(1))) .build()) .build(), PointsUpdateOperation.newBuilder() .setSetPayload( SetPayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .putAllPayload( Map.of(""test_payload_2"", value(2), ""test_payload_3"", value(3))) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePayload( DeletePayload.newBuilder() .setPointsSelector( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .addKeys(""test_payload_2"") .build()) .build(), PointsUpdateOperation.newBuilder() .setClearPayload( ClearPayload.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build(), PointsUpdateOperation.newBuilder() .setDeletePoints( DeletePoints.newBuilder() .setPoints( PointsSelector.newBuilder() .setPoints(PointsIdsList.newBuilder().addIds(id(1)).build()) .build()) .build()) .build())) .get(); ``` To batch many points with a single operation type, please use batching functionality in that operation directly.,source:documentation/concepts/points.mddocument:```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""create_alias"": { ""collection_name"": ""test_collection1"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.CreateAliasOperation( create_alias=models.CreateAlias( collection_name=""example_collection"", alias_name=""production_collection"" ) ) ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { create_alias: { collection_name: ""example_collection"", alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.create_alias(""example_collection"", ""production_collection"").await?; ``` ```java client.createAliasAsync(""production_collection"", ""example_collection"").get(); ``` ```csharp await client.CreateAliasAsync(aliasName: ""production_collection"", collectionName: ""example_collection""); ``` ### Remove alias ```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""delete_alias"": { ""collection_name"": ""test_collection1"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```http POST /collections/aliases { ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } } ] } ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.DeleteAliasOperation( delete_alias=models.DeleteAlias(alias_name=""production_collection"") ), ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { delete_alias: { alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.delete_alias(""production_collection"").await?; ``` ```java client.deleteAliasAsync(""production_collection"").get(); ``` ```csharp await client.DeleteAliasAsync(""production_collection""); ``` ### Switch collection Multiple alias actions are performed atomically. For example, you can switch underlying collection with the following command: ```http POST /collections/aliases { ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } }, { ""create_alias"": { ""collection_name"": ""test_collection2"", ""alias_name"": ""production_collection"" } } ] } ``` ```bash curl -X POST http://localhost:6333/collections/aliases \\ -H \'Content-Type: application/json\' \\ --data-raw \'{ ""actions"": [ { ""delete_alias"": { ""alias_name"": ""production_collection"" } }, { ""create_alias"": { ""collection_name"": ""test_collection2"", ""alias_name"": ""production_collection"" } } ] }\' ``` ```python client.update_collection_aliases( change_aliases_operations=[ models.DeleteAliasOperation( delete_alias=models.DeleteAlias(alias_name=""production_collection"") ), models.CreateAliasOperation( create_alias=models.CreateAlias( collection_name=""example_collection"", alias_name=""production_collection"" ) ), ] ) ``` ```typescript client.updateCollectionAliases({ actions: [ { delete_alias: { alias_name: ""production_collection"", }, }, { create_alias: { collection_name: ""example_collection"", alias_name: ""production_collection"", }, }, ], }); ``` ```rust client.delete_alias(""production_collection"").await?; client.create_alias(""example_collection"", ""production_collection"").await?; ``` ```java client.deleteAliasAsync(""production_collection"").get(); client.createAliasAsync(""production_collection"", ""example_collection"").get(); ``` ```csharp await client.DeleteAliasAsync(""production_collection""); await client.CreateAliasAsync(aliasName: ""production_collection"", collectionName: ""example_collection""); ``` ### List collection aliases ```http GET /collections/test_collection2/aliases ``` ```bash curl -X GET http://localhost:6333/collections/test_collection2/aliases ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_collection_aliases(collection_name=""{collection_name}"") ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getCollectionAliases(""{collection_name}""); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_collection_aliases(""{collection_name}"").await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listCollectionAliasesAsync(""{collection_name}"").get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.ListCollectionAliasesAsync(""{collection_name}""); ``` ### List all aliases ```http GET /aliases ``` ```bash curl -X GET http://localhost:6333/aliases ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_aliases() ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getAliases(); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_aliases().await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listAliasesAsync().get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.ListAliasesAsync(); ``` ### List all collections ```http GET /collections ``` ```bash curl -X GET http://localhost:6333/collections ``` ```python from qdrant_client import QdrantClient client = QdrantClient(""localhost"", port=6333) client.get_collections() ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.getCollections(); ``` ```rust use qdrant_client::client::QdrantClient; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client.list_collections().await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client.listCollectionsAsync().get(); ``` ```csharp using Qdrant.Client; var client ,source:documentation/concepts/collections.mddocument:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.md']",CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,0.9791735920543108,0,0.9999999999,0.7349447903184112,0.6666666666666666,0.9999999999,0.3333333333333333,0.009259259259259259,False
6,6,What is the purpose of ef_construct in HNSW ?,The `ef_construct` parameter in HNSW (Hierarchical Navigable Small World) is used to control the search performance and accuracy during the construction phase of the index. It specifies the number of neighbors to consider during the exploration of the graph structure. A higher value of `ef_construct` can lead to more accurate search results but may also increase the computational cost. You can find more information about the `ef_construct` parameter in the documentation under the 'Collection info' section. (source: documentation/concepts/collections.md),"['document:), ) ``` ```typescript client.updateCollection(""{collection_name}"", { vectors: { my_vector: { hnsw_config: { m: 32, ef_construct: 123, }, quantization_config: { product: { compression: ""x32"", always_ram: true, }, }, on_disk: true, }, }, hnsw_config: { ef_construct: 123, }, quantization_config: { scalar: { type: ""int8"", quantile: 0.8, always_ram: true, }, }, }); ``` ```rust use qdrant_client::client::QdrantClient; use qdrant_client::qdrant::{ quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff, QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff, VectorsConfigDiff, }; client .update_collection( ""{collection_name}"", None, None, None, Some(&HnswConfigDiff { ef_construct: Some(123), ..Default::default() }), Some(&VectorsConfigDiff { config: Some(Config::ParamsMap( qdrant_client::qdrant::VectorParamsDiffMap { map: HashMap::from([( (""my_vector"".into()), VectorParamsDiff { hnsw_config: Some(HnswConfigDiff { m: Some(32), ef_construct: Some(123), ..Default::default() }), ..Default::default() }, )]), }, )), }), Some(&QuantizationConfigDiff { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8 as i32, quantile: Some(0.8), always_ram: Some(true), ..Default::default() })), }), ) .await?; ``` ```java import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.UpdateCollection; import io.qdrant.client.grpc.Collections.VectorParamsDiff; import io.qdrant.client.grpc.Collections.VectorParamsDiffMap; import io.qdrant.client.grpc.Collections.VectorsConfigDiff; client .updateCollectionAsync( UpdateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setHnswConfig(HnswConfigDiff.newBuilder().setEfConstruct(123).build()) .setVectorsConfig( VectorsConfigDiff.newBuilder() .setParamsMap( VectorParamsDiffMap.newBuilder() .putMap( ""my_vector"", VectorParamsDiff.newBuilder() .setHnswConfig( HnswConfigDiff.newBuilder() .setM(3) .setEfConstruct(123) .build()) .build()))) .setQuantizationConfig( QuantizationConfigDiff.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setQuantile(0.8f) .setAlwaysRam(true) .build())) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpdateCollectionAsync( collectionName: ""{collection_name}"", hnswConfig: new HnswConfigDiff { EfConstruct = 123 }, vectorsConfig: new VectorParamsDiffMap { Map = { { ""my_vector"", new VectorParamsDiff { HnswConfig = new HnswConfigDiff { M = 3, EfConstruct = 123 } } } } }, quantizationConfig: new QuantizationConfigDiff { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, Quantile = 0.8f, AlwaysRam = true } } ); ``` ## Collection info Qdrant allows determining the configuration parameters of an existing collection to better understand how the points are distributed and indexed. ```http GET /collections/test_collection1 ``` ```bash curl -X GET http://localhost:6333/collections/test_collection1 ``` ```python client.get_collection(collection_name=""{collection_name}"") ``` ```typescript client.getCollection(""{collection_name}""); ``` ```rust client.collection_info(""{collection_name}"").await?; ``` ```java client.getCollectionInfoAsync(""{collection_name}"").get(); ``` <details> <summary>Expected result</summary> ```json { ""result"": { ""status"": ""green"", ""optimizer_status"": ""ok"", ""vectors_count"": 1068786, ""indexed_vectors_count"": 1024232, ""points_count"": 1068786, ""segments_count"": 31, ""config"": { ""params"": { ""vectors"": { ""size"": 384, ""distance"": ""Cosine"" }, ""shard_number"": 1, ""replication_factor"": 1, ""write_consistency_factor"": 1, ""on_disk_payload"": false }, ""hnsw_config"": { ""m"": 16, ""ef_construct"": 100, ""full_scan_threshold"": 10000, ""max_indexing_threads"": 0 }, ""optimizer_config"": { ""deleted_threshold"": 0.2, ""vacuum_min_vector_number"": 1000, ""default_segment_number"": 0, ""max_segment_size"": null, ""memmap_threshold"": null, ""indexing_threshold"": 20000, ""flush_interval_sec"": 5, ""max_optimization_threads"": 1 }, ""wal_config"": { ""wal_capacity_mb"": 32, ""wal_segments_ahead"": 0 } }, ""payload_schema"": {} }, ""status"": ""ok"", ""time"": 0.00010143 } ``` </details> <br/> ```csharp await client.GetCollectionInfoAsync(""{collection_name}""); ``` If you insert the vectors into the collection, the `status` field may become `yellow` whilst it is optimizing. It will become `green` once all the points are successfully processed. The following color statuses are possible: - 🟢 `green`: collection is ready - 🟡 `yellow`: collection is optimizing - 🔴 `red`: an error occurred which the engine could not recover from ### Approximate point and vector counts You may be interested in the count attributes: - `points_count` - total number of objects (vectors and their payloads) stored in the collection - `vectors_count` - total number of vectors in a collection, useful if you have multiple vectors per point - `indexed_vectors_count` - total number of vectors stored in the HNSW or sparse index. Qdrant does not store all the vectors in the index, but only if an ,source:documentation/concepts/collections.mddocument:import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.HnswConfigDiff; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setHnswConfig(HnswConfigDiff.newBuilder().setPayloadM(16).setM(0).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, hnswConfig: new HnswConfigDiff { PayloadM = 16, M = 0 } ); ``` 3. Create keyword payload index for `group_id` field. ```http PUT /collections/{collection_name}/index { ""field_name"": ""group_id"", ""field_schema"": ""keyword"" } ``` ```python client.create_payload_index( collection_name=""{collection_name}"", field_name=""group_id"", field_schema=models.PayloadSchemaType.KEYWORD, ) ``` ```typescript client.createPayloadIndex(""{collection_name}"", { field_name: ""group_id"", field_schema: ""keyword"", }); ``` ```rust use qdrant_client::{client::QdrantClient, qdrant::FieldType}; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_field_index( ""{collection_name}"", ""group_id"", FieldType::Keyword, None, None, ) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.PayloadSchemaType; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createPayloadIndexAsync( ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null) .get(); ``` ```csharp using Qdrant.Client; var client = new QdrantClient(""localhost"", 6334); await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id""); ``` ## Limitations One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.mddocument:io.qdrant.client.grpc.Points.PointStruct; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .upsertAsync( ""{collection_name}"", List.of( PointStruct.newBuilder() .setId(id(UUID.fromString(""5c56c793-69f3-4fbf-87e6-c4bf54c28c26""))) .setVectors(vectors(0.05f, 0.61f, 0.76f, 0.74f)) .putAllPayload(Map.of(""color"", value(""Red""))) .build())) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = Guid.Parse(""5c56c793-69f3-4fbf-87e6-c4bf54c28c26""), Vectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f }, Payload = { [""city""] = ""red"" } } } ); ``` and ```http PUT /collections/{collection_name}/points { ""points"": [ { ""id"": 1, ""payload"": {""color"": ""red""}, ""vector"": [0.9, 0.1, 0.1] } ] } ``` ```python client.upsert( collection_name=""{collection_name}"", points=[ models.PointStruct( id=1, payload={ ""color"": ""red"", }, vector=[0.9, 0.1, 0.1], ), ], ) ``` ```typescript client.upsert(""{collection_name}"", { points: [ { id: 1, payload: { color: ""red"", }, vector: [0.9, 0.1, 0.1], }, ], }); ``` ```rust use qdrant_client::qdrant::PointStruct; use serde_json::json; client .upsert_points_blocking( 1, None, vec![PointStruct::new( ""5c56c793-69f3-4fbf-87e6-c4bf54c28c26"".to_string(), vec![0.05, 0.61, 0.76, 0.74], json!( {""color"": ""Red""} ) .try_into() .unwrap(), )], None, ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Points.PointStruct; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .upsertAsync( ""{collection_name}"", List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.05f, 0.61f, 0.76f, 0.74f)) .putAllPayload(Map.of(""color"", value(""Red""))) .build())) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f }, Payload = { [""city""] = ""red"" } } } ); ``` are both possible. ## Upload points To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call. Batching allows you to minimize the overhead of creating a network connection. The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. Create points with batch: ```http PUT /collections/{collection_name}/points { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, }); ``` or record-oriented equivalent: ```http PUT /collections/{collection_name}/points { ""points"": [ { ""id"": 1, ""payload"": {""color"": ""red""}, ""vector"": [0.9, 0.1, 0.1] }, { ""id"": 2, ""payload"": {""color"": ""green""}, ""vector"": [0.1, 0.9, 0.1] }, { ""id"": 3, ""payload"": {""color"": ""blue""}, ""vector"": [0.1, 0.1, 0.9] } ] } ``` ```python client.upsert( collection_name=""{collection_name}"", points=[ models.PointStruct( id=1, payload={ ""color"": ""red"", }, vector=[0.9, 0.1, 0.1], ), models.PointStruct( id=2, payload={ ""color"": ""green"", }, vector=[0.1, 0.9, 0.1], ), models.PointStruct( id=3, payload={ ""color"": ""blue"", }, vector=[0.1, 0.1, 0.9], ), ], ) ``` ```typescript client.upsert(""{collection_name}"", { points: [ { id: 1, payload: { color: ""red"" }, vector: [0.9, 0.1, 0.1], }, { id: ,source:documentation/concepts/points.md']","In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100",0.9552003976985409,0,0.9999999999,0.4829040992263529,0.8333333333333334,0.9999999999,0.3333333333333333,0.011494252873563218,True
7,7,How do you use ‘ordering’ parameter?,"To use the 'ordering' parameter, you can specify the desired ordering type when performing write operations in Qdrant. There are two options available:

1. `medium` ordering: This option serializes all write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change.
2. `strong` ordering: This option serializes all write operations through the permanent leader, providing strong consistency. However, write operations may be unavailable if the leader is down.

You can set the 'ordering' parameter in your code when performing write operations. Here is an example in different programming languages:

Python:
```python
http PUT /collections/{collection_name}/points?ordering=strong { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } }
```

TypeScript:
```typescript
client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, ordering: ""strong"", });
```

Rust:
```rust
client.upsert_points_blocking( ""{collection_name}"", None, vec![ PointStruct::new( 1, vec![0.9, 0.1, 0.1], json!({ ""color"": ""red"" }) .try_into() .unwrap(), ), PointStruct::new( 2, vec![0.1, 0.9, 0.1], json!({ ""color"": ""green"" }) .try_into() .unwrap(), ), PointStruct::new( 3, vec![0.1, 0.1, 0.9], json!({ ""color"": ""blue"" }) .try_into() .unwrap(), ), ], Some(WriteOrdering { r#type: WriteOrderingType::Strong.into(), }), ).await?;
```

Java:
```java
client .upsertAsync( UpsertPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPoints( List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.9f, 0.1f, 0.1f)) .putAllPayload(Map.of(""color"", value(""red""))) .build(), PointStruct.newBuilder() .setId(id(2)) .setVectors(vectors(0.1f, 0.9f, 0.1f)) .putAllPayload(Map.of","['document:can be freely reordered. - `medium` ordering serializes all write operations through a dynamically elected leader, which might cause minor inconsistencies in case of leader change. - `strong` ordering serializes all write operations through the permanent leader, which provides strong consistency, but write operations may be unavailable if the leader is down. ```http PUT /collections/{collection_name}/points?ordering=strong { ""batch"": { ""ids"": [1, 2, 3], ""payloads"": [ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""} ], ""vectors"": [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9] ] } } ``` ```python client.upsert( collection_name=""{collection_name}"", points=models.Batch( ids=[1, 2, 3], payloads=[ {""color"": ""red""}, {""color"": ""green""}, {""color"": ""blue""}, ], vectors=[ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], ), ordering=""strong"", ) ``` ```typescript client.upsert(""{collection_name}"", { batch: { ids: [1, 2, 3], payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }], vectors: [ [0.9, 0.1, 0.1], [0.1, 0.9, 0.1], [0.1, 0.1, 0.9], ], }, ordering: ""strong"", }); ``` ```rust use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType}; use serde_json::json; client .upsert_points_blocking( ""{collection_name}"", None, vec![ PointStruct::new( 1, vec![0.9, 0.1, 0.1], json!({ ""color"": ""red"" }) .try_into() .unwrap(), ), PointStruct::new( 2, vec![0.1, 0.9, 0.1], json!({ ""color"": ""green"" }) .try_into() .unwrap(), ), PointStruct::new( 3, vec![0.1, 0.1, 0.9], json!({ ""color"": ""blue"" }) .try_into() .unwrap(), ), ], Some(WriteOrdering { r#type: WriteOrderingType::Strong.into(), }), ) .await?; ``` ```java import java.util.List; import java.util.Map; import static io.qdrant.client.PointIdFactory.id; import static io.qdrant.client.ValueFactory.value; import static io.qdrant.client.VectorsFactory.vectors; import io.qdrant.client.grpc.Points.PointStruct; import io.qdrant.client.grpc.Points.UpsertPoints; import io.qdrant.client.grpc.Points.WriteOrdering; import io.qdrant.client.grpc.Points.WriteOrderingType; client .upsertAsync( UpsertPoints.newBuilder() .setCollectionName(""{collection_name}"") .addAllPoints( List.of( PointStruct.newBuilder() .setId(id(1)) .setVectors(vectors(0.9f, 0.1f, 0.1f)) .putAllPayload(Map.of(""color"", value(""red""))) .build(), PointStruct.newBuilder() .setId(id(2)) .setVectors(vectors(0.1f, 0.9f, 0.1f)) .putAllPayload(Map.of(""color"", value(""green""))) .build(), PointStruct.newBuilder() .setId(id(3)) .setVectors(vectors(0.1f, 0.1f, 0.94f)) .putAllPayload(Map.of(""color"", value(""blue""))) .build())) .setOrdering(WriteOrdering.newBuilder().setType(WriteOrderingType.Strong).build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.UpsertAsync( collectionName: ""{collection_name}"", points: new List<PointStruct> { new() { Id = 1, Vectors = new[] { 0.9f, 0.1f, 0.1f }, Payload = { [""city""] = ""red"" } }, new() { Id = 2, Vectors = new[] { 0.1f, 0.9f, 0.1f }, Payload = { [""city""] = ""green"" } }, new() { Id = 3, Vectors = new[] { 0.1f, 0.1f, 0.9f }, Payload = { [""city""] = ""blue"" } } }, ordering: WriteOrderingType.Strong ); ``` ## Listener mode <aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside> In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations. There are several scenarios where this can be useful: - Listener option can be used to store data in a separate node, which can be used for backup purposes or to store data for a long time. - Listener node can be used to syncronize data into another region, while still performing search operations in the local region. To enable listener mode, set `node_type` to `Listener` in the config file: ```yaml storage: node_type: ""Listener"" ``` Listener node will not participate in search operations, but will still accept write operations and will store the data in the local storage. All shards, stored on the listener node, will be converted to the `Listener` state. Additionally, all write requests sent to ,source:documentation/guides/distributed_deployment.mddocument:""must"": [ { ""key"": ""food"", ""match"": { ""value"": ""meat"" } }, { ""key"": ""likes"", ""match"": { ""value"": true } } ] } } }] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.NestedCondition( nested=models.Nested( key=""diet"", filter=models.Filter( must=[ models.FieldCondition( key=""food"", match=models.MatchValue(value=""meat"") ), models.FieldCondition( key=""likes"", match=models.MatchValue(value=True) ), ] ), ) ) ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { nested: { key: ""diet"", filter: { must: [ { key: ""food"", match: { value: ""meat"" }, }, { key: ""likes"", match: { value: true }, }, ], }, }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, NestedCondition, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must([NestedCondition { key: ""diet"".to_string(), filter: Some(Filter::must([ Condition::matches(""food"", ""meat"".to_string()), Condition::matches(""likes"", true), ])), } .into()])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.match; import static io.qdrant.client.ConditionFactory.matchKeyword; import static io.qdrant.client.ConditionFactory.nested; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addMust( nested( ""diet"", Filter.newBuilder() .addAllMust( List.of( matchKeyword(""food"", ""meat""), match(""likes"", true))) .build())) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); await client.ScrollAsync( collectionName: ""{collection_name}"", filter: Nested(""diet"", MatchKeyword(""food"", ""meat"") & Match(""likes"", true)) ); ``` The matching logic is modified to be applied at the level of an array element within the payload. Nested filters work in the same way as if the nested filter was applied to a single element of the array at a time. Parent document is considered to match the condition if at least one element of the array matches the nested filter. **Limitations** The `has_id` condition is not supported within the nested object filter. If you need it, place it in an adjacent `must` clause. ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must"": [ ""nested"": { { ""key"": ""diet"", ""filter"":{ ""must"": [ { ""key"": ""food"", ""match"": { ""value"": ""meat"" } }, { ""key"": ""likes"", ""match"": { ""value"": true } } ] } } }, { ""has_id"": [1] } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.NestedCondition( nested=models.Nested( key=""diet"", filter=models.Filter( must=[ models.FieldCondition( key=""food"", match=models.MatchValue(value=""meat"") ), models.FieldCondition( key=""likes"", match=models.MatchValue(value=True) ), ] ), ) ), models.HasIdCondition(has_id=[1]), ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { nested: { key: ""diet"", filter: { must: [ { key: ""food"", match: { value: ""meat"" }, }, { key: ""likes"", match: { value: true }, }, ], }, }, }, { has_id: [1], }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, NestedCondition, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must([ NestedCondition { key: ""diet"".to_string(), filter: Some(Filter::must([ Condition::matches(""food"", ""meat"".to_string()), Condition::matches(""likes"", true), ])), } .into(), Condition::has_id([1]), ])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.hasId; import static io.qdrant.client.ConditionFactory.match; import static io.qdrant.client.ConditionFactory.matchKeyword; import static io.qdrant.client.ConditionFactory.nested; import static io.qdrant.client.PointIdFactory.id; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addMust( nested( ""diet"", Filter.newBuilder() .addAllMust( List.of( matchKeyword(""food"", ""meat""), match(""likes"", true))) .build())) .addMust(hasId(id(1))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); await client.ScrollAsync( collectionName: ""{collection_name}"", filter: Nested(""diet"", MatchKeyword(""food"", ""meat"") & Match(""likes"", true)) & HasId(1) ); ``` ### Full Text Match *Available as of v0.10.0* A special case of the `match` condition is the `text` match ,source:documentation/concepts/filtering.mddocument:} ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( should=[ models.FieldCondition( key=""city"", match=models.MatchValue(value=""London""), ), models.FieldCondition( key=""color"", match=models.MatchValue(value=""red""), ), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { should: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::should([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; import java.util.List; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllShould( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // | operator combines two conditions in an OR disjunction(should) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"") ); ``` Filtered points would be: ```json [ { ""id"": 1, ""city"": ""London"", ""color"": ""green"" }, { ""id"": 2, ""city"": ""London"", ""color"": ""red"" }, { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" }, { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" } ] ``` When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied. In this sense, `should` is equivalent to the operator `OR`. ### Must Not Example: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must_not"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } }, { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must_not=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ] ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must_not: [ { key: ""city"", match: { value: ""London"" }, }, { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter::must_not([ Condition::matches(""city"", ""london"".to_string()), Condition::matches(""color"", ""red"".to_string()), ])), ..Default::default() }) .await?; ``` ```java import java.util.List; import static io.qdrant.client.ConditionFactory.matchKeyword; import io.qdrant.client.grpc.Points.Filter; import io.qdrant.client.grpc.Points.ScrollPoints; client .scrollAsync( ScrollPoints.newBuilder() .setCollectionName(""{collection_name}"") .setFilter( Filter.newBuilder() .addAllMustNot( List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red""))) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using static Qdrant.Client.Grpc.Conditions; var client = new QdrantClient(""localhost"", 6334); // The ! operator negates the condition(must not) await client.ScrollAsync( collectionName: ""{collection_name}"", filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")) ); ``` Filtered points would be: ```json [ { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" }, { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" } ] ``` When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied. In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`. ### Clauses combination It is also possible to use several clauses simultaneously: ```http POST /collections/{collection_name}/points/scroll { ""filter"": { ""must"": [ { ""key"": ""city"", ""match"": { ""value"": ""London"" } } ], ""must_not"": [ { ""key"": ""color"", ""match"": { ""value"": ""red"" } } ] } } ``` ```python client.scroll( collection_name=""{collection_name}"", scroll_filter=models.Filter( must=[ models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")), ], must_not=[ models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")), ], ), ) ``` ```typescript client.scroll(""{collection_name}"", { filter: { must: [ { key: ""city"", match: { value: ""London"" }, }, ], must_not: [ { key: ""color"", match: { value: ""red"" }, }, ], }, }); ``` ```rust use qdrant_client::qdrant::{Condition, Filter, ScrollPoints}; client .scroll(&ScrollPoints { collection_name: ""{collection_name}"".to_string(), filter: Some(Filter { ,source:documentation/concepts/filtering.md']","Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.",0.8421246213729302,0,0.9999999999,0.7328484387870013,0.3888888888888889,0.9999999999,1.0,0.008849557522123894,False
8,8,What is significance of ‘on_disk_payload’ setting?,"The 'on_disk_payload' setting in Qdrant determines whether the payload data associated with vectors is stored on disk or in memory. When this setting is enabled, it allows for the payload data to be stored on disk, reducing the memory footprint of the service. This setting is particularly useful when dealing with large collections and fast storage devices like SSDs or NVMe.

You can configure the 'on_disk_payload' setting by setting the 'on_disk' option for vectors in the collection create API. This feature is available as of version 1.2.0. For more detailed information on configuring memmap storage and the usage of 'on_disk' option, you can refer to the documentation on [Storage](/documentation/concepts/storage.md).","['document:as cloud, happening when users were pushing massive amounts of data into the service using `wait=false`. This data influx resulted in an overreaching of disk or RAM limits before the Write-Ahead Logging (WAL) was fully applied. This situation was causing Qdrant to attempt a restart and reapplication of WAL, failing recurrently due to the same memory constraints and pushing the service into a frustrating crash loop with many Out-of-Memory errors. Qdrant 1.2 enters recovery mode, if enabled, when it detects a failure on startup. That makes the service halt the loading of collection data and commence operations in a partial state. This state allows for removing collections but doesn\'t support search or update functions. **Recovery mode [has to be enabled by user](/documentation/administration/#recovery-mode).** ### Appendable mmap For a long time, segments using mmap storage were `non-appendable` and could only be constructed by the optimizer. Dynamically adding vectors to the mmap file is fairly complicated and thus not implemented in Qdrant, but we did our best to implement it in the recent release. If you want to read more about segments, check out our docs on [vector storage](/documentation/storage/#vector-storage). ## Security There are two major changes in terms of [security](/documentation/security/): 1. **API-key support** - basic authentication with a static API key to prevent unwanted access. Previously API keys were only supported in [Qdrant Cloud](https://cloud.qdrant.io/). 2. **TLS support** - to use encrypted connections and prevent sniffing/MitM attacks. ## Release notes As usual, [our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.2.0) describe all the changes introduced in the latest version.,source:articles/qdrant-1.2.x.mddocument:= false } }, limit: 3 ); ``` - **All on Disk** - all vectors, original and quantized, are stored on disk. This mode allows to achieve the smallest memory footprint, but at the cost of the search speed. It is recommended to use this mode if you have a large collection and fast storage (e.g. SSD or NVMe). This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage: ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"" }, ""optimizers_config"": { ""memmap_threshold"": 20000 }, ""quantization_config"": { ""scalar"": { ""type"": ""int8"", ""always_ram"": false } } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE), optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000), quantization_config=models.ScalarQuantization( scalar=models.ScalarQuantizationConfig( type=models.ScalarType.INT8, always_ram=False, ), ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", }, optimizers_config: { memmap_threshold: 20000, }, quantization_config: { scalar: { type: ""int8"", always_ram: false, }, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{ quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance, OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization, VectorParams, VectorsConfig, }, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), ..Default::default() })), }), optimizers_config: Some(OptimizersConfigDiff { memmap_threshold: Some(20000), ..Default::default() }), quantization_config: Some(QuantizationConfig { quantization: Some(Quantization::Scalar(ScalarQuantization { r#type: QuantizationType::Int8.into(), always_ram: Some(false), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.OptimizersConfigDiff; import io.qdrant.client.grpc.Collections.QuantizationConfig; import io.qdrant.client.grpc.Collections.QuantizationType; import io.qdrant.client.grpc.Collections.ScalarQuantization; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .build()) .build()) .setOptimizersConfig( OptimizersConfigDiff.newBuilder().setMemmapThreshold(20000).build()) .setQuantizationConfig( QuantizationConfig.newBuilder() .setScalar( ScalarQuantization.newBuilder() .setType(QuantizationType.Int8) .setAlwaysRam(false) .build()) .build()) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine }, optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 }, quantizationConfig: new QuantizationConfig { Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = false } } ); ```,source:documentation/guides/quantization.mddocument:--- title: Storage weight: 80 aliases: - ../storage --- # Storage All data within one collection is divided into segments. Each segment has its independent vector and payload storage as well as indexes. Data stored in segments usually do not overlap. However, storing the same point in different segments will not cause problems since the search contains a deduplication mechanism. The segments consist of vector and payload storages, vector and payload [indexes](../indexing), and id mapper, which stores the relationship between internal and external ids. A segment can be `appendable` or `non-appendable` depending on the type of storage and index used. You can freely add, delete and query data in the `appendable` segment. With `non-appendable` segment can only read and delete data. The configuration of the segments in the collection can be different and independent of one another, but at least one `appendable\' segment must be present in a collection. ## Vector storage Depending on the requirements of the application, Qdrant can use one of the data storage options. The choice has to be made between the search speed and the size of the RAM used. **In-memory storage** - Stores all vectors in RAM, has the highest speed since disk access is required only for persistence. **Memmap storage** - Creates a virtual address space associated with the file on disk. [Wiki](https://en.wikipedia.org/wiki/Memory-mapped_file). Mmapped files are not directly loaded into RAM. Instead, they use page cache to access the contents of the file. This scheme allows flexible use of available memory. With sufficient RAM, it is almost as fast as in-memory storage. ### Configuring Memmap storage There are two ways to configure the usage of memmap(also known as on-disk) storage: - Set up `on_disk` option for the vectors in the collection create API: *Available as of v1.2.0* ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 768, ""distance"": ""Cosine"", ""on_disk"": true } } ``` ```python from qdrant_client import QdrantClient, models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams( size=768, distance=models.Distance.COSINE, on_disk=True ), ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 768, distance: ""Cosine"", on_disk: true, }, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".to_string(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 768, distance: Distance::Cosine.into(), on_disk: Some(true), ..Default::default() })), }), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( ""{collection_name}"", VectorParams.newBuilder() .setSize(768) .setDistance(Distance.Cosine) .setOnDisk(true) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( ""{collection_name}"", new VectorParams { Size = 768, Distance = Distance.Cosine, OnDisk = true } ); ``` This will create a collection with all vectors immediately stored in memmap storage. This is the recommended way, in case your Qdrant instance operates with fast disks and you are working with large collections. - Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage. There are two ways to do ,source:documentation/concepts/storage.md']","The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.",0.8822701162401129,0,0.9999999999,0.7683041169455346,0.0,0.9999999999,1.0,0.022988505747126436,False
9,9,What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` in Qdrant represents the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, write operations become more tolerant to network partitions in the cluster but require a higher number of active replicas to perform write operations. This setting can be configured at the collection's creation time. You can refer to the documentation on distributed deployment for more details and examples on how to set the `write_consistency_factor` parameter (source: documentation/guides/distributed_deployment.md).","['document:request dispatches all operations according to the current topology in order to keep the data synchronized across the cluster. - reads are using a partial fan-out strategy to optimize latency and availability - writes are executed in parallel on all active sharded replicas ![Embeddings](/docs/concurrent-operations-replicas.png) However, in some cases, it is necessary to ensure additional guarantees during possible hardware instabilities, mass concurrent updates of same documents, etc. Qdrant provides a few options to control consistency guarantees: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations. - Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low. - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical. ### Write consistency factor The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default. It can be configured at the collection\'s creation time. ```http PUT /collections/{collection_name} { ""vectors"": { ""size"": 300, ""distance"": ""Cosine"" }, ""shard_number"": 6, ""replication_factor"": 2, ""write_consistency_factor"": 2, } ``` ```python from qdrant_client import QdrantClient from qdrant_client.http import models client = QdrantClient(""localhost"", port=6333) client.create_collection( collection_name=""{collection_name}"", vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE), shard_number=6, replication_factor=2, write_consistency_factor=2, ) ``` ```typescript import { QdrantClient } from ""@qdrant/js-client-rest""; const client = new QdrantClient({ host: ""localhost"", port: 6333 }); client.createCollection(""{collection_name}"", { vectors: { size: 300, distance: ""Cosine"", }, shard_number: 6, replication_factor: 2, write_consistency_factor: 2, }); ``` ```rust use qdrant_client::{ client::QdrantClient, qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig}, }; let client = QdrantClient::from_url(""http://localhost:6334"").build()?; client .create_collection(&CreateCollection { collection_name: ""{collection_name}"".into(), vectors_config: Some(VectorsConfig { config: Some(Config::Params(VectorParams { size: 300, distance: Distance::Cosine.into(), ..Default::default() })), }), shard_number: Some(6), replication_factor: Some(2), write_consistency_factor: Some(2), ..Default::default() }) .await?; ``` ```java import io.qdrant.client.QdrantClient; import io.qdrant.client.QdrantGrpcClient; import io.qdrant.client.grpc.Collections.CreateCollection; import io.qdrant.client.grpc.Collections.Distance; import io.qdrant.client.grpc.Collections.VectorParams; import io.qdrant.client.grpc.Collections.VectorsConfig; QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build()); client .createCollectionAsync( CreateCollection.newBuilder() .setCollectionName(""{collection_name}"") .setVectorsConfig( VectorsConfig.newBuilder() .setParams( VectorParams.newBuilder() .setSize(300) .setDistance(Distance.Cosine) .build()) .build()) .setShardNumber(6) .setReplicationFactor(2) .setWriteConsistencyFactor(2) .build()) .get(); ``` ```csharp using Qdrant.Client; using Qdrant.Client.Grpc; var client = new QdrantClient(""localhost"", 6334); await client.CreateCollectionAsync( collectionName: ""{collection_name}"", vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine }, shardNumber: ,source:documentation/guides/distributed_deployment.mddocument:this consideration can lead not only to functional errors but also erode the trust of users due to inconsistency and confusion, which then leads to them no longer using my work. **3. Speak Up and Effectively Communicate** Finally, In the course of development, encountering differing opinions is commonplace. It\'s essential to remain open to others\' ideas, while also possessing the resolve to communicate one\'s own perspective clearly. This fosters productive discussions and ultimately elevates the quality of the development process. ### Wrap up Being selected for Google Summer of Code 2023 and collaborating with Arnaud and the other Qdrant engineers, along with all the other community members, has been a true privilege. I\'m deeply grateful to those who invested their time and effort in reviewing my code, engaging in discussions about alternatives and design choices, and offering assistance when needed. Through these interactions, I\'ve experienced firsthand the essence of open source and the culture that encourages collaboration. This experience not only allowed me to write Rust code for a real-world product for the first time, but it also opened the door to the amazing world of open source. Without a doubt, I\'m eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I\'ve also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I\'m excited to witness new users and contributors emerge from within my own network! If you want to try out my work, read the [documentation](https://qdrant.tech/documentation/concepts/filtering/#geo-polygon) and then, either sign up for a free [cloud account](https://cloud.qdrant.io) or download the [Docker image](https://hub.docker.com/r/qdrant/qdrant). I look forward to seeing how people are using my work in their own applications!,source:articles/geo-polygon-filter-gsoc.mddocument:many queries are not IO bound, so the overhead may or may not become measurable in your workload. Finally, on-device disks typically carry lower latency than network drives, which may also affect mmap overhead. Therefore before you roll out io\\_uring, perform the above or a similar benchmark with both mmap and io\\_uring and measure both wall time and IOps). Benchmarks are always highly use-case dependent, so your mileage may vary. Still, doing that benchmark once is a small price for the possible performance wins. Also please [tell us](https://discord.com/channels/907569970500743200/907569971079569410) about your benchmark results!,source:articles/io_uring.md']","The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.",0.9043153549496795,0,0.9999999999,0.859950013724051,0.8,0.9999999999,1.0,0.01694915254237288,True
