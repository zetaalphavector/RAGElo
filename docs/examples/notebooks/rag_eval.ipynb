{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "5767ab9f-1ec6-4668-a47f-d076e3f77387",
         "metadata": {},
         "outputs": [],
         "source": [
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "b6a54813-ad03-480d-bf36-e280a456a53c",
         "metadata": {},
         "outputs": [],
         "source": [
            "from ragelo.utils import load_answers_from_multiple_csvs\n",
            "import glob\n",
            "import os\n",
            "from getpass import getpass\n",
            "import openai"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "e0abde0e-e382-40ac-996d-c7fdfd874fe4",
         "metadata": {},
         "outputs": [],
         "source": [
            "data_folder = \"../data/\"\n",
            "csvs = glob.glob(f\"{data_folder}rag_response_*.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "195c7755-df60-4a55-b82b-27cfa2a808bc",
         "metadata": {},
         "outputs": [],
         "source": [
            "if not (openai_api_key := os.environ.get(\"OPENAI_API_KEY\")):\n",
            "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
            "openai.api_key = openai_api_key\n",
            "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d5c7f760-e2b4-4892-b327-15c3cdf3ab3e",
         "metadata": {},
         "source": [
            "RAGelo is completely independent from your retrieval pipeline. All that it needs are, for each agent/pipeline, their answers and the documents retrieved when building the answers."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "09ab8a62",
         "metadata": {},
         "outputs": [],
         "source": [
            "queries = load_answers_from_multiple_csvs(csvs, query_text_col=\"question\")\n",
            "query_ids = {q.query: q.qid for q in queries}\n",
            "query_dict = {q.qid: q for q in queries}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "a2b13efc",
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "def parse_docs(raw_docs) -> list[tuple[str, str]]:\n",
            "    docs = raw_docs.split(\"\\n\")\n",
            "    documents = []\n",
            "    for d in docs:\n",
            "        doc_text = d.split(\"document:\", maxsplit=1)[1]\n",
            "        doc_source = d.split(\"source:\", maxsplit=1)[1]\n",
            "        documents.append((doc_source, doc_text))\n",
            "    return documents\n",
            "\n",
            "for csv in csvs:\n",
            "    df = pd.read_csv(csv)\n",
            "    for i, row in df.iterrows():\n",
            "        query_id = query_ids[row[\"question\"]]\n",
            "        answer = row[\"answer\"]\n",
            "        docs = parse_docs(row[\"contexts\"])\n",
            "        query = query_dict[query_id]\n",
            "        for doc_source, doc_text in docs:\n",
            "            query.add_retrieved_doc(doc_text, doc_source)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e9f2f56a",
         "metadata": {},
         "source": [
            "## Evaluate retrieved documents"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "62b6f1ca",
         "metadata": {},
         "outputs": [],
         "source": [
            "from ragelo import (\n",
            "    Query,\n",
            "    get_answer_evaluator,\n",
            "    get_llm_provider,\n",
            "    get_retrieval_evaluator,\n",
            ")\n",
            "from ragelo.types.configurations import DomainExpertEvaluatorConfig\n",
            "\n",
            "llm_provider = get_llm_provider(\"openai\", model_name=\"gpt-4o\", max_tokens=2048)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "bec9a5b7",
         "metadata": {},
         "outputs": [],
         "source": [
            "retrieval_evaluator_config = DomainExpertEvaluatorConfig(\n",
            "    expert_in=\"the details of how to better use the Qdrant vector database and vector search engine\",\n",
            "    rich_print=True,\n",
            "    n_processes=20,\n",
            ")\n",
            "retrieval_evaluator = get_retrieval_evaluator(llm_provider=llm_provider, config=retrieval_evaluator_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "482ebe28",
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle\n",
            "# queries = retrieval_evaluator.batch_evaluate(queries)\n",
            "queries = pickle.load(open(\"queries.pkl\", \"rb\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b2c194dd",
         "metadata": {},
         "source": [
            "Let's look at the evaluations produced by the LLM:\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "6bc54a7c",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "LLM Reasoning:\n",
                  "The user query asks about the purpose of the function `CreatePayloadIndexAsync` in the context of using the Qdrant vector database. The query specifically seeks to understand the function's role or utility, likely within the broader operations of managing or manipulating data in the database.\n",
                  "\n",
                  "The provided document passage, however, does not directly mention or discuss the `CreatePayloadIndexAsync` function. Instead, the passage details various operations related to managing payloads and points in the Qdrant database, such as setting, deleting, and updating payloads and points. It includes code snippets and examples of operations like `SetPayload`, `DeletePayload`, `ClearPayload`, and others, which are part of batch operations in managing data points.\n",
                  "\n",
                  "Given the guidelines for relevance:\n",
                  "- **Not Relevant**: The document does not address the `CreatePayloadIndexAsync` function or its purpose. While it discusses related functionalities within the same system (Qdrant), it does not provide information about the specific function queried.\n",
                  "- **Somewhat Relevant**: The document is somewhat relevant because it deals with operations in the same system (Qdrant) and involves manipulation of similar data types (payloads and points). However, it does not provide information specifically about the `CreatePayloadIndexAsync` function.\n",
                  "- **Highly Relevant**: This category does not apply as the document does not directly address the query about the `CreatePayloadIndexAsync` function.\n",
                  "\n",
                  "Based on the analysis, the document is **Somewhat Relevant**. It provides context and examples of related functionalities within the Qdrant system but does not specifically address the query regarding the purpose of `CreatePayloadIndexAsync`. The information is tangentially related to the query as it involves similar types of operations in the same database system, but the primary focus of the document does not align directly with the user's question.\n",
                  "LLM score:\n",
                  "1\n"
               ]
            }
         ],
         "source": [
            "print(\"LLM Reasoning:\")\n",
            "print(queries[5].retrieved_docs[2].evaluation.raw_answer)\n",
            "print(\"LLM score:\")\n",
            "print(queries[5].retrieved_docs[2].evaluation.answer)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "91d7ee4b",
         "metadata": {},
         "outputs": [],
         "source": [
            "from ragelo.types.configurations import PairwiseDomainExpertEvaluatorConfig, answer_evaluator_configs\n",
            "answer_evaluator_config = PairwiseDomainExpertEvaluatorConfig(\n",
            "    expert_in=\"the details of how to better use the Qdrant vector database and vector search engine\",\n",
            "    company = \"Qdrant\",\n",
            "    rich_print=True,\n",
            "    n_processes=20,\n",
            ")\n",
            "    "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "8dbe9114",
         "metadata": {},
         "outputs": [],
         "source": [
            "answer_evaluator = get_answer_evaluator(llm_provider=llm_provider, config=answer_evaluator_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "a3181600",
         "metadata": {},
         "outputs": [],
         "source": [
            "# queries = answer_evaluator.batch_evaluate(queries)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "69e5acdd",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Both Assistant A and Assistant B provide answers regarding the purpose of the 'CreatePayloadIndexAsync' function. They both explain that it is used to create a keyword payload index for a specific field in a collection, which facilitates efficient indexing and retrieval of payload data associated with the field.\n",
                  "\n",
                  "Assistant A cites \"documentation/guides/multiple-partitions.md\" as the source, which is relevant to the question as indicated by the relevance score of 1. Assistant B, however, cites \"documentation/concepts/collections.md\" as the source, which has a relevance score of 0, indicating that it is not relevant to the question.\n",
                  "\n",
                  "The key difference between the two responses lies in the accuracy and relevance of the source cited. Assistant A's response is supported by a relevant document, making it more reliable and trustworthy. Assistant B, despite providing a similar explanation, cites a non-relevant document, which undermines the credibility of the response.\n",
                  "\n",
                  "Given that both assistants provide a similar level of detail and correctness in their explanations, but Assistant A uses a relevant source while Assistant B does not, Assistant A's response is preferable.\n",
                  "\n",
                  "Final verdict: [[A]]\n"
               ]
            }
         ],
         "source": [
            "print(queries[5].pairwise_games[0].evaluation.raw_answer)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "aec93fc9",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:ragelo.logger:Overwriting elo in Answer Evaluator registry\n"
               ]
            }
         ],
         "source": [
            "from ragelo import get_agent_ranker\n",
            "elo_ranker = get_agent_ranker(\"elo\")\n",
            "\n",
            "# elo_ranker = get_"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 192,
         "id": "fa3f0eb4",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "------- Agent Scores by Elo Agent Ranker -------\n",
                  "rag_response_512_5: 1061.0\n",
                  "rag_response_512_5_reranked: 987.0\n",
                  "rag_response_512_3: 973.0\n",
                  "rag_response_512_4_reranked: 953.0\n",
                  "rag_response_512_3_reranked: 949.0\n",
                  "rag_response_512_4: 930.0\n"
               ]
            }
         ],
         "source": [
            "elo_ranker.run(queries)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "df8a217f",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
