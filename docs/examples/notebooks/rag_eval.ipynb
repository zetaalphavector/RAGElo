{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5767ab9f-1ec6-4668-a47f-d076e3f77387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2deaa-75c9-41a7-a3c0-fbd3f8b64320",
   "metadata": {},
   "source": [
    "# **Evaluating multiple RAG pipelines with RAGElo**\n",
    "Unlike other LLM and RAG evaluation frameworks that try to evaluate every individual LLM answer individually, RAGElo focuses on comparing **pairs** of answers in an Elo-style tournement.\n",
    "\n",
    "The idea is that, without a golden answer, LLMs can't really judge any individual answer in isolation. Rather, analyzing if answer A is better than Asnswer B is a more reliable metric of quality, and makes it easier to decide if a new RAG pipeline is better than another.\n",
    "\n",
    "RAGElo works in three steps when evaluating a RAG pipeline\n",
    "1. Gather all documents retrieved by all agents and annotate their relevance to the user's query.\n",
    "2. For each question, generate \"_games_\" between agents, asking the judging LLM to analyze if one agent is better than another, rather than asigning individual scores to each answer.\n",
    "3. With these games, compute the Elo score for each agent, creating a final ranking of agents.\n",
    "\n",
    "Importantly, RAGElo is **agnostic** to your pipeline. Meaning, it will not directly call your RAG system, and can work with any framework or pipeline you use. When used as a library, as we do here, we should create the `Query` objects with your agent's answers, as we show below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5d43c-5c46-4955-8a93-a4f30ff6cd34",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a54813-ad03-480d-bf36-e280a456a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragelo.utils import load_answers_from_multiple_csvs\n",
    "import glob\n",
    "import os\n",
    "from getpass import getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from ragelo import (\n",
    "    Query,\n",
    "    get_answer_evaluator,\n",
    "    get_llm_provider,\n",
    "    get_retrieval_evaluator,\n",
    ")\n",
    "from ragelo.types.configurations import DomainExpertEvaluatorConfig\n",
    "from ragelo.types.configurations import PairwiseDomainExpertEvaluatorConfig\n",
    "from ragelo import get_agent_ranker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a38530-f6ef-41b2-930a-0cf3c83f0e2d",
   "metadata": {},
   "source": [
    "## 2. Setup openai key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755414cd-5b99-4cb6-89ac-e12aad257f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🔑 Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "if not (openai_api_key := os.environ.get(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2a083-236a-4e50-800e-0c215e4c36f3",
   "metadata": {},
   "source": [
    "## 3.Load the answers generated by your RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0abde0e-e382-40ac-996d-c7fdfd874fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/\"\n",
    "csvs = glob.glob(f\"{data_folder}rag_response_*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ab8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_answers_from_multiple_csvs(\n",
    "    csvs,  # A list of all the CSVs with the answers produced by our RAG pipelinesZZZ\n",
    "    query_text_col=\"question\", # this tells RAGelo which column of the CSV has the query itself.\n",
    ")\n",
    "query_ids = {q.query: q.qid for q in queries}\n",
    "query_dict = {q.qid: q for q in queries}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862faa9-2dd7-4995-83c2-173573e581f5",
   "metadata": {},
   "source": [
    "### Parse the documents into the queries\n",
    "\n",
    "Note that the retrieved documents are not tied to a single answer, but to the query itself. In a RAG system, it means that, if the pipeline did not retrieve a relevant document, but another did, the one that didn't performed _worst_ than the one that did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b13efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(raw_docs) -> list[tuple[str, str]]:\n",
    "    docs = raw_docs.split(\"\\n\")\n",
    "    documents = []\n",
    "    for d in docs:\n",
    "        doc_text = d.split(\"document:\", maxsplit=1)[1]\n",
    "        doc_source = d.split(\"source:\", maxsplit=1)[1]\n",
    "        documents.append((doc_source, doc_text))\n",
    "    return documents\n",
    "\n",
    "for csv in csvs:\n",
    "    df = pd.read_csv(csv)\n",
    "    for i, row in df.iterrows():\n",
    "        query_id = query_ids[row[\"question\"]]\n",
    "        answer = row[\"answer\"]\n",
    "        docs = parse_docs(row[\"contexts\"])\n",
    "        query = query_dict[query_id]\n",
    "        for doc_source, doc_text in docs:\n",
    "            query.add_retrieved_doc(doc_text, doc_source) # Here, we use the source as the id of the cocumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2f56a",
   "metadata": {},
   "source": [
    "## 4. Prepare the Evaluators\n",
    "RAGElo uses _evaluators_ as judges. We will instantiate a **retrieval evaluator**, an **answer evaluator** and an **Agents ranker** with their corresponding settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b6f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLM provider can be shared accross all evaluators.\n",
    "llm_provider = get_llm_provider(\"openai\", model_name=\"gpt-4o\", max_tokens=2048)\n",
    "\n",
    "# The DomainExpertEvaluator mimics a persona that is an expert in a given field\n",
    "retrieval_evaluator_config = DomainExpertEvaluatorConfig(\n",
    "    expert_in=\"the details of how to better use the Qdrant vector database and vector search engine\",\n",
    "    company = \"Qdrant\",\n",
    "    n_processes=20, # How many threads to use when evaluating the retrieved documents. Will do that many parallel calls to OpenAI.\n",
    ")\n",
    "retrieval_evaluator = get_retrieval_evaluator(llm_provider=llm_provider, config=retrieval_evaluator_config)\n",
    "\n",
    "\n",
    "#The PairwiseDomainExpertEvaluator is an Answer evaluator similar to the one above, but evaluates pairs of answers.\n",
    "answer_evaluator_config = PairwiseDomainExpertEvaluatorConfig(\n",
    "    expert_in=\"the details of how to better use the Qdrant vector database and vector search engine\",\n",
    "    company = \"Qdrant\",\n",
    "    n_processes=20,\n",
    "    n_games_per_query = 10, # The maximum number of games per query to generate. In this case, 10 pairwise games will be generated for each query.\n",
    ")\n",
    "\n",
    "answer_evaluator = get_answer_evaluator(llm_provider=llm_provider, config=answer_evaluator_config)\n",
    "\n",
    "#The Elo ranker doesn't need an LLM. Here, we instantiate it with the basic settings.\n",
    "elo_ranker = get_agent_ranker(\"elo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b598d-dd9c-454a-b339-cbd3171b3d4d",
   "metadata": {},
   "source": [
    "## 5. Call the evaluators\n",
    "Now, we call each evaluator. Note that they all modify the same queries object, adding more information to it as they go. This also avoids the need of keeping extensinve CSVs or JSONs with intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = retrieval_evaluator.batch_evaluate(queries)\n",
    "queries = answer_evaluator.batch_evaluate(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c16f0-fc6d-42f9-b817-2f7e7e721d28",
   "metadata": {},
   "source": [
    "## 6. Analyze the results\n",
    "Let's look at the evaluations produced by the Evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b52ddd-5c95-4236-94a4-0f8451622bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Reasoning:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Reasoning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mretrieved_docs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mraw_answer)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM score:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(queries[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mretrieved_docs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39manswer)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"LLM Reasoning:\")\n",
    "print(queries[5].retrieved_docs[2].evaluation.raw_answer)\n",
    "print(\"LLM score:\")\n",
    "print(queries[5].retrieved_docs[2].evaluation.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3f0eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Agent Scores by Elo Agent Ranker -------\n"
     ]
    }
   ],
   "source": [
    "elo_ranker.run(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a217f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
