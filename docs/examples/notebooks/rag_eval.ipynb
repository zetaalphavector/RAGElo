{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0157040f",
            "metadata": {},
            "source": [
                "# **Evaluating multiple RAG pipelines with RAGElo**\n",
                "Unlike other LLM and RAG evaluation frameworks that try to evaluate every individual LLM answer individually, RAGElo focuses on comparing **pairs** of answers in an Elo-style tournament.\n",
                "\n",
                "The idea is that, without a golden answer, LLMs can't really judge any individual answer in isolation. Rather, analyzing if answer A is better than answer B is a more reliable metric of quality, and makes it easier to decide if a new RAG pipeline is better than another.\n",
                "\n",
                "RAGElo works in three steps when evaluating a RAG pipeline\n",
                "1. Gather all documents retrieved by all agents and annotate their relevance to the user's query.\n",
                "2. For each question, generate \"_games_\" between agents, asking the judging LLM to analyze if one agent is better than another, rather than assigning individual scores to each answer.\n",
                "3. With these games, compute the Elo score for each agent, creating a final ranking of agents.\n",
                "\n",
                "Importantly, RAGElo is **agnostic** to your pipeline. Meaning, it will _not_ directly call your RAG system. The good thing is that it can work with any framework or pipeline you use. When used as a library, as we do here, a collection of queries is managed  by an `Experiment` object that can be initialized as shown here:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2e5d43c-5c46-4955-8a93-a4f30ff6cd34",
            "metadata": {},
            "source": [
                "## 1. Import packages and setup OpenAI API key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "b6a54813-ad03-480d-bf36-e280a456a53c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import json\n",
                "from getpass import getpass\n",
                "\n",
                "import openai\n",
                "\n",
                "from ragelo import (\n",
                "    get_agent_ranker,\n",
                "    get_answer_evaluator,\n",
                "    get_llm_provider,\n",
                "    get_retrieval_evaluator,\n",
                ")\n",
                "\n",
                "# RAGElo is based around Experiments, that have multiple Queries. \n",
                "from ragelo import Experiment\n",
                "\n",
                "if not (openai_api_key := os.environ.get(\"OPENAI_API_KEY\")):\n",
                "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
                "openai.api_key = openai_api_key\n",
                "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37f2a083-236a-4e50-800e-0c215e4c36f3",
            "metadata": {},
            "source": [
                "## 3. Load the queries, documents retrieved and answers generated by your RAG pipelines\n",
                "The simplest way to load queries, documents and answers from multiple pipelines is to pass their CSV paths in the `Experiment` initialization:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "id": "2e862c55",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading existing experiment from ragelo_cache/RAGELo_evaluation.json\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Loading existing experiment from ragelo_cache/RAGELo_evaluation.json\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "experiment = Experiment(\n",
                "    experiment_name=\"RAGELo_evaluation\",\n",
                "    queries_csv_path=\"./data/queries.csv\",\n",
                "    documents_csv_path=\"./data/documents.csv\",\n",
                "    answers_csv_path=\"./data/answers.csv\",\n",
                "    csv_agent_col=\"agent\", \n",
                "    verbose=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e9f2f56a",
            "metadata": {},
            "source": [
                "## 4. Prepare the Evaluators\n",
                "RAGElo uses _evaluators_ as judges. We will instantiate a **retrieval evaluator**, an **answer evaluator** and an **agents ranker** with their corresponding settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "id": "8f9aa7fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The LLM provider can be shared across all evaluators.\n",
                "llm_provider = get_llm_provider(\"openai\", model_name=\"gpt-4o-mini\")\n",
                "\n",
                "# The DomainExpertEvaluator and the PairwiseDomainExpertEvaluator mimics a persona that is an expert in a given field from a specific company.\n",
                "# The evaluators will add this persona and company information to the prompts when evaluating the answers.\n",
                "kwargs = {\n",
                "    \"llm_provider\": llm_provider,\n",
                "    \"expert_in\": \"the details of how to better use the Qdrant vector database and vector search engine\",\n",
                "    \"company\": \"Qdrant\",\n",
                "    \"n_processes\": 20, # How many threads to use when evaluating the retrieved documents. Will do that many parallel calls to OpenAI.\n",
                "    \"rich_print\": True, # Wether or not to use rich to print colorful outputs.\n",
                "    # \"force\": True, # Whether or not to overwrite any existing files.\n",
                "}\n",
                "\n",
                "retrieval_evaluator = get_retrieval_evaluator(\n",
                "    \"domain_expert\",\n",
                "    **kwargs,\n",
                ")\n",
                "\n",
                "answer_evaluator = get_answer_evaluator(\n",
                "    \"domain_expert\",\n",
                "    **kwargs,\n",
                "    bidirectional=False, # Whether or not to evaluate the answers in both directions.\n",
                "    n_games_per_query=20, # The number of games to play for each query.\n",
                "    document_relevance_threshold=2, # The minimum relevance score a document needs to have to be considered relevant.\n",
                ")\n",
                "\n",
                "# The Elo ranker doesn't need an LLM. Here, we instantiate it with the basic settings.\n",
                "# We can also instantiate evaluators and agents by passing the configuration directly to the get_agent_ranker or get_*__evaluator functions:\n",
                "elo_ranker = get_agent_ranker(\n",
                "    \"elo\",\n",
                "    k=32,  # The k-factor for the Elo ranking algorithm\n",
                "    initial_score=1000,  # Initial score for the agents. This will be updated after each game.\n",
                "    rounds=1000, # Number of tournaments to play\n",
                "    **kwargs,\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc6b598d-dd9c-454a-b339-cbd3171b3d4d",
            "metadata": {},
            "source": [
                "## 5. Call the evaluators\n",
                "Now, we actually run the evaluators. \n",
                "\n",
                "As the evaluators run, we write their intermediate outputs to disk. In this example, each evaluation output is written as a new line in `.ragelo_cache/RAGELo_evaluation_results.jsonl`. This allows us to, if something breaks, re-load the experiments and not have to re-run the evaluations that were completed. \n",
                "\n",
                "After each evaluator finishes running, it also dumps the whole `Experiment` state to`./ragelo_cache/RAGELo_evaluation.json`. This allows us to re-load this experiment later, and use it again somewhere else later.\n",
                "\n",
                "As the focus of RAGElo is to evaluate the quality of RAG pipelines, the __retrieval__ component is extremely important, and the answers of the agents are evaluated based not only on the quality of the documents they have retrieved, but the quality of all the documents retrieved by any agent. The intuition here is that if there are many relevant documents in the corpus, potentially retrieved by other agents, the evaluation should take these into account, even if a specific agent did not retrieve them.\n",
                "\n",
                "When evaluating a (pair of) answer(s), the LLM will be able to see all the relevant documents retrieved by all agents, and will be able to compare the quality of the answers based on the quality of _all_ the relevant documents retrieved by any agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "id": "482ebe28",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">All <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> documents are already evaluated.\n",
                            "If you want to re-evaluate them, use the --force flag\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "All \u001b[1;36m50\u001b[0m documents are already evaluated.\n",
                            "If you want to re-evaluate them, use the --force flag\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "42bcf37bbdf84c64a23097d5a9a07d04",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                        ],
                        "text/plain": []
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">All <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span> answers are already evaluated.\n",
                            "If you want to re-evaluate them, use the --force flag\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "All \u001b[1;36m150\u001b[0m answers are already evaluated.\n",
                            "If you want to re-evaluate them, use the --force flag\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "77a5508830364dbb98955d11eea109fd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                        ],
                        "text/plain": []
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Evaluate all the retrieved documents for all the queries\n",
                "retrieval_evaluator.evaluate_experiment(experiment) \n",
                "# As the evaluator is a PairwiseDomainExpertEvaluator, it will create random pairs of agent's answers for the same query and evaluate them.\n",
                "answer_evaluator.evaluate_experiment(experiment)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52d2b2e3",
            "metadata": {},
            "source": [
                "### Let's see what is happening under the hood\n",
                "An experiment contain multiple queries, each with their own documents, answers and pairwise games.\n",
                "We will select a random query and take a look at its contents and the evaluations of the documents, answers and pairwise games.\n",
                "\n",
                "In more details, an experiment contains:\n",
                "- A `Query` object that contains the query itself, the documents retrieved by all the agents (`retrieved_docs`), the answers generated by each agent (`answers`) and the pairwise games that the AnswerEvaluator generated (`pairwise_games`).\n",
                "- Each document, answer and pairwise game may have an evaluation (`.evaluation`), that contains the raw LLM output for the object and a parsed version of it, according to the evaluator's settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "id": "fac013dc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "🔎 The query object:\n",
                        "\t💬 Query text: \"What is vaccum optimizer ?\"\n",
                        "\t📚 5 retrieved documents by all agents\n",
                        "\t📊 Average relevance score of the retrieved documents : 0.4\n",
                        "\t🕵️ 6 Agents answered the query\n",
                        "\t🏆 15 games were evaluated for this query\n",
                        "--------------------------------------------------------------------------------\n",
                        "📜 The document object:\n",
                        "\t📄 Document text: \"--- title: Optimize Resources weight: 11 aliases: - ../tutorials/optimize --- # Optimize Qdrant Diff\" (...)\n",
                        "\t📈 Document's evaluation:\n",
                        "\t\t💭 LLM's raw output for the evaluation (reasoning): \"To evaluate the relevance of the retrieved document passage in relation to the user query \"What is v\" (...)\n",
                        "\t\t💯 Document's relevance score (between 0 and 2): 0.0\n",
                        "--------------------------------------------------------------------------------\n",
                        "🆚 Pairwise games played:\n",
                        "\tGame between agents 🕵️agent_1 🆚 🕵️agent_3\n",
                        "\t💭 LLM's reasoning for the quality of the answer of agent A: \"Assistant A provides a clear and concise explanation of the Vacuum Optimizer in Qdrant, detailing it (...)\"\n",
                        "\t💭 LLM's reasoning for the quality of the answer of agent B: \"Assistant B also offers a thorough explanation of the Vacuum Optimizer, similar to Assistant A. It i (...)\"\n",
                        "\t💭 LLM's reasoning when comparing the two answers: \"Both assistants provide accurate and relevant information regarding the Vacuum Optimizer. However, A (...)\"\n",
                        "\t💯 Game's winner: {'answer_a_reasoning': 'Assistant A provides a clear and concise explanation of the Vacuum Optimizer in Qdrant, detailing its purpose and functionality. It correctly explains that the optimizer addresses the accumulation of deleted records and how it operates by marking records as deleted rather than immediately removing them. However, it lacks specific details about the parameters that can be set for optimization, which could enhance the completeness of the answer.', 'answer_b_reasoning': 'Assistant B also offers a thorough explanation of the Vacuum Optimizer, similar to Assistant A. It includes all the key points about the accumulation of deleted records and the triggering conditions for the optimizer. Additionally, it mentions specific parameters that can be set, such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment, which adds depth and completeness to the response. The reference to the documentation is also appropriately cited.', 'comparison_reasoning': \"Both assistants provide accurate and relevant information regarding the Vacuum Optimizer. However, Assistant B's response is more comprehensive as it includes additional details about the parameters that can be configured for the optimizer, which enhances the overall helpfulness and depth of the answer. Assistant A, while correct, is less detailed and misses these important aspects.\", 'winner': 'B'}\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "query = random.choice(list(experiment.queries.values()))\n",
                "\n",
                "print(\"🔎 The query object:\")\n",
                "print(f'\\t💬 Query text: \"{query.query}\"')\n",
                "print(f\"\\t📚 {len(query.retrieved_docs)} retrieved documents by all agents\")\n",
                "average_relevance = sum([int(d.evaluation.answer) for d in query.retrieved_docs.values()]) / len(\n",
                "    query.retrieved_docs\n",
                ")\n",
                "print(f\"\\t📊 Average relevance score of the retrieved documents : {average_relevance}\")\n",
                "print(f\"\\t🕵️ {len(query.answers)} Agents answered the query\")\n",
                "print(f\"\\t🏆 {len(query.pairwise_games)} games were evaluated for this query\")\n",
                "document = random.choice(list(query.retrieved_docs.values()))\n",
                "print(\"-\" * 80)\n",
                "print(\"📜 The document object:\")\n",
                "print(f'\\t📄 Document text: \"{document.text[:100]}\" (...)')\n",
                "print(\"\\t📈 Document's evaluation:\")\n",
                "document_evaluation = document.evaluation\n",
                "print(\n",
                "    f'\\t\\t💭 LLM\\'s raw output for the evaluation (reasoning): \"{document_evaluation.raw_answer[:100]}\" (...)'\n",
                ")\n",
                "print(\n",
                "    f\"\\t\\t💯 Document's relevance score (between 0 and 2): {document_evaluation.answer}\"\n",
                ")\n",
                "print(\"-\" * 80)\n",
                "print(\"🆚 Pairwise games played:\")\n",
                "game = random.choice(query.pairwise_games)\n",
                "llm_raw_answer = json.loads(game.evaluation.raw_answer)\n",
                "print(\n",
                "    f\"\\tGame between agents 🕵️{game.agent_a_answer.agent} 🆚 🕵️{game.agent_b_answer.agent}\"\n",
                ")\n",
                "print(\n",
                "    f'\\t💭 LLM\\'s reasoning for the quality of the answer of agent A: \"{llm_raw_answer[\"answer_a_reasoning\"][:100]} (...)\"'\n",
                ")\n",
                "print(\n",
                "    f'\\t💭 LLM\\'s reasoning for the quality of the answer of agent B: \"{llm_raw_answer[\"answer_b_reasoning\"][:100]} (...)\"'\n",
                ")\n",
                "print(f'\\t💭 LLM\\'s reasoning when comparing the two answers: \"{llm_raw_answer[\"comparison_reasoning\"][:100]} (...)\"')\n",
                "best_agent = game.evaluation.answer\n",
                "if best_agent == \"A\":\n",
                "    best_agent = game.agent_a_answer.agent\n",
                "elif best_agent == \"B\":\n",
                "    best_agent = game.agent_b_answer.agent\n",
                "elif best_agent == \"C\":\n",
                "    best_agent = \"TIE\"\n",
                "print(f\"\\t💯 Game's winner: {best_agent}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c81c16f0-fc6d-42f9-b817-2f7e7e721d28",
            "metadata": {},
            "source": [
                "## 6. Rank the agents\n",
                "Based on the results of the games played, we now run the Elo ranker to determine which agent wins the tournament.\n",
                "\n",
                "If we re-run the tournament multiple times, small variations may happen. Therefore, we re-run the tournament multiple times and average the results to get a more stable ranking. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "id": "fa3f0eb4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\"> Agents Elo Ratings </span>-------\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "-------\u001b[1;37m Agents Elo Ratings \u001b[0m-------\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_1        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1295.5</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">210.6</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_1        \u001b[0m: \u001b[1;35m1295.5\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m210.6\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_5        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1157.3</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133.8</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_5        \u001b[0m: \u001b[1;35m1157.3\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m133.8\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_2        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1072.2</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">252.7</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_2        \u001b[0m: \u001b[1;35m1072.2\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m252.7\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_0        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">822.2</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238.3</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_0        \u001b[0m: \u001b[1;35m822.2\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m238.3\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_3        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">815.6</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">232.2</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_3        \u001b[0m: \u001b[1;35m815.6\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m232.2\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_4        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">689.5</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">163.4</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_4        \u001b[0m: \u001b[1;35m689.5\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m163.4\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "EloTournamentResult(agents=['agent_1', 'agent_4', 'agent_0', 'agent_3', 'agent_5', 'agent_2'], scores={'agent_1': 1295.5, 'agent_4': 689.5, 'agent_0': 822.2, 'agent_3': 815.6, 'agent_5': 1157.3, 'agent_2': 1072.2}, games_played={'agent_1': 500, 'agent_4': 500, 'agent_0': 500, 'agent_3': 500, 'agent_5': 500, 'agent_2': 500}, wins={'agent_4': 210, 'agent_3': 210, 'agent_5': 240, 'agent_1': 270, 'agent_2': 250, 'agent_0': 210}, loses={'agent_1': 210, 'agent_0': 240, 'agent_3': 240, 'agent_5': 220, 'agent_4': 270, 'agent_2': 210}, ties={'agent_0': 50, 'agent_5': 40, 'agent_3': 50, 'agent_4': 20, 'agent_1': 20, 'agent_2': 40}, std_dev={'agent_1': 210.61873136072205, 'agent_4': 163.4290365877496, 'agent_0': 238.2627960886886, 'agent_3': 232.20602920682313, 'agent_5': 133.84995330593134, 'agent_2': 252.6985555953971}, total_games=1500, total_tournaments=10)"
                        ]
                    },
                    "execution_count": 73,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "elo_ranker = get_agent_ranker(\n",
                "    \"elo\",\n",
                "    k=32,  # The k-factor for the Elo ranking algorithm\n",
                "    initial_score=1000,  # Initial score for the agents. This will be updated after each game.\n",
                "    rounds=1000, # Number of tournaments to play\n",
                "    **kwargs,\n",
                ")\n",
                "\n",
                "elo_ranker.run(experiment)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
