{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767ab9f-1ec6-4668-a47f-d076e3f77387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a54813-ad03-480d-bf36-e280a456a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragelo.utils import load_answers_from_multiple_csvs\n",
    "import glob\n",
    "import os\n",
    "from getpass import getpass\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d593eb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0abde0e-e382-40ac-996d-c7fdfd874fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data/\"\n",
    "csvs = glob.glob(f\"{data_folder}rag_response_*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "195c7755-df60-4a55-b82b-27cfa2a808bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.environ.get(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7f760-e2b4-4892-b327-15c3cdf3ab3e",
   "metadata": {},
   "source": [
    "RAGelo is completely independent from your retrieval pipeline. All that it needs are, for each agent/pipeline, their answers and the documents retrieved when building the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ab8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_answers_from_multiple_csvs(csvs, query_text_col=\"question\")\n",
    "query_ids = {q.query: q.qid for q in queries}\n",
    "query_dict = {q.qid: q for q in queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b13efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def parse_docs(raw_docs) -> list[tuple[str, str]]:\n",
    "    docs = raw_docs.split(\"\\n\")\n",
    "    documents = []\n",
    "    for d in docs:\n",
    "        doc_text = d.split(\"document:\", maxsplit=1)[1]\n",
    "        doc_source = d.split(\"source:\", maxsplit=1)[1]\n",
    "        documents.append((doc_source, doc_text))\n",
    "    return documents\n",
    "\n",
    "for csv in csvs:\n",
    "    df = pd.read_csv(csv)\n",
    "    for i, row in df.iterrows():\n",
    "        query_id = query_ids[row[\"question\"]]\n",
    "        answer = row[\"answer\"]\n",
    "        docs = parse_docs(row[\"contexts\"])\n",
    "        query = query_dict[query_id]\n",
    "        for doc_source, doc_text in docs:\n",
    "            query.add_retrieved_doc(doc_text, doc_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2f56a",
   "metadata": {},
   "source": [
    "## Evaluate retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b6f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragelo import (\n",
    "    Query,\n",
    "    get_answer_evaluator,\n",
    "    get_llm_provider,\n",
    "    get_retrieval_evaluator,\n",
    ")\n",
    "from ragelo.types.configurations import DomainExpertEvaluatorConfig\n",
    "\n",
    "llm_provider = get_llm_provider(\"openai\", model_name=\"gpt-4o\", max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bec9a5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown retrieval evaluator domain_expert\nValid options are []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m retrieval_evaluator_config \u001b[38;5;241m=\u001b[39m DomainExpertEvaluatorConfig(\n\u001b[1;32m      2\u001b[0m     expert_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe details of how to better use the Qdrant vector database and vector search engine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     rich_print\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m retrieval_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieval_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrieval_evaluator_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m queries \u001b[38;5;241m=\u001b[39m retrieval_evaluator\u001b[38;5;241m.\u001b[39mbatch_evaluate(queries)\n",
      "File \u001b[0;32m~/zav/ragelo/ragelo/evaluators/retrieval_evaluators/base_retrieval_evaluator.py:263\u001b[0m, in \u001b[0;36mget_retrieval_evaluator\u001b[0;34m(evaluator_name, llm_provider, config, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither the evaluator_name or a config object must be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m         )\n\u001b[1;32m    261\u001b[0m     evaluator_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mevaluator_name\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrievalEvaluatorFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zav/ragelo/ragelo/evaluators/retrieval_evaluators/base_retrieval_evaluator.py:236\u001b[0m, in \u001b[0;36mRetrievalEvaluatorFactory.create\u001b[0;34m(cls, evaluator_name, llm_provider, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     llm_provider_instance \u001b[38;5;241m=\u001b[39m llm_provider\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregistry:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown retrieval evaluator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregistry[evaluator_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown retrieval evaluator domain_expert\nValid options are []"
     ]
    }
   ],
   "source": [
    "retrieval_evaluator_config = DomainExpertEvaluatorConfig(\n",
    "    expert_in=\"the details of how to better use the Qdrant vector database and vector search engine\",\n",
    "    rich_print=True,\n",
    ")\n",
    "retrieval_evaluator = get_retrieval_evaluator(llm_provider=llm_provider, config=retrieval_evaluator_config)\n",
    "queries = retrieval_evaluator.batch_evaluate(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bc54a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object BaseRetrievalEvaluator.batch_evaluate at 0x13a808040>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06df752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval(df['contexts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba30588d-9f98-4563-8229-e7cde3cefb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b7f519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from ragelo import AgentAnswer\n",
    "from ragelo.utils import load_answers_from_multiple_csvs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91d7ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(csvs[0]) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for l in reader:\n",
    "        break\n",
    "    # columns = reader.fieldnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dbe9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'answer', 'contexts', 'ground_truth'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3181600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', 'answer', 'contexts', 'ground_truth']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f623620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is vaccum optimizer ?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[1]['question']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
