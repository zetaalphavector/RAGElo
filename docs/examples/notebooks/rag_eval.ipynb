{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0157040f",
            "metadata": {},
            "source": [
                "# **Evaluating multiple RAG pipelines with RAGElo**\n",
                "Unlike other LLM and RAG evaluation frameworks that try to evaluate every individual LLM answer individually, RAGElo focuses on comparing **pairs** of answers in an Elo-style tournament.\n",
                "\n",
                "The idea is that, without a golden answer, LLMs can't really judge any individual answer in isolation. Rather, analyzing if answer A is better than answer B is a more reliable metric of quality, and makes it easier to decide if a new RAG pipeline is better than another.\n",
                "\n",
                "RAGElo works in three steps when evaluating a RAG pipeline\n",
                "1. Gather all documents retrieved by all agents and annotate their relevance to the user's query.\n",
                "2. For each question, generate \"_games_\" between agents, asking the judging LLM to analyze if one agent is better than another, rather than assigning individual scores to each answer.\n",
                "3. With these games, compute the Elo score for each agent, creating a final ranking of agents.\n",
                "\n",
                "Importantly, RAGElo is **agnostic** to your pipeline. Meaning, it will _not_ directly call your RAG system. The good thing is that it can work with any framework or pipeline you use. When used as a library, as we do here, a collection of queries is managed  by an `Experiment` object that can be initialized as shown here:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2e5d43c-5c46-4955-8a93-a4f30ff6cd34",
            "metadata": {},
            "source": [
                "## 1. Import packages and setup OpenAI API key"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b6a54813-ad03-480d-bf36-e280a456a53c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import json\n",
                "from getpass import getpass\n",
                "\n",
                "import openai\n",
                "\n",
                "from ragelo import (\n",
                "    get_agent_ranker,\n",
                "    get_answer_evaluator,\n",
                "    get_llm_provider,\n",
                "    get_retrieval_evaluator,\n",
                ")\n",
                "\n",
                "# RAGElo is based around Experiments that contain multiple Queries. \n",
                "from ragelo import Experiment\n",
                "\n",
                "if not (openai_api_key := os.environ.get(\"OPENAI_API_KEY\")):\n",
                "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
                "openai.api_key = openai_api_key\n",
                "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37f2a083-236a-4e50-800e-0c215e4c36f3",
            "metadata": {},
            "source": [
                "## 3. Load the queries, documents retrieved and answers generated by your RAG pipelines\n",
                "The simplest way to load queries, documents and answers from multiple pipelines is to pass their CSV paths in the `Experiment` initialization:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "2e862c55",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading existing experiment from ragelo_cache/RAGELo_evaluation.json\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Loading existing experiment from ragelo_cache/RAGELo_evaluation.json\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing all evaluations for </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">10</span><span style=\"color: #808000; text-decoration-color: #808000\"> queries</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mClearing all evaluations for \u001b[0m\u001b[1;33m10\u001b[0m\u001b[33m queries\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleared <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> document evaluations, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span> game evaluations, and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> answer evaluations\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "Cleared \u001b[1;36m50\u001b[0m document evaluations, \u001b[1;36m150\u001b[0m game evaluations, and \u001b[1;36m0\u001b[0m answer evaluations\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "experiment = Experiment(\n",
                "    experiment_name=\"RAGELo_evaluation\",\n",
                "    queries_csv_path=\"./data/queries.csv\",\n",
                "    documents_csv_path=\"./data/documents.csv\",\n",
                "    answers_csv_path=\"./data/answers.csv\",\n",
                "    csv_agent_col=\"agent\", \n",
                "    verbose=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e9f2f56a",
            "metadata": {},
            "source": [
                "## 4. Prepare the Evaluators\n",
                "RAGElo uses _evaluators_ as judges. We will instantiate a **retrieval evaluator**, an **answer evaluator** and an **agents ranker** with their corresponding settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "8f9aa7fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The LLM provider can be shared across all evaluators.\n",
                "llm_provider = get_llm_provider(\"openai\", model=\"gpt-4.1-nano\")\n",
                "\n",
                "# The DomainExpertEvaluator and the PairwiseDomainExpertEvaluator mimics a persona that is an expert in a given field from a specific company.\n",
                "# The evaluators will add this persona and company information to the prompts when evaluating the answers.\n",
                "kwargs = {\n",
                "    \"llm_provider\": llm_provider,\n",
                "    \"expert_in\": \"the details of how to better use the Qdrant vector database and vector search engine\",\n",
                "    \"company\": \"Qdrant\",\n",
                "    \"n_processes\": 20, # How many threads to use when evaluating the retrieved documents. Will do that many parallel calls to OpenAI.\n",
                "    \"rich_print\": True, # Wether or not to use rich to print colorful outputs.\n",
                "    # \"force\": True, # Whether or not to overwrite any existing files.\n",
                "}\n",
                "\n",
                "retrieval_evaluator = get_retrieval_evaluator(\n",
                "    \"domain_expert\",\n",
                "    **kwargs,\n",
                ")\n",
                "\n",
                "answer_evaluator = get_answer_evaluator(\n",
                "    \"domain_expert\",\n",
                "    **kwargs,\n",
                "    bidirectional=False, # Whether or not to evaluate the answers in both directions.\n",
                "    n_games_per_query=20, # The number of games to play for each query.\n",
                "    document_relevance_threshold=2, # The minimum relevance score a document needs to have to be considered relevant.\n",
                ")\n",
                "\n",
                "# The Elo ranker doesn't need an LLM. Here, we instantiate it with the basic settings.\n",
                "# We can also instantiate evaluators and agents by passing the configuration directly to the get_agent_ranker or get_*__evaluator functions:\n",
                "elo_ranker = get_agent_ranker(\n",
                "    \"elo\",\n",
                "    k=32,  # The k-factor for the Elo ranking algorithm\n",
                "    initial_score=1000,  # Initial score for the agents. This will be updated after each game.\n",
                "    rounds=1000, # Number of tournaments to play\n",
                "    **kwargs,\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc6b598d-dd9c-454a-b339-cbd3171b3d4d",
            "metadata": {},
            "source": [
                "## 5. Call the evaluators\n",
                "Now, we actually run the evaluators. \n",
                "\n",
                "As the evaluators run, we write their intermediate outputs to disk. In this example, each evaluation output is written as a new line in `.ragelo_cache/RAGELo_evaluation_results.jsonl`. This allows us to, if something breaks, re-load the experiments and not have to re-run the evaluations that were completed. \n",
                "\n",
                "After each evaluator finishes running, it also dumps the whole `Experiment` state to `./ragelo_cache/RAGELo_evaluation.json`. This allows us to re-load this experiment later, and use it again somewhere else later.\n",
                "\n",
                "As the focus of RAGElo is to evaluate the quality of RAG pipelines, the __retrieval__ component is extremely important, and the answers of the agents are evaluated based not only on the quality of the documents they have retrieved, but the quality of all the documents retrieved by any agent. The intuition here is that if there are many relevant documents in the corpus, potentially retrieved by other agents, the evaluation should take these into account, even if a specific agent did not retrieve them.\n",
                "\n",
                "When evaluating a (pair of) answer(s), the LLM will be able to see all the relevant documents retrieved by all agents, and will be able to compare the quality of the answers based on the quality of _all_ the relevant documents retrieved by any agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "482ebe28",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1d895df3b9944174986a43c8a4308850",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                        ],
                        "text/plain": []
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "017d41e24ebe4aa9b3f2ad45853d18f9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Output()"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">No relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[33mNo relevant documents were retrieved for the query q_6.No documents will be provided to the Answer Evaluator.\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                        ],
                        "text/plain": []
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Evaluate all the retrieved documents for all the queries\n",
                "retrieval_evaluator.evaluate_experiment(experiment) \n",
                "# As the evaluator is a PairwiseDomainExpertEvaluator, it will create random pairs of agent's answers for the same query and evaluate them.\n",
                "answer_evaluator.evaluate_experiment(experiment)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52d2b2e3",
            "metadata": {},
            "source": [
                "### Let's see what is happening under the hood\n",
                "An experiment contains multiple queries, each with their own documents, answers and pairwise games.\n",
                "We will select a random query and take a look at its contents and the evaluations of the documents, answers and pairwise games.\n",
                "\n",
                "In more details, an experiment contains:\n",
                "- A `Query` object that contains the query itself, the documents retrieved by all the agents (`retrieved_docs`), the answers generated by each agent (`answers`) and the pairwise games that the AnswerEvaluator generated (`pairwise_games`).\n",
                "- Each document, answer and pairwise game may have an evaluation (`.evaluation`), that contains the raw LLM output for the object and a parsed version of it, according to the evaluator's settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "id": "fac013dc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "🔎 The query object:\n",
                        "\t💬 Query text: \"How does oversampling helps?\"\n",
                        "\t📚 5 retrieved documents by all agents\n",
                        "\t📊 Average relevance score of the retrieved documents : 1.8\n",
                        "\t🕵️ 6 Agents answered the query\n",
                        "\t🏆 15 games were evaluated for this query\n",
                        "--------------------------------------------------------------------------------\n",
                        "📜 The document object:\n",
                        "\t📄 Document text: \"leaving those two parameters out of the search query. ## Benchmark results We retrieved some early r\" (...)\n",
                        "\t📈 Document's evaluation:\n",
                        "\t\t💭 LLM's raw output for the evaluation (reasoning): \"To evaluate the relevance of the retrieved document passage to the user query \"How does oversampling\" (...)\n",
                        "\t\t💯 Document's relevance score (between 0 and 2): 2.0\n",
                        "--------------------------------------------------------------------------------\n",
                        "🆚 Pairwise games played:\n",
                        "\tGame between agents 🕵️agent_0 🆚 🕵️agent_4\n",
                        "\t💭 LLM's reasoning for the quality of the answer of agent A: \"Assistant A provides a clear explanation of how oversampling helps by stating that it controls the p (...)\"\n",
                        "\t💭 LLM's reasoning for the quality of the answer of agent B: \"Assistant B offers a similar explanation to Assistant A regarding how oversampling helps in controll (...)\"\n",
                        "\t💭 LLM's reasoning when comparing the two answers: \"Both assistants provide similar explanations regarding the benefits of oversampling, focusing on pre (...)\"\n",
                        "\t💯 Game's winner: {'answer_a_reasoning': 'Assistant A provides a clear explanation of how oversampling helps by stating that it controls the precision of search results in real-time. It mentions the retrieval of more vectors than needed and the assignment of more precise scores during rescoring, which is relevant to the user question. The answer also highlights that this technique improves accuracy without the need to rebuild the index, which adds depth to the explanation. However, it lacks specific references to the provided documents, which could enhance its credibility.', 'answer_b_reasoning': 'Assistant B offers a similar explanation to Assistant A regarding how oversampling helps in controlling precision and improving accuracy in real-time search. It also mentions the retrieval of more vectors and the reassignment of scores during rescoring. Additionally, it references the documentation by Andrey Vasnetsov, which adds a layer of authority to the response. However, like Assistant A, it does not directly cite the provided documents, which could have strengthened the answer.', 'comparison_reasoning': 'Both assistants provide similar explanations regarding the benefits of oversampling, focusing on precision control and accuracy improvement. However, Assistant B slightly edges out Assistant A by referencing the documentation author, which adds credibility to its claims. Both answers lack direct citations from the provided documents, but the mention of Andrey Vasnetsov in Assistant B gives it a slight advantage in terms of authority.', 'winner': 'B'}\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "query = random.choice(list(experiment.queries.values()))\n",
                "\n",
                "print(\"🔎 The query object:\")\n",
                "print(f'\\t💬 Query text: \"{query.query}\"')\n",
                "print(f\"\\t📚 {len(query.retrieved_docs)} retrieved documents by all agents\")\n",
                "average_relevance = sum([int(d.evaluation.answer) for d in query.retrieved_docs.values()]) / len(query.retrieved_docs) # type: ignore\n",
                "print(f\"\\t📊 Average relevance score of the retrieved documents : {average_relevance}\")\n",
                "print(f\"\\t🕵️ {len(query.answers)} Agents answered the query\")\n",
                "print(f\"\\t🏆 {len(query.pairwise_games)} games were evaluated for this query\")\n",
                "document = random.choice(list(query.retrieved_docs.values()))\n",
                "print(\"-\" * 80)\n",
                "print(\"📜 The document object:\")\n",
                "print(f'\\t📄 Document text: \"{document.text[:100]}\" (...)')\n",
                "print(\"\\t📈 Document's evaluation:\")\n",
                "document_evaluation = document.evaluation\n",
                "print(\n",
                "    f'\\t\\t💭 LLM\\'s raw output for the evaluation (reasoning): \"{document_evaluation.raw_answer[:100]}\" (...)' # type: ignore\n",
                ")\n",
                "print(\n",
                "    f\"\\t\\t💯 Document's relevance score (between 0 and 2): {document_evaluation.answer}\" # type: ignore\n",
                ")\n",
                "print(\"-\" * 80)\n",
                "print(\"🆚 Pairwise games played:\")\n",
                "game = random.choice(query.pairwise_games)\n",
                "llm_raw_answer = json.loads(game.evaluation.raw_answer) # type: ignore\n",
                "print(\n",
                "    f\"\\tGame between agents 🕵️{game.agent_a_answer.agent} 🆚 🕵️{game.agent_b_answer.agent}\"\n",
                ")\n",
                "print(\n",
                "    f'\\t💭 LLM\\'s reasoning for the quality of the answer of agent A: \"{llm_raw_answer[\"answer_a_reasoning\"][:100]} (...)\"'\n",
                ")\n",
                "print(\n",
                "    f'\\t💭 LLM\\'s reasoning for the quality of the answer of agent B: \"{llm_raw_answer[\"answer_b_reasoning\"][:100]} (...)\"'\n",
                ")\n",
                "print(f'\\t💭 LLM\\'s reasoning when comparing the two answers: \"{llm_raw_answer[\"comparison_reasoning\"][:100]} (...)\"')\n",
                "best_agent = game.evaluation.answer # type: ignore\n",
                "if best_agent == \"A\":\n",
                "    best_agent = game.agent_a_answer.agent\n",
                "elif best_agent == \"B\":\n",
                "    best_agent = game.agent_b_answer.agent\n",
                "elif best_agent == \"C\":\n",
                "    best_agent = \"TIE\"\n",
                "print(f\"\\t💯 Game's winner: {best_agent}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c81c16f0-fc6d-42f9-b817-2f7e7e721d28",
            "metadata": {},
            "source": [
                "## 6. Rank the agents\n",
                "Based on the results of the games played, we now run the Elo ranker to determine which agent wins the tournament.\n",
                "\n",
                "If we re-run the tournament multiple times, small variations may happen. Therefore, we re-run the tournament multiple times and average the results to get a more stable ranking. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "id": "fa3f0eb4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\"> Agents Elo Ratings </span>-------\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "-------\u001b[1;37m Agents Elo Ratings \u001b[0m-------\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_2        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1265.5</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128.5</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_2        \u001b[0m: \u001b[1;35m1265.5\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m128.5\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_1        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1226.1</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187.5</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_1        \u001b[0m: \u001b[1;35m1226.1\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m187.5\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_5        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">949.8</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136.8</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_5        \u001b[0m: \u001b[1;35m949.8\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m136.8\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_0        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">923.8</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">193.4</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_0        \u001b[0m: \u001b[1;35m923.8\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m193.4\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_3        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">777.9</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">232.5</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_3        \u001b[0m: \u001b[1;35m777.9\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m232.5\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">agent_4        </span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">709.2</span><span style=\"font-weight: bold\">(</span>±<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">132.6</span><span style=\"font-weight: bold\">)</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1;37magent_4        \u001b[0m: \u001b[1;35m709.2\u001b[0m\u001b[1m(\u001b[0m±\u001b[1;36m132.6\u001b[0m\u001b[1m)\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "EloTournamentResult(agents=['agent_1', 'agent_3', 'agent_4', 'agent_5', 'agent_0', 'agent_2'], scores={'agent_1': 1226.1, 'agent_3': 777.9, 'agent_4': 709.2, 'agent_5': 949.8, 'agent_0': 923.8, 'agent_2': 1265.5}, games_played={'agent_1': 500, 'agent_3': 500, 'agent_4': 500, 'agent_5': 500, 'agent_0': 500, 'agent_2': 500}, wins={'agent_1': 270, 'agent_3': 210, 'agent_5': 240, 'agent_2': 250, 'agent_0': 210, 'agent_4': 210}, loses={'agent_3': 240, 'agent_4': 270, 'agent_0': 240, 'agent_5': 220, 'agent_2': 210, 'agent_1': 210}, ties={'agent_0': 50, 'agent_3': 50, 'agent_4': 20, 'agent_5': 40, 'agent_2': 40, 'agent_1': 20}, std_dev={'agent_1': 187.49797332238023, 'agent_3': 232.5142791314116, 'agent_4': 132.59547503591514, 'agent_5': 136.77777597256068, 'agent_0': 193.4015511830244, 'agent_2': 128.48910459645987}, total_games=1500, total_tournaments=10)"
                        ]
                    },
                    "execution_count": 77,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "elo_ranker.run(experiment)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
